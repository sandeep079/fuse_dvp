{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Graph Convolutional Networks (GCNs)**\n","=====================================\n","\n","**Module Objectives**\n","--------------------\n","\n","In this module, you will:\n","\n","1. **Understand** how Graph Convolutional Networks (GCNs) extend traditional convolution operations to graph-structured data.\n","2. **Learn** how information is propagated between nodes using adjacency and degree matrices.\n","3. **Explore** the layer-wise propagation rule and how GCNs aggregate features from neighboring nodes.\n","4. **Discover** key applications of GCNs in node classification and link prediction tasks.\n","\n"],"metadata":{"id":"yZJquWwdcOY-"}},{"cell_type":"markdown","source":["---------\n","**Introduction to Graph Convolutional Networks (GCNs)**\n","------------------------\n","\n","Graph Convolutional Networks (GCNs) are a specialized class of neural networks designed to operate directly on graph-structured data. Unlike traditional data formats like images (grids) or text (sequences), graphs represent relationships between entities using nodes (vertices) and edges (links).\n","\n","**GCNs vs. CNNs**\n","-------------------\n","\n","GCNs and Convolutional Neural Networks (CNNs) share a similar principle: both learn features by aggregating information from neighboring units. However, the main difference lies in the type of data they process. CNNs are designed for regular, Euclidean-structured data like images or sequential text, whereas GCNs generalize this idea to work on arbitrary, unordered, and non-Euclidean data such as graphs.\n","\n","![GCNs vs. CNNs](https://i.postimg.cc/L4YqL0GN/download.png)\n","---\n","**Figure 1:** Comparison between GCNs and CNNs\n","-----------"],"metadata":{"id":"AcCSPNsLcbbM"}},{"cell_type":"markdown","source":["**Key Components of Graphs**\n","-----------------------------\n","\n","* **Nodes (Vertices)**: Represent individual data points (e.g., users, articles, molecules)\n","* **Edges (Links)**: Represent relationships or interactions between nodes (e.g., friendships, citations, chemical bonds)\n","\n","**How GCNs Work**\n","------------------\n","\n","GCNs extend the concept of convolution, commonly used in image processing, to graphs. Instead of applying filters over neighboring pixels in a grid, GCNs aggregate information from neighboring nodes in the graph. This enables each node to update its feature representation based on its local neighborhood.\n","\n","![GCNs vs. CNNs](https://i.postimg.cc/7hWxqYf8/gcn-web.png)\n","---\n","**Figure 2:** Graph Convolutional Networks\n"],"metadata":{"id":"yA-lVZHJcwbM"}},{"cell_type":"markdown","source":["-----------\n","**Definition of GCNs**\n","----------------------\n","\n","Graph Convolutional Networks (GCNs) share a common architecture, where the convolutional filter parameters are **shared across all nodes in the graph**. The objective is to **learn a function** that operates on **signals or features defined over a graph** $G = (V, E)$.\n","\n","-------\n","**GCN Architecture**\n","---------------------\n","\n","A GCN typically consists of multiple layers, each of which can be expressed as a non-linear transformation:\n","\n","$$\n","H^{(l+1)} = f(H^{(l)}, A)\n","$$\n","\n","where:\n","\n","* $H^{(0)} = X$ is the input feature matrix, where $X \\in \\mathbb{R}^{N \\times D}$.\n","* $H^{(L)} = Z$ (or $z$ for graph-level outputs) after $L$ layers.\n","\n","**Input and Output**\n","--------------------\n","\n","The input to a GCN is:\n","\n","* A **feature matrix** $X \\in \\mathbb{R}^{N \\times D}$, where each row $x_i$ represents the feature vector of node $i$.\n","* A **matrix representing the graph structure**, typically the **adjacency matrix** $A \\in \\mathbb{R}^{N \\times N}$.\n","\n","The output of a GCN is:\n","\n","* A **node-level feature matrix** $Z \\in \\mathbb{R}^{N \\times F}$, where $F$ is the number of output features per node.\n","* For graph-level tasks, a **pooling operation** can be applied to aggregate node features into a graph representation.\n","\n","--------\n","**Layer-wise Propagation Rule**\n","------------------------------\n","\n","A simple form of the layer-wise propagation rule is:\n","\n","$$\n","f(H^{(l)}, A) = \\sigma \\left( A H^{(l)} W^{(l)} \\right)\n","$$\n","\n","where:\n","\n","* $W^{(l)}$ is the learnable weight matrix at layer $l$.\n","* $\\sigma$ is a non-linear activation function such as ReLU.\n","\n","**Limitations of Basic Formulation**\n","------------------------------------\n","\n","1. **Self-Node Exclusion**: Multiplying by $A$ aggregates feature vectors of all neighboring nodes but excludes the node itself. To address this, **self-loops** are added by incorporating the identity matrix $I$, resulting in $\\hat{A} = A + I$.\n","2. **Lack of Normalization**: The adjacency matrix $A$ is not normalized, which can cause scaling issues when multiplying with feature vectors.\n","\n","----------\n","**Normalized Adjacency Matrices with Self-Loops**\n","-----------------------------------------------\n","![GCNs vs. CNNs](https://i.postimg.cc/htB76cQZ/download-1.png)\n","---\n","**Figure 3:** Normalized Adjacency Matrices with Self-Loops\n","\n","\n","To address these limitations, GCNs use normalized adjacency matrices with self-loops, as introduced in the seminal GCN paper by Kipf and Welling:\n","\n","$$\n","f(H^{(l)}, A) = \\sigma \\left( \\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{(l)} W^{(l)} \\right)\n","$$\n","\n","where $\\hat{A} = A + I$ is the adjacency matrix with added self-loops, and $\\hat{D}$ is the corresponding diagonal degree matrix.\n","\n","**Mathematical Derivations**\n","---------------------------\n","\n","### Adjacency Matrix\n","\n","The adjacency matrix $A$ represents the graph structure, where $A_{ij} =1$ if there is an edge between nodes $i$ and $j$, and $0$ otherwise.\n","\n","### Degree Matrix\n","\n","The degree matrix $D$ is a diagonal matrix, where $D_{ii} = \\sum_{j} A_{ij}$.\n","\n","### Feature Propagation\n","\n","The feature propagation process in GCNs can be represented as:\n","\n","$$\n","H^{(l+1)} = \\sigma \\left( \\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2} H^{(l)} W^{(l)} \\right)\n","$$\n","\n","where $\\sigma$ is a non-linear activation function.\n","\n","**Applications in Node Classification and Link Prediction**\n","---------------------------------------------------\n","\n","GCNs have been successfully applied to various tasks, including:\n","\n","* **Node Classification**: Predicting labels for nodes in a graph, such as detecting fake accounts in a social network.\n","* **Link Prediction**: Predicting missing or future edges in a graph, such as friend recommendations.\n","\n","**Conclusion**\n","----------\n","\n","In this module, we have introduced the concept of Graph Convolutional Networks (GCNs) and their applications in graph-structured data. We have discussed the key components of GCNs, including adjacency and degree matrices, feature propagation, linear transformation, and non-linearity. We have also explored the layer-wise propagation rule and how GCNs aggregate features from neighboring nodes. Finally, we have discussed the limitations of the basic formulation and how they are addressed using normalized adjacency matrices with self-loops.\n","\n","**Key Takeaways**\n","-----------------\n","\n","* GCNs are a class of neural networks designed to operate on graph-structured data.\n","* GCNs extend the concept of convolution to graphs by aggregating information from neighboring nodes.\n","* GCNs can learn context-aware representations of nodes and capture both local and global structural patterns.\n","* GCNs have been successfully applied to node classification and link prediction tasks.\n","\n","**Future Directions**\n","--------------------\n","\n","* **GCN variants**: There are many variants of GCNs, including Graph Attention Networks (GATs), Graph Autoencoders (GAEs), and Graph Transformers.\n","* **Applications in real-world problems**: GCNs have been applied to various real-world problems, including social network analysis, recommendation systems, and computer vision.\n"],"metadata":{"id":"LBAupSkCrUAn"}},{"cell_type":"markdown","source":["-----------\n","## **Types of GCNs**\n","\n","\n","GCNs can be broadly classified into two categories:\n","\n","1. **Spatial Graph Convolutional Networks**: Directly operate on the graph's structure by aggregating neighbor features in the spatial domain.\n","2. **Spectral Graph Convolutional Networks**: Define convolutions based on the graph's spectral (frequency) properties derived from its Laplacian matrix.\n","-------"],"metadata":{"id":"rAqb-GpkdaQz"}},{"cell_type":"markdown","source":["## **Spectral vs Spatial Methods in GCNs**\n","\n","### **Spectral Methods**\n","\n","- Based on **graph signal processing**.\n","- Use the graph Laplacian matrix to define convolution in the **frequency domain**.\n","- Convolution is defined as:\n","\n","$$\n","g_\\theta * x = U g_\\theta(\\Lambda) U^T x\n","$$\n","\n","Where:\n","- $U$ = eigenvectors of the Laplacian\n","- $\\Lambda$ = eigenvalues\n","- $g_\\theta$ = learnable filter\n","- $x$ = input features\n","\n","**Limitations:**\n","- Requires eigen decomposition → very slow for large graphs\n","- Filters are not localized (not focused only on neighbors)\n","- Hard to apply across different graph structures\n","\n","---\n","\n","### **Spatial Methods**\n","\n","- Work **directly on the graph** (no frequency transform).\n","- Each node updates its features using **its neighbors’ features**.\n","- Much more efficient and easier to scale.\n","\n","---\n","\n","### GCN as a Simplification of Spectral Method\n","\n","Kipf & Welling (2017) simplified spectral convolution using **first-order approximation**:\n","\n","$$\n","H^{(l+1)} = \\sigma\\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)} \\right)\n","$$\n","\n","Where:\n","- $\\tilde{A} = A + I$ (adjacency matrix with self-loops)\n","- $\\tilde{D}$ = degree matrix of $\\tilde{A}$\n","- $H^{(l)}$ = features at layer $l$\n","- $W^{(l)}$ = weight matrix\n","- $\\sigma$ = activation function (e.g., ReLU)\n","\n","**What's happening:**\n","- Each node averages its own and neighbors’ features\n","- Applies linear transformation and activation\n","- No eigendecomposition needed → fast and scalable\n","\n","---\n","\n","### Summary Table\n","\n","| Category        | Spectral Method                         | Spatial Method / GCN                     |\n","|----------------|------------------------------------------|-------------------------------------------|\n","| Works in       | Frequency domain                         | Node neighborhood                         |\n","| Speed          | Slow (eigendecomposition)                | Fast (matrix multiplication)              |\n","| Generalizable  | Hard to apply to new graphs              | Easy to generalize                        |\n","| GCN role       | Uses spectral ideas (first-order)        | Acts like spatial method in practice      |\n"],"metadata":{"id":"wQA3eBBlo4I6"}},{"cell_type":"markdown","source":["## **Over-Smoothing & Limitations in GCNs**\n","\n","###  Why GCNs suffer as the number of layers increases?\n","\n","- When we stack many GCN layers, each node keeps aggregating features from more distant neighbors.\n","- Eventually, **node representations become very similar**, losing unique information.\n","- This is known as **over-smoothing**.\n","\n","####  What is Over-Smoothing?\n","\n","> Over-smoothing means that all node embeddings become indistinguishable after many GCN layers.\n","\n","Mathematically, as $L \\rightarrow \\infty$:\n","\n","$$\n","H^{(L)} \\rightarrow \\text{a constant matrix (all rows similar)}\n","$$\n","\n","So, increasing layers can hurt performance instead of improving it.\n","\n","---\n","\n","###  How to Reduce Over-Smoothing?\n","\n","One common solution is to use **skip connections**, like in **Jumping Knowledge Networks (JK-Nets)**.\n","\n","---\n","\n","##  Jumping Knowledge Networks (JK-Nets)\n","\n","- Instead of using only the final layer, JK-Nets combine outputs from **multiple intermediate GCN layers**.\n","- This allows the model to **adaptively choose** information from different depths.\n","\n","###  Idea:\n","\n","Let $H^{(1)}, H^{(2)}, ..., H^{(L)}$ be the outputs of each layer. Then the final node representation is:\n","\n","$$\n","H^{\\text{final}} = \\text{Aggregate}(H^{(1)}, H^{(2)}, ..., H^{(L)})\n","$$\n","\n","- Aggregation can be **concatenation**, **max-pooling**, or **attention-based**.\n","- This helps preserve both shallow and deep neighborhood information.\n","\n","---\n","\n","###  Benefits of Skip Connections\n","\n","- Helps avoid over-smoothing.\n","- Allows the model to **learn from both low-level and high-level features**.\n","- Improves performance on deep GCN architectures.\n","\n","---\n","\n","###  Summary\n","\n","| Problem           | Solution                             |\n","|------------------|--------------------------------------|\n","| Over-smoothing   | Use fewer layers, or skip connections |\n","| Loss of features | Combine features from different layers |\n","| Deep GCN issues  | Apply Jumping Knowledge Networks      |\n"],"metadata":{"id":"enlAB0eV1-id"}},{"cell_type":"markdown","source":["---------\n","\n","**Importance of GCNs**\n","----------------------\n","\n","Through graph-based convolution, GCNs can:\n","\n","1. **Learn context-aware representations** of nodes\n","2. **Capture both local** (1-hop) and **global** (multi-hop) structural patterns\n","3. **Enable powerful graph-level learning** with minimal supervision\n","\n","**Key Applications of GCNs**\n","-----------------------------\n","\n","* **Node Classification**: Predict labels for nodes (e.g., detecting fake accounts in a social network)\n","* **Graph Classification**: Assign labels to entire graphs (e.g., molecule property prediction)\n","* **Link Prediction**: Predict missing or future edges (e.g., friend recommendations)\n"],"metadata":{"id":"YmkwWQSAc6eG"}},{"cell_type":"markdown","source":["**References**\n","--------------\n","\n","* **Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with Graph Convolutional Networks. ICLR.**\n","* **Hamilton, W. L., Ying, R., & Leskovec, J. (2017). Inductive representation learning on large graphs. NeurIPS.**"],"metadata":{"id":"iZ6SvQhGdtDX"}},{"cell_type":"markdown","source":["## Suggestion Points:\n","\n","* Brefiy explain Spectral vs Spatial Methods:\n","  * How original spectral convolutions are computationalyy expensive.\n","  * How GCN is just a simplification of Spectral convolution (first order approximation)\n","\n","*  Over Smoothing & Limitations:\n","  * Why GCNs suffers as layer increase?\n","  * The role of skip connections (Jumping Knowledge Networks)"],"metadata":{"id":"7fEjNrGOcnTw"}},{"cell_type":"code","source":[],"metadata":{"id":"0PX6eurqhWHT"},"execution_count":null,"outputs":[]}]}