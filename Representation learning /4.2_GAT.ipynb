{"cells":[{"cell_type":"markdown","source":["## Objectives\n","\n","This module aims to provide a comprehensive understanding of Graph Attention Networks (GATs). Specifically, upon completion, you should be able to:\n","1. **Understand Graphical Neural Network** Understand GNN mechanisms like adjacency matrix, message passing, Invariant Permutation\n","2.  **Understand how attention mechanisms enhance information aggregation in graphs:** Grasp the core idea behind using attention to selectively weigh the importance of neighboring nodes when updating a node's representation.\n","3.  **Understand the multi-head attention for node representations:** Comprehend how multiple independent attention mechanisms can be used to capture diverse relationships and improve the robustness of node embeddings."],"metadata":{"id":"BKWz182zPRkC"}},{"cell_type":"markdown","metadata":{"id":"8LL1fHcbp-L-"},"source":["# Graph based Network\n","A graph-based network or GNN is any neural network that uses the graph’s structure (nodes and edges) to learn meaningful representations or make predictions.\n","Graph structure is important because they represent relationship between entities but using Graph as a neural network comes with challenges which is every graph data is different.\n","<img src =\"https://i.postimg.cc/MHkm8601/graph.png\">\n","Fig: Different Graph structure at different scenarios\n","\n","To handle arbitary input shape there is a feature in graph known as **isomorphism** which states that two graph that looks different can still be strucuturally identical.\n","Like if we flip an image we get a new image but if we flip a graph the only thing that changes is the order of the node.\n","\n","The algorithm that handle graph data needs to be **Permutation Invariant**\n","\n","In the structure of graph **euclidean distance** are not clearly defined because distance only cannot incorporate distance between two nodes. Node embeddings contains **structural** as well as **feature information** of other nodes in the graph.\n","<img src= \"https://i.postimg.cc/4N9Qw9gw/euc.png\">\n","**Left:** Image in euclidean space   **Right:** Graph in non euclidean space\n","\n","For every graph shown, each node can be described which a **feature vector**. For example, if the node of a graph represents  a person the feature vector would be his attributes like name, address, contact details, etc.\n","And the structural representation of graphs can be described by **adjacency matrix**. Adjacency matrix tells us which nodes of the graph is connected. The adjacency matrix cannot be used in feed forward network  because adjacency matrix encodes explicit pairwise connections (edges) between nodes in a graph. It represents structure: who is connected to whom. Meanwhile a feedforward layer (like a fully connected dense layer) assumes that every input unit is independent of the others in how it connects to the weights. There is no notion of explicit pairwise relationships between inputs — the connections are learned freely through weight matrices.\n","\n","**Message Passing Layers**\n","They combine the node and edge information inyo the node embeddings. The graph information ( node features and structural properties) are fed through message passing layers. It construct node embeddings that contain the knowledge about other nodes and edges in a compressed format. This is done by gathering the current information of neighbor nodes combining it in certain ways to get a new embeddings and updateing the node features or states with these embeddings. This is alos called Graph Convolution and can be seen as extention of convolution to graph data.\n","\n","**How Message Passing Layers Works**\n","<img src = \"https://i.postimg.cc/brgB3Sbm/Message-Passing.png\">\n","\n","Lets say each node has 50 features. For node 1 we take fetaure of the node, aggregate it with neighbour nodes and update the feature of node 1.\n","\n","**Operation in Message Passing Layer**\n","\n","\n","---\n","\n","A typical message passing layer has **three main steps**:\n","\n","---\n","\n","## 1. Message Computation\n","\n","Each node \\$v\\$ gathers messages from its neighbors \\$u \\in N(v)\\$:\n","\n","$$\n","m_{uv} = M(h_u, h_v, e_{uv})\n","$$\n","\n","Where:\n","\n","* \\$h\\_u\\$ = feature vector of the neighbor node \\$u\\$\n","* \\$h\\_v\\$ = feature vector of the target node \\$v\\$\n","* \\$e\\_{uv}\\$ = optional edge feature between \\$u\\$ and \\$v\\$\n","* \\$M\\$ = learnable or fixed message function (often a neural network or linear layer)\n","\n","---\n","\n","## 2. Message Aggregation\n","\n","Each node collects all incoming messages:\n","\n","$$\n","m_v = AGG\\big( \\{ m_{uv} : u \\in N(v) \\} \\big)\n","$$\n","<img src = \"https://i.postimg.cc/G2WyVXP2/featureupdate.png\">\n","Here, `AGG` is an aggregation function, other function that could be used:\n","\n","* **Sum:** \\$\\sum\\$\n","* **Mean:** mean\n","* **Max:** \\$\\max\\$\n","\n","---\n","\n","## 3. Node Update\n","\n","Each node updates its own feature using the aggregated message:\n","\n","$$\n","h'_v = U(h_v, m_v)\n","$$\n","\n","Where:\n","\n","* \\$U\\$ is an update function (often an MLP or simply \\$\\text{ReLU}(W \\cdot \\text{concat}(h\\_v, m\\_v))\\$)\n","\n","---\n","\n","## Putting It Together\n","<img src =\"https://i.postimg.cc/prCfyzrX/fdfd.png\">\n","\n","A general message passing layer can be written as:\n","\n","$$\n","h'_v = U\\Big( h_v,\\; AGG\\big( \\{ M(h_u, h_v, e_{uv}) : u \\in N(v) \\} \\big) \\Big)\n","$$\n","\n","---\n","\n","## Example: Simple GCN Layer\n","\n","A classic Graph Convolutional Network (GCN) layer is a special case:\n","\n","* No edge features\n","* Messages are just the neighbor’s feature times a weight matrix\n","\n","The layer is:\n","\n","$$\n","H' = \\sigma(\\tilde{A} H W)\n","$$\n","\n","Where:\n","\n","* \\$\\tilde{A} = D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}\\$ is the normalized adjacency matrix with self-loops\n","* \\$H\\$ is the node feature matrix\n","* \\$W\\$ is a learnable weight matrix\n","* \\$\\sigma\\$ is an activation function (e.g., ReLU)\n","\n","In the GCN layer:\n","\n","$$\n","M(h_u) = h_u,\\quad\n","AGG = \\text{weighted sum by adjacency},\\quad\n","U = \\text{linear + activation}.\n","$$\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","source":["#GNN Variants\n","<ul>\n","<li>Graph Convolution Network</li>\n","<li>Graph Multi Layer Perceptron</li>\n","<li>Graph Attention Networks</li>\n","<li>Gated Graph Neural Networks</li>\n","</ul>"],"metadata":{"id":"c3eDmkEh0IkD"}},{"cell_type":"markdown","source":["#Graph Attention Networks"],"metadata":{"id":"b04MGodZ0IZl"}},{"cell_type":"markdown","metadata":{"id":"k3SJJDpZp-L_"},"source":[]},{"cell_type":"markdown","metadata":{"id":"GzXJnWBfp-L_"},"source":["## Key Topics"]},{"cell_type":"markdown","metadata":{"id":"f-MEDVyOp-MA"},"source":["### 2.1 Attention Coefficients and Neighborhood Weighting"]},{"cell_type":"markdown","metadata":{"id":"6URFMBNAp-MA"},"source":["Traditional Graph Convolutional Networks (GCNs) treat all neighbors equally, or apply weights based on fixed graph structure (e.g., degree normalization). GATs introduce an attention mechanism that allows each node to **learnably assign different importance (attention coefficients)** to its neighbors during the message passing process. This dynamic weighting is crucial for handling complex graph structures and capturing heterogeneous relationships.\n","\n","<img src = \"https://i.postimg.cc/k5x4N0p2/AM.png\" width =50% >\n","\n","Let $h_i \\in \\mathbb{R}^F$ be the input features of node $i$, and $h_j \\in \\mathbb{R}^F$ be the input features of node $j$ (a neighbor of $i$). The attention mechanism first transforms the input features using a shared linear transformation parameterized by a weight matrix $W \\in \\mathbb{R}^{F' \\times F}$, where $F'$ is the number of output features. So, we have $Wh_i$ and $Wh_j$.\n","\n","The attention score $e_{ij}$ between node $i$ and its neighbor $j$ is then computed using a shared attentional mechanism $a$:\n","$$e_{ij} = a(Wh_i, Wh_j)$$\n","This attentional mechanism $a$ is a single-layer feedforward neural network, parameterized by a weight vector $\\vec{a} \\in \\mathbb{R}^{2F'}$, and followed by a LeakyReLU non-linearity:\n","$$e_{ij} = \\text{LeakyReLU}(\\vec{a}^T [Wh_i \\, || \\, Wh_j])$$\n","where $||$ denotes concatenation.\n","\n","To make attention coefficients comparable across different nodes and easier to interpret as weights, we normalize them using the softmax function over all neighbors $j \\in N_i$ of node $i$:\n","$$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in N_i} \\exp(e_{ik})}$$\n","These normalized attention coefficients $\\alpha_{ij}$ represent the importance of node $j$'s features to node $i$'s new representation. Finally, the output feature for node $i$, $h'_i$, is computed as a weighted sum of its neighbors' transformed features, weighted by the attention coefficients:\n","$$h'_i = \\sigma\\left(\\sum_{j \\in N_i} \\alpha_{ij} Wh_j\\right)$$\n","where $\\sigma$ is an activation function (e.g., ELU)."]},{"cell_type":"markdown","source":["Here’s your text rewritten cleanly in **Markdown** with **LaTeX-style math** using `$` for all equations:\n","\n","---\n","\n","# Attention Mechanism\n","\n","The attention mechanism \\$a\\$ is a **single-layer feedforward neural network**, parameterized by a weight vector \\$\\vec{a} \\in \\mathbb{R}^{2F'}\\$, and applies the **LeakyReLU** nonlinearity with a negative input slope \\$\\alpha = 0.2\\$.\n","\n","Fully expanded, the coefficients computed by the attention mechanism  can be expressed as:\n","\n","$$\n","\\alpha_{ij} = \\frac{\n","\\exp\\Big( \\text{LeakyReLU}\\big( \\vec{a}^T [ W \\vec{h}_i \\parallel W \\vec{h}_j ] \\big) \\Big)\n","}{\n","\\sum_{k \\in N_i} \\exp\\Big( \\text{LeakyReLU}\\big( \\vec{a}^T [ W \\vec{h}_i \\parallel W \\vec{h}_k ] \\big) \\Big)\n","}\n","$$\n","\n","Where:\n","\n","* \\$\\vec{h}\\_i\\$ and \\$\\vec{h}\\_j\\$ are the feature vectors of nodes \\$i\\$ and \\$j\\$\n","* \\$W\\$ is a learnable weight matrix\n","* \\$\\parallel\\$ denotes concatenation\n","* \\$N\\_i\\$ is the neighborhood of node \\$i\\$\n","\n","---\n","Once obtained, the normalized attention coefficients are used to compute a linear combination of the\n","features corresponding to them, to serve as the final output features for every node (after potentially applying a nonlinearity, σ):\n","\n","\n","After computing the attention coefficients, the updated node feature is:\n","\n","$$\n","\\vec{h}'_i = \\sigma \\Big( \\sum_{j \\in N_i} \\alpha_{ij} \\, W \\vec{h}_j \\Big)\n","$$\n","\n","Where:\n","\n","* \\$\\vec{h}'\\_i\\$ is the updated feature vector for node \\$i\\$\n","* \\$\\sigma\\$ is a nonlinearity (e.g., ELU or ReLU)\n","* \\$\\alpha\\_{ij}\\$ are the normalized attention coefficients\n","* \\$W\\$ is a learnable weight matrix\n","* \\$N\\_i\\$ is the neighborhood of node \\$i\\$\n","\n","---\n","To stabilize the learning process of self-attention, multi-head attention is employed\n","\n","---\n","\n"],"metadata":{"id":"9EkoRP-VfpcQ"}},{"cell_type":"markdown","metadata":{"id":"z13mHsOIp-MB"},"source":["### 2.2 Multi-head Attention Layers"]},{"cell_type":"markdown","metadata":{"id":"7R1_FC3pp-MB"},"source":["To enhance the expressive power and robustness of GATs, the concept of **multi-head attention** is introduced. Instead of performing a single attention calculation, $K$ independent attention mechanisms (or \"heads\") are run in parallel. Each head learns a different set of attention parameters ($\\vec{a}^k$) and performs the transformation with its own weight matrix ($W^k$).\n","<img src = \"https://i.postimg.cc/X7m7RNMZ/MAM.png\">\n","\n","For each head $k$, we compute a set of attention-weighted features $h'_i{}^{(k)}$:\n","$$h'_i{}^{(k)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}{}^{(k)} W^k h_j\\right)$$\n","\n","<img src = \"https://i.postimg.cc/G2gxrTxw/X.png\">\n","After computing the output features for each of the $K$ heads, there are two common ways to aggregate these results:\n","\n","\n","## 1. Concatenation (for hidden layers)\n","\n","The features from all attention heads are **concatenated** to form the final output feature for node \\$i\\$. If each head outputs \\$F'\\$ features, the concatenated output will have \\$K \\times F'\\$ features:\n","\n","$$\n","h'_i = \\mathop{\\|}_{k=1}^{K} h'_i{}^{(k)}\n","$$\n","\n","This approach allows the model to learn multiple, potentially complementary, representations for each node.\n","\n","---\n","\n","## 2. Averaging (for the output layer or when keeping feature dimension constant)\n","\n","The features from all heads are **averaged**. This is typically used in the final layer of a GAT to produce a single, consolidated embedding or classification output:\n","\n","$$\n","h'_i = \\frac{1}{K} \\sum_{k=1}^{K} h'_i{}^{(k)}\n","$$\n","\n","---\n","\n","\n","Multi-head attention allows the model to capture diverse relationships and dependencies within the graph, making it more robust to noisy or irrelevant connections. It effectively stabilizes the learning process and often leads to better performance."]},{"cell_type":"markdown","metadata":{"id":"0_SHrVW5p-MB"},"source":["### 2.3 Comparison to GCNs"]},{"cell_type":"markdown","metadata":{"id":"4kLVHkl_p-MB"},"source":["| Feature                 | Graph Convolutional Networks (GCNs)                           | Graph Attention Networks (GATs)                               |\n","| :---------------------- | :------------------------------------------------------------ | :------------------------------------------------------------ |\n","| **Information Aggregation** | Fixed, pre-defined aggregation based on graph structure (e.g., degree normalization). | **Learned, dynamic attention weights** for each neighbor, allowing selective aggregation. |\n","| **Weight Sharing** | Weights are shared across all nodes for feature transformation. | Weights for feature transformation ($W$) are shared across nodes. Attentional mechanism ($a$) weights are also shared. |\n","| **Inductive Capability** | Inherently inductive as weights are shared, but attention mechanism adds more flexibility. | **Strongly inductive** due to shared attention mechanism, easily generalizes to unseen graph structures. |\n","| **Computational Cost** | Typically less complex, matrix multiplications with sparse adjacency. | Higher computational cost due to attention coefficient calculation for each edge, especially in dense graphs. $\\mathcal{O}(VF' + EA_F)$ vs $\\mathcal{O}((V+E)F')$ |\n","| **Interpretation** | Less direct interpretation of neighbor importance.            | **Attention coefficients offer interpretability** on which neighbors are more important. |\n","| **Performance** | Good performance on many tasks.                               | Often achieve **state-of-the-art performance**, especially on transductive and inductive tasks.\n","| **Local Information** | Aggregates information from immediate neighbors.              | Can selectively focus on relevant neighbors, potentially capturing more nuanced local structures.\n","\n","**Key Advantages of GATs over GCNs:**\n","\n","* **Inductive Learning:** GATs are inherently better suited for inductive learning tasks (generalizing to unseen nodes or entire graphs) because their attention mechanism does not depend on the global graph structure. The attention weights are computed on-the-fly based on node features, making them highly transferable.\n","* **Varying Neighbor Importance:** GATs overcome the limitation of GCNs where all neighbors contribute equally (or based on pre-defined structural coefficients like degree). They can learn to assign different importance to different neighbors, which is crucial for heterogeneous graphs or graphs with varying relational strengths.\n","* **Interpretability:** The learned attention coefficients provide a level of interpretability, indicating which neighbors are more influential in forming a node's representation. This can be valuable for understanding the model's decisions.\n","* **Robustness to Noise:** By focusing on more relevant neighbors and down-weighting less relevant ones, GATs can be more robust to noisy or spurious connections in the graph."]},{"cell_type":"markdown","metadata":{"id":"cJ6HRIAWp-MB"},"source":[]},{"cell_type":"markdown","source":["\n","---\n","Example\n","* **Graph:**\n","\n","  * Node **A** with neighbors **B** and **C**\n","* **Feature dimension (input):** \\$F = 2\\$\n","* **Feature dimension (output per head):** \\$F' = 1\\$\n","* **Number of heads:** \\$K = 2\\$\n","\n","Suppose:\n","\n","$$\n","\\vec{h}_A = [1, 2]^T,\\quad\n","\\vec{h}_B = [0, 1]^T,\\quad\n","\\vec{h}_C = [1, 1]^T.\n","$$\n","\n","---\n","\n","##1. Linear Transformation\n","\n","Each node’s feature is transformed by a learnable **weight matrix** \\$W\\$.\n","\n","Suppose for **head 1**:\n","\n","$$\n","W^{(1)} = [0.5, 1.0].\n","\\quad (\\text{shape: } 1 \\times 2)\n","$$\n","\n","So the transformed features for head 1 are:\n","\n","$$\n","W^{(1)} \\vec{h}_A = 0.5(1) + 1.0(2) = 2.5 \\\\\n","W^{(1)} \\vec{h}_B = 0.5(0) + 1.0(1) = 1.0 \\\\\n","W^{(1)} \\vec{h}_C = 0.5(1) + 1.0(1) = 1.5\n","$$\n","\n","---\n","\n","## 2. Attention Coefficient (Self-Attention)\n","\n","For head 1, suppose the learnable vector \\$\\vec{a}^{(1)} = \\[1.0, 1.0]^T\\$ (shape \\$2 \\times 1\\$).\n","\n","**Step 1:** Concatenate the transformed features for each pair.\n","\n","$$\n","[W^{(1)} \\vec{h}_A \\parallel W^{(1)} \\vec{h}_B] = [2.5, 1.0] \\\\\n","[W^{(1)} \\vec{h}_A \\parallel W^{(1)} \\vec{h}_C] = [2.5, 1.5]\n","$$\n","\n","**Step 2:** Compute raw scores:\n","\n","$$\n","e_{AB} = \\vec{a}^{(1)T} [2.5, 1.0]^T = 2.5 + 1.0 = 3.5 \\\\\n","e_{AC} = \\vec{a}^{(1)T} [2.5, 1.5]^T = 2.5 + 1.5 = 4.0.\n","$$\n","\n","Apply **LeakyReLU** (with \\$\\alpha = 0.2\\$):\n","\n","$$\n","\\text{LeakyReLU}(x) = \\begin{cases} x, & x > 0 \\\\ 0.2x, & x < 0 \\end{cases}\n","\\quad \\Longrightarrow \\quad\n","\\text{LeakyReLU}(3.5) = 3.5, \\quad \\text{LeakyReLU}(4.0) = 4.0.\n","$$\n","\n","---\n","\n","## 3. Normalize (Softmax)\n","\n","$$\n","\\alpha_{AB} = \\frac{\\exp(3.5)}{\\exp(3.5) + \\exp(4.0)} = \\frac{33.12}{33.12 + 54.60} = 0.38. \\\\\n","Similary, \\alpha_{AC} = 0.62.\n","$$\n","\n","So, for head 1: Node A will pay **38% attention** to B and **62% attention** to C.\n","\n","---\n","\n","## 4. Aggregation\n","\n","Aggregate neighbor features for A:\n","\n","$$\n","h'_A{}^{(1)} = \\sigma\\big( \\alpha_{AB} W^{(1)} \\vec{h}_B + \\alpha_{AC} W^{(1)} \\vec{h}_C \\big) \\\\\n","= \\sigma\\big( 0.38(1.0) + 0.62(1.5) \\big) \\\\\n","= \\sigma(0.38 + 0.93) = \\sigma(1.31).\n","$$\n","\n","Suppose \\$\\sigma\\$ is identity → \\$h'\\_A{}^{(1)} = 1.31\\$.\n","\n","---\n","\n","## 5.  Multi-Head Attention\n","\n","Now do the same for **head 2** with different weights.\n","\n","Suppose:\n","\n","$$\n","W^{(2)} = [1.0, -1.0], \\quad \\vec{a}^{(2)} = [1.0, 1.0]^T.\n","$$\n","\n","* \\$W^{(2)} \\vec{h}\\_A = 1(1) + (-1)(2) = -1\\$\n","* \\$W^{(2)} \\vec{h}\\_B = 1(0) + (-1)(1) = -1\\$\n","* \\$W^{(2)} \\vec{h}\\_C = 1(1) + (-1)(1) = 0\\$\n","\n","Then:\n","\n","$$\n","[W^{(2)} \\vec{h}_A \\parallel W^{(2)} \\vec{h}_B] = [-1, -1] \\\\\n","[W^{(2)} \\vec{h}_A \\parallel W^{(2)} \\vec{h}_C] = [-1, 0]\n","$$\n","\n","Dot products:\n","\n","$$\n","e_{AB}^{(2)} = (-1) + (-1) = -2,\\quad e_{AC}^{(2)} = (-1) + 0 = -1.\n","$$\n","\n","LeakyReLU:\n","\n","$$\n","\\text{LeakyReLU}(-2) = 0.2(-2) = -0.4,\\quad \\exp(-0.4) = 0.67 \\\\\n","\\text{LeakyReLU}(-1) = 0.2(-1) = -0.2,\\quad \\exp(-0.2) = 0.82.\n","$$\n","\n","Normalized:\n","\n","$$\n","\\alpha_{AB}^{(2)} = \\frac{0.67}{0.67 + 0.82} = 0.45, \\quad\n","\\alpha_{AC}^{(2)} = 0.55.\n","$$\n","\n","Aggregate:\n","\n","$$\n","h'_A{}^{(2)} = \\alpha_{AB}^{(2)} W^{(2)} \\vec{h}_B + \\alpha_{AC}^{(2)} W^{(2)} \\vec{h}_C \\\\\n","= 0.45(-1) + 0.55(0) = -0.45.\n","$$\n","\n","---\n","\n","## Combine Heads\n","\n","* If it’s a **hidden layer** → **concatenate**:\n","\n","  $$\n","  h'_A = [\\, 1.31 \\; || \\; -0.45 \\,].\n","  $$\n","\n","* If it’s an **output layer** → **average**:\n","\n","  $$\n","  h'_A = \\frac{1}{2} (1.31 + (-0.45)) = 0.43.\n","  $$\n","\n","---\n","\n","\n","\n","* **How self-attention scores are computed** (dot product + nonlinearity)\n","* **How multiple heads learn different views** (different \\$W\\$ and \\$\\vec{a}\\$)\n","* **How they’re combined** (concat or average)\n","\n","---\n"],"metadata":{"id":"3ZrJFDXeqpJ0"}},{"cell_type":"markdown","source":["## Reference Materials\n","\n","For a practical deep dive and implementation details, refer to the following tutorial, which often serves as a foundational example for GATs:\n","\n","* **GAT Node Classification Tutorial (PyTorch Geometric):** This official tutorial provides a hands-on example of implementing and training a GAT for node classification. It's an excellent resource for understanding the practical aspects of GATs.\n","    * [PyTorch Geometric GAT Node Classification Example](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAT.html) (Check PyTorch Geometric documentation for current links and tutorials, a good starting point is usually their examples folder or the specific module documentation for `GATConv`).\n","    * You might also find relevant examples in their GitHub repository: [PyTorch Geometric GitHub Examples](https://github.com/pyg-team/pytorch_geometric/tree/master/examples)\n","\n","**Original Paper:**\n","\n","* **Graph Attention Networks** by Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio. (2018). Available on arXiv: [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)"],"metadata":{"id":"HzsTBTg2r_ik"}},{"cell_type":"code","source":[],"metadata":{"id":"sdWTwsKosALo"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}