{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Introduction to Self-Supervised Learning and Constrastive Learning**\n","\n","### Prerequisites\n","\n","- Familiarity with self-supervised learning (SSL) concepts\n","- Basic understanding of deep learning, particularly CNN architectures"],"metadata":{"id":"wfFseaK2uIBP"}},{"cell_type":"markdown","source":["##**What is Self-Supervised Learning?**\n","\n","Traditional supervised learning relies heavily on large labeled datasets, which are expensive and time-consuming to create. In contrast, **self-supervised learning (SSL)** aims to harness the abundance of **unlabeled data** by generating supervisory signals directly from the data itself. Think of it like solving a puzzle: the model learns by finding patterns in the data without being told the answers.\n","\n","**Contrastive learning** is a powerful subset of SSL that teaches models to recognize similarities and differences in data, much like how a child learns to match objects in a game. It has shown impressive results in fields like computer vision (example image classification) and natural language processing (example sentence embeddings).\n","\n","\n","<img src=\"https://amitness.com/posts/images/contrastive-find-a-pair.png\" width=\"45%\" /> &nbsp; <img src=\"https://amitness.com/posts/images/contrastive-puzzle.gif\" width=\"45%\" />\n","\n","Imagine a game where as kids, we matched a cat on the left with the same cat hidden among other animals on the right by recognizing its features. Contrastive learning mimics this process: it teaches models to bring similar items (example two images of the same cat) closer together in a feature space while pushing dissimilar items (example like a cat and a dog) farther apart. Over time, the model learns meaningful patterns without explicit labels\n","\n","**Example:** Imagine sorting a photo library. You group similar photos of the same person together while separating them from photos of others. Contrastive learning does this by bringing similar items (like two views of the same image) closer in a feature space and pushing dissimilar items (like images of different objects) apart."],"metadata":{"id":"9tA2-kBxuQDc"}},{"cell_type":"markdown","source":["## **Core Idea**\n","\n","The core idea behind **contrastive learning** is to learn a feature space where semantically **similar samples (positives)** are close together, while **dissimilar samples (negatives)** are pushed far apart. This encourages models to focus on meaningful patterns and structures in data, leading to robust and generalizable representations.\n","\n","Imagine mapping data points into a high-dimensional latent space:\n","\n","* For each **anchor** sample, its **positive pair** (e.g., an augmented version or a semantically similar item) should have a close representation.\n","* Meanwhile, **negative pairs** (different or unrelated samples) should be positioned far away.\n","\n","Success in contrastive learning depends on carefully selecting and designing these pairs:\n","\n","* **Positive pairs**: Different views of the same underlying data point, often created via augmentation (like rotation or color jitter) or semantic similarity (e.g., paraphrased sentences in NLP).\n","* **Negative pairs**: Samples unrelated to the anchor, often randomly selected from the dataset or batch."],"metadata":{"id":"RVL8hsLJ6nG0"}},{"cell_type":"markdown","source":["## **Common Strategies**\n","\n","* **SimCLR(A Simple Framework for Contrastive Learning of Visual Representations)**: Generates positive pairs via **strong augmentations** and uses all other samples in a batch as negatives.\n","\n","* **MoCo(Momentum Contrast for Unsupervised Visual Representation Learning)**: Maintains a dynamic memory bank of negative samples for stable training.\n","\n","* **BYOL(Bootstrap Your Own Latent) and SimSiam**: Avoid using negative pairs entirely by relying on asymmetry and momentum encoders.\n","\n","By contrasting the right kinds of pairs, models can learn to represent meaningful structures in data without human-provided labels."],"metadata":{"id":"zPqK5QJ1wuDr"}},{"cell_type":"markdown","source":["## **Supervised and Semi-Supervised Contrastive Learning**\n","\n","After the success of **self-supervised contrastive learning** methods like **SimCLR** and **MoCo**, researchers began exploring ways to **enhance contrastive learning using available label information**. This led to the development of **Supervised Contrastive Learning (SCL)**, which integrates class labels into the contrastive framework to form richer and more semantically meaningful positive pairs.\n","\n","At the same time, the community recognized that in many real-world scenarios, **only a small portion of the data is labeled**, while a large pool of unlabeled data is available. To address this, **Semi-Supervised Contrastive Learning (SSCL)** emerged as a natural progressionâ€”**combining the strengths of both supervised and self-supervised methods**.\n","\n","These approaches reflect the growing trend of building flexible learning paradigms that can operate across different levels of supervision, allowing models to better generalize and adapt to the data at hand.\n","\n","\n","## **Supervised Contrastive Learning (SCL)**\n","\n","**Supervised Contrastive Learning** extends the standard contrastive learning paradigm by utilizing **class label information** to define positive and negative pairs more effectively.\n","\n","In SCL, for a given **anchor**, all samples **from the same class** are considered **positives**, while those from **different classes** are treated as **negatives**. This allows the model to learn class-discriminative features directly through the contrastive objective.\n","\n","> For instance, for anchor sample labeled \"cat\":\n",">\n","> * **Positive samples** = all other \"cat\" images (not just augmented versions)\n","> * **Negative samples** = images labeled with any other class (e.g., dog, car)\n","\n","#### Benifits:\n","\n","* Produces compact class-specific clusters in the feature space\n","* Often surpasses traditional cross-entropy classifiers, especially in noisy or imbalanced settings\n","\n","> ðŸ“„ Introduced in the paper: **\"Supervised Contrastive Learning\" (Khosla et al., NeurIPS 2020)**\n","\n","\n","## **Semi-Supervised Contrastive Learning (SSCL)**\n","\n","**Semi-Supervised Contrastive Learning** builds on both self-supervised and supervised methods, offering a powerful strategy when **labeling is expensive or limited**.\n","\n","SSCL applies:\n","\n","* A **supervised contrastive loss** to the small set of labeled data\n","* A **self-supervised contrastive loss** (e.g., SimCLR) to the large set of unlabeled data\n","\n","This results in a **hybrid loss function**:\n","\n","$$\n","\\mathcal{L}_{\\text{total}} = \\lambda_1 \\mathcal{L}_{\\text{supervised}} + \\lambda_2 \\mathcal{L}_{\\text{self-supervised}}\n","$$\n","\n","Where $\\lambda_1$ and $\\lambda_2$ are weighting coefficients that balance the two components.\n","\n","### Benefits:\n","\n","* Leverages structure in unlabeled data while benefiting from label guidance\n","* Achieves strong performance even with limited supervision\n","* Adaptable to real-world scenarios where labeled data is scarce\n","\n"],"metadata":{"id":"aKADVr0h5Fr3"}},{"cell_type":"markdown","source":["## **Applications and Limitations**\n","\n","### **Applications**\n","Contrastive learning has a wide range of applications:\n","\n","* **Computer Vision**: Pre-training models for tasks like image classification, object detection, or segmentation (e.g., SimCLR pre-trained models fine-tuned for medical imaging).\n","\n","* **Natural Language Processing**: Learning sentence embeddings for tasks like text classification or translation (e.g., Sentence-BERT uses contrastive learning).\n","\n","*  **Multimodal Learning**: Aligning images and text (example CLIP for image caption matching).\n","\n","*  **Real-World Use Cases**: Improving recommendation systems, fraud detection, or autonomous driving by learning robust representations from diverse data.\n","\n","## **Limitations**\n","While contrastive learning is powerful, it faces several challenges:\n","\n","*   **Computationally Intensive**: Requires large batches or memory banks (example MoCo) for effective training, demanding significant computational resources.\n","\n","*   **Augmentation Dependency**: Performance heavily relies on choosing appropriate augmentations. Poor augmentations can lead to weak representations.\n","\n","*   **Negative Pair Challenges**: Selecting meaningful negatives is tricky; random negatives may include false negatives (example two cats mislabeled as different).\n","\n","*   **Scalability**: Applying contrastive learning to very large datasets or complex tasks (example, 3D data) can be challenging.\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"cbGVdBEMF8gK"}}]}