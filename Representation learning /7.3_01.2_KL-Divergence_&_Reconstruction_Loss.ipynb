{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# **Introduction to KL Divergence and Reconstruction Loss**\n","\n","## Pre-requisite\n","\n","Before starting this lesson, the students must have knowledge on:\n","- Probability\n","- Bayes' Theorem\n","\n","\n","\n","\n","\n"],"metadata":{"id":"lJVQwjhkeGsU"}},{"cell_type":"markdown","metadata":{"id":"9-MspFUL3-As"},"source":["### **Information**\n","\n","Suppose we want to measure how much information a sentence carries. In real life:\n","\n","- If a sentence is **unique or rare**, it carries **more information**.  \n","- If it is **common or obvious**, it carries **less information**.\n","\n","For example:  \n","* \"**The Sun rises in the East**\" — very common → **low information**  \n","* \"**The President will get killed tomorrow**\" — rare and shocking → **high information**\n","\n","**Information** quantifies how *unexpected* an event is. Less likely events carry more information.\n","\n","Mathematically,\n","\n","$$\n","I(x) = -\\log P(x)\n","$$\n","\n","> As $P(x) \\to 0$, $I(x)$ increases. If $P(x) = 1$, then $I(x) = 0$, meaning a certain event carries no new information.\n"]},{"cell_type":"markdown","source":["\n","### **Entropy (Shannon Entropy)**\n","\n","Entropy can be interpreted as the **average level of information** (or surprise or uncertainty) across all  possible outcomes.\n","\n","* Think of it as the **expected information(surprise)** from a distribution.\n","* Higher randomness = higher entropy.\n","\n","$$\n","H(P) = -\\sum_i P_i(x) \\log P_i(x)\n","$$\n","\n","$$\n","\\text{or}\n","$$\n","\n","$$\n","H(P) = -\\mathbb{E}_{P}[ \\log P(x) ]\n","$$\n","\n","\n"],"metadata":{"id":"wvzkdnapx0pa"}},{"cell_type":"markdown","source":["### **Cross-Entropy**\n","\n","Cross-entropy compares two distributions:\n","\n","* True distribution $P$\n","* Estimated model distribution $q$\n","\n","$$\n","H(P, q) = -\\sum_i P_i(x) \\log q_i(x)\n","$$\n","\n","> Commonly used as a **loss function** in machine learning to measure how close predictions are to ground truth.\n"],"metadata":{"id":"SJ_UAgZVx2rO"}},{"cell_type":"markdown","metadata":{"id":"bgDiw7Za3_ij"},"source":["## **Kullback-Leibler Divergence (KL Divergence)**\n","\n","Also known as **Relative Entropy**. KL-divergence shows the dissimilarity between two distributions. Say two distributions $P$ and $q$, KL-Divergence of $P$ w\\.r.t. $q$ is given as:\n","\n","$$\n","\\mathbb{KL}(P||q) = H(P, q)\\ (\\text{cross-entropy})\\ -\\ H(P)\\ (\\text{entropy})\n","$$\n","\n","$$\n","\\mathbb{KL}(P||q) = -\\sum P(x) \\log q(x) + \\sum P(x) \\log P(x)\n","$$\n","\n","$$\n","\\mathbb{KL}(P||q) = -\\sum P(x) \\log \\frac{q(x)}{P(x)} \\tag{6}\n","$$\n","\n","Things to note about KL-Divergence:\n","\n","* The value of KL-Divergence is always greater or equal to zero, $\\mathbb{KL} \\geq 0$. The value of KL-divergence equals zero if the two distributions are the same because $\\log(1) = 0$\n","* KL-divergence is non-symmetric in nature, that is, $\\mathbb{KL}(P||q) \\neq \\mathbb{KL}(q||P)$\n","\n","> Note: The summation terms are converted to integrals for continuous data."]},{"cell_type":"markdown","source":["## **Reconstruction Loss**\n","\n","Reconstruction loss measures how well a model can recreate the original input from its internal representation. It is commonly used in models like **autoencoders** and **variational autoencoders (VAEs)**.\n","\n","For input $x$ and its reconstruction $\\hat{x}$, common reconstruction loss functions include:\n","\n","* **Mean Squared Error (MSE):**\n","\n","$$\n","\\mathcal{L}_{\\text{MSE}} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n","$$\n","\n","* **Binary Cross-Entropy (BCE):**\n","\n","$$\n","\\mathcal{L}_{\\text{BCE}} = -\\sum_i \\left[x_i \\log \\hat{x}_i + (1 - x_i) \\log(1 - \\hat{x}_i)\\right]\n","$$\n","\n","In **VAEs**, reconstruction loss represents the negative log-likelihood of the observed data under the decoder’s output distribution:\n","\n","$$\n","\\mathcal{L}_{\\text{recon}} = -\\mathbb{E}_{q(z|x)} \\left[\\log p(x|z)\\right]\n","$$\n","\n","It encourages the model to preserve important information about the input during encoding and accurately regenerate it during decoding.\n"],"metadata":{"id":"B883DM25nP_L"}}]}