{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Objectives**\n",
        "- To grasp the concept of bidirectional context in language understanding.\n",
        "- To learn about Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) as BERT‚Äôs pre-training tasks.\n",
        "- To understand BERT‚Äôs architecture and how it leverages bidirectionality.\n",
        "- To explore how BERT is fine-tuned for downstream NLP tasks such as classification, NER, and QA.\n"
      ],
      "metadata": {
        "id": "CBx9byUt4QeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "##  **Bidirectional Context in BERT**\n",
        "Traditional models like GPT are unidirectional (left-to-right). BERT is **bidirectional**, meaning it understands context from both directions.\n",
        "\n",
        "Example:\n",
        "For the sentence: ‚ÄúThe bank was flooded after the storm.‚Äù\n",
        "\n",
        "Unidirectional models might not understand if \"bank\" means **financial institution** or **riverbank**. BERT considers both sides of the word and better captures meaning.\n"
      ],
      "metadata": {
        "id": "nNIep0dq5Jkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  **Introduction to BERT**\n",
        "\n",
        "Before BERT, most language models could only read text in one direction‚Äîeither from left to right or right to left. But many tasks in Natural Language Processing (NLP), like **question answering** or **named entity recognition**, need context from **both directions** to fully understand the meaning of words and sentences.\n",
        "\n",
        "Some earlier models, like **ELMo**, tried to fix this using two separate models‚Äîone reading left to right and one right to left. But it wasn‚Äôt ideal.\n",
        "\n",
        "That‚Äôs where **BERT** comes in.  \n",
        "BERT stands for **Bidirectional Encoder Representations from Transformers**.\n",
        "\n",
        "It introduced a new way to train language models using:\n",
        "\n",
        "1. **Masked Language Modeling (MLM)** ‚Äì randomly hiding words and asking the model to guess them.\n",
        "2. **Next Sentence Prediction (NSP)** ‚Äì helping the model understand the relationship between two sentences.\n",
        "\n",
        "BERT uses a **transformer encoder** that looks at both the left and right sides of a word at the same time. This is called a **bidirectional** approach and makes BERT much better at understanding context.\n",
        "\n",
        "After pre-training on large amounts of text (like Wikipedia), BERT can be fine-tuned on smaller datasets for tasks like:\n",
        "\n",
        "- Text classification  \n",
        "- Paraphrase detection  \n",
        "- Named entity recognition  \n",
        "- Question answering\n",
        "\n",
        "All you need is to add a small output layer on top of BERT and train it for your task.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.postimg.cc/LsSVbTgj/image.png\" width=\"500\"/>\n",
        "\n",
        "**Figure 1**: Different model directions:  \n",
        "(a) Left-to-right (unidirectional)  \n",
        "(b) Combined left and right separately (still unidirectional)  \n",
        "(c) True bidirectional (BERT's approach)\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rrSUIt2p5uxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "###  **BERT Achievements**\n",
        "\n",
        "BERT made a big impact when it was released. It achieved **state-of-the-art results** on several NLP tasks:\n",
        "\n",
        "- **GLUE score**: 80.5% (‚Üë7.7%)\n",
        "- **MultiNLI accuracy**: 86.7% (‚Üë4.6%)\n",
        "- **SQuAD v1.1 (F1 score)**: 93.2 (‚Üë1.5)\n",
        "- **SQuAD v2.0 (F1 score)**: 83.1 (‚Üë5.1)\n",
        "\n",
        "----\n",
        "\n",
        "###  **Real-World Impact**\n",
        "\n",
        "Google adopted BERT into its **Search engine** to better understand the meaning behind search queries. In 2020, Google announced that **almost every English search query** was being processed by BERT, and the model has since been expanded to many other languages.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-E9i3mPgdjBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "##  **Working of BERT**\n",
        "\n",
        "BERT is built on top of the **Transformer encoder architecture**, introduced in the paper [‚ÄúAttention is All You Need‚Äù](https://arxiv.org/abs/1706.03762). Unlike traditional models that read text from left to right (or right to left), BERT reads the **entire sequence in both directions at once** using **self-attention**. This allows it to understand the full context of a word based on the words before and after it.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8w1yPWPieDJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##  **Model Architecture**\n",
        "\n",
        "BERT uses **multiple stacked Transformer encoder layers** to process text:\n",
        "\n",
        "- **BERT-Base**: 12 layers, 768 hidden units, 12 attention heads\n",
        "- **BERT-Large**: 24 layers, 1024 hidden units, 16 attention heads\n",
        "\n",
        "Each layer contains:\n",
        "- **Multi-head self-attention mechanism**\n",
        "- **Feed-forward neural network**\n",
        "- **Layer normalization and residual connections**\n",
        "\n",
        "These components allow BERT to capture relationships between words in a sentence, regardless of their position or distance.\n",
        "\n"
      ],
      "metadata": {
        "id": "KBMQD-hleG1z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfab951"
      },
      "source": [
        "###  **Attention Mechanism Visualization with BERTViz**\n",
        "\n",
        "BERT uses a **self-attention mechanism** to understand the relationship between words in a sentence. This allows the model to focus on relevant words when processing each token.\n",
        "\n",
        "To visualize how attention works in BERT, we can use [**BERTViz**](https://github.com/jessevig/bertviz) ‚Äî a visualization tool that shows attention patterns across layers and attention heads.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example Input**\n",
        "\n",
        "`[CLS] The rabbit quickly hopped [SEP] the turtle slowly crawled [SEP]`\n",
        "\n",
        "Below is a **head view** visualization from BERTViz, which shows attention weights between tokens:\n",
        "\n",
        "![Head View](https://raw.githubusercontent.com/jessevig/bertviz/master/images/head-view.gif)\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use BERTViz?**\n",
        "\n",
        "- Understand which words the model attends to\n",
        "- Gain insights into BERT‚Äôs decision-making\n",
        "- Useful for debugging and model interpretability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "###  **Input Format**\n",
        "\n",
        "In BERT, the term ‚Äúsentence‚Äù refers to any span of continuous text, and a ‚Äúsequence‚Äù refers to the tokenized input.\n",
        "\n",
        "The input to BERT always follows a specific format:\n",
        "\n",
        "- The first token is always **[CLS]** (used for classification tasks)\n",
        "- If there are two sentences, they are separated by **[SEP]**\n",
        "- The input also includes:\n",
        "  - **Token embeddings**: representations of each word/subword\n",
        "  - **Segment embeddings**: to distinguish sentence A from sentence B\n",
        "  - **Position embeddings**: to keep track of token positions\n",
        "\n",
        "Example:\n",
        "\n",
        "All these embeddings are added together and passed into the encoder layers.\n",
        "\n"
      ],
      "metadata": {
        "id": "NAosw3PReLXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "###  **Contextual Embeddings**\n",
        "\n",
        "BERT creates **contextual embeddings**, meaning that the representation of a word depends on its surrounding words.\n",
        "\n",
        "For example:\n",
        "- ‚Äúbank‚Äù in ‚Äúriver bank‚Äù will have a different embedding than ‚Äúbank‚Äù in ‚Äúsavings bank‚Äù.\n",
        "\n",
        "This is in contrast to traditional word embeddings like Word2Vec or GloVe, where a word has the same embedding no matter where it appears.\n",
        "\n",
        "Because BERT considers both the left and right context of a word, its embeddings are much more powerful and flexible."
      ],
      "metadata": {
        "id": "XVDPwmjGeWWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "----\n",
        "\n",
        "\n",
        "###  **Outputs from BERT**\n",
        "\n",
        "After processing the input through all encoder layers, BERT produces:\n",
        "\n",
        "- A contextual embedding for **each token** in the input\n",
        "- A special embedding for the **[CLS]** token that represents the entire sequence (useful for classification tasks)\n",
        "\n",
        "These outputs can then be fine-tuned for various downstream NLP tasks such as sentiment analysis, named entity recognition, question answering, etc.\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.postimg.cc/28yLm4F5/image.png\" width=\"600\"/>\n",
        "\n",
        "**Figure 2**: BERT Model Architecture ‚Äî showing input tokens, special tokens like [CLS] and [SEP], and the stacked Transformer encoders.\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IIoNzWyXS2ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  **Pretraining**\n",
        "\n",
        "The first step in BERT‚Äôs process is **unsupervised pretraining**. The goal here is to teach the model a deep understanding of language by learning from a large amount of unlabeled text.\n",
        "\n",
        "BERT uses two main training tasks during pretraining:\n",
        "\n",
        "- **Masked Language Model (MLM)**  \n",
        "- **Next Sentence Prediction (NSP)**\n",
        "\n",
        "Let‚Äôs take a quick look at what these mean.\n"
      ],
      "metadata": {
        "id": "Z5FrVILTT2KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* * *\n",
        "## **1. Masked Language Model (MLM)**\n",
        "\n",
        "Before we dive into MLM, try to fill in the blank in this sentence:\n",
        "\n",
        "<center> **We went to the library to ____ books.** </center>\n",
        "\n",
        "You might guess words like *read*, *study*, or *borrow*‚Äîbut probably not *play*. This kind of test is called a **cloze test** (or occlusion test). To get the right answer, you need to understand the context and the meaning of words around the blank.\n",
        "\n",
        "The **Masked Language Model** is based on this idea.\n",
        "\n",
        "During BERT‚Äôs pretraining, some words in the input are randomly selected and replaced with a special **[MASK]** token or other tokens. The model‚Äôs job is to predict the original words that were masked out.\n",
        "\n",
        "Specifically, 15% of the tokens in each sequence are randomly selected. Of these 15% selected tokens:\n",
        "\n",
        "- **80%** are replaced with the `[MASK]` token.\n",
        "- **10%** are replaced with a random token from the vocabulary.\n",
        "- **10%** are left unchanged.\n",
        "\n",
        "This strategy prevents the model from simply learning to predict `[MASK]` tokens and encourages it to rely on the surrounding context.\n",
        "\n",
        "For example, consider the sentence:\n",
        "\n",
        "<center> \"The **man** went to the **store**.\" </center>\n",
        "\n",
        "If we mask \"store\", the input might become:\n",
        "\n",
        "<center> \"The man went to the **[MASK]**.\" </center>\n",
        "\n",
        "The model tries to guess the missing word based on the surrounding words.\n",
        "\n",
        "Instead of predicting the entire sequence, BERT only predicts these masked words. It uses the hidden representations of the masked tokens and applies a softmax function over the vocabulary to predict the most likely word. The loss is only calculated for the masked tokens.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.postimg.cc/MHTH6SBL/image.png\" width=\"500\"/>\n",
        "\n",
        "**Figure 3:** Masked Language Model (MLM) ‚Äî predicting masked words from context.\n",
        "</center>\n",
        "\n",
        "* * *"
      ],
      "metadata": {
        "id": "3JLLR-_GUHEi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "683961ef"
      },
      "source": [
        "## **2.  Next Sentence Prediction (NSP)**\n",
        "\n",
        "While language models like MLM learn about individual words and their context, they don‚Äôt directly capture the relationship between **sentences**. To address this, BERT uses a task called **Next Sentence Prediction** during pretraining.\n",
        "\n",
        "This task is designed to help BERT understand the relationship between two sentences by training it on a **binary classification problem**. For each pre-training example, two sentences, Sentence A and Sentence B, are constructed.\n",
        "\n",
        "- In 50% of the examples, Sentence B **actually follows** Sentence A in the original text. These pairs are labeled as **IsNext**.\n",
        "- In the other 50%, Sentence B is a **random sentence** taken from a different document. These pairs are labeled as **NotNext**.\n",
        "\n",
        "BERT's objective is to learn to predict whether Sentence B logically follows Sentence A based on the context provided by both sentences.\n",
        "\n",
        "To perform this prediction, the input to BERT is formatted as: `[CLS] Sentence A [SEP] Sentence B [SEP]`. The special **[CLS]** token's final hidden state is used as the aggregate representation of the entire sequence pair. This vector is then passed through a simple classifier (a linear layer followed by a softmax function) to predict either **IsNext** or **NotNext**. The model is trained to minimize the cross-entropy loss on this binary classification task.\n",
        "\n",
        "<center>\n",
        "<figure>\n",
        "<img src=\"https://i.postimg.cc/43QxpdbM/image.png\" width=\"550\"/>\n",
        "<figcaption>(a) IsNext example: Sentence B follows Sentence A.</figcaption>\n",
        "</figure>\n",
        "<figure>\n",
        "<img src=\"https://i.postimg.cc/sXnf0NL4/image.png\" width=\"550\"/>\n",
        "<figcaption>(b) NotNext example: Sentence B is random, unrelated to Sentence A.</figcaption>\n",
        "</figure>\n",
        "\n",
        "**Figure 4:** Next Sentence Prediction (NSP) ‚Äî (a) IsNext case, (b) NotNext case.\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* * *\n",
        "## **Finetuning**\n",
        "\n",
        "One of BERT‚Äôs most powerful features is its ability to **transfer the knowledge** it learned during pretraining to many different NLP tasks quickly and effectively. This process is called **finetuning**.\n",
        "\n",
        "We start with a **pre-trained BERT model** and then train it a little more on a specific task, like sentiment analysis, named entity recognition (NER), or question answering. Sometimes, we add a small task-specific layer on top of BERT‚Äôs output to help with that task.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.postimg.cc/ZYMK8XpT/image.png\" width=\"750\"/>\n",
        "\n",
        "**Figure 5:** BERT Framework ‚Äî (a) Pretraining on MLM & NSP, (b) Finetuning for different NLP tasks (e.g., MNLI, NER, SQuAD).\n",
        "</center>\n",
        "\n",
        "During finetuning, **all layers** of BERT are trained together with this task-specific layer, usually for just a few epochs. This allows the model to quickly adapt and perform well on the new task.\n",
        "---\n",
        "## **Handling [MASK] during Fine-tuning:**\n",
        "\n",
        "It's important to note that the `[MASK]` token, used during the MLM pre-training task, is **not** used during fine-tuning. The model has already learned rich contextual representations of words by predicting masked tokens during pre-training. In the fine-tuning phase, these learned representations (the hidden states of the original tokens) are used as input to the task-specific output layer for the downstream task. The model no longer performs the masking and prediction of individual words; instead, it uses the full sequence context to solve the target task.\n",
        "\n",
        "----\n",
        "### **Example: Finetuning BERT for Named Entity Recognition (NER)**\n",
        "\n",
        "In the NER task, BERT produces contextual embeddings for every token. These token embeddings are passed through a **feed-forward network (FFN)** followed by a **softmax layer** to classify each token into categories like person, location, organization, or none.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=18G20zWoN8nJTMMlTCM--ojkSJfLNFRe4\" width=\"700\"/>\n",
        "\n",
        "**Figure 6:** Fine-tuning BERT for the NER task\n",
        "</center>"
      ],
      "metadata": {
        "id": "zGQZORzUexO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "----\n",
        "\n",
        "###  **Finetuning for Different NLP Tasks**\n",
        "\n",
        "The input format during finetuning depends on the task but often follows the same structure as in pretraining:\n",
        "\n",
        "- The two sentences (sentence A and sentence B) from pretraining can represent:\n",
        "  - Sentence pairs in **paraphrasing**\n",
        "  - Hypothesis-premise pairs in **natural language inference (entailment)**\n",
        "  - Question and passage pairs in **question answering**\n",
        "  - Or just a single sentence paired with a dummy input in **text classification** or **sequence tagging**\n",
        "\n",
        "The output depends on the task type:\n",
        "\n",
        "- For **token-level tasks** (e.g., NER, POS tagging, question answering), the model uses the final hidden vectors for each token and feeds them into an output layer.\n",
        "- For **sentence-level classification tasks** (e.g., sentiment analysis, entailment), the model uses the hidden vector corresponding to the **[CLS]** token and feeds it into the output layer.\n",
        "\n",
        "\n",
        "This flexible finetuning approach makes BERT suitable for a wide range of NLP problems, often achieving state-of-the-art results with relatively little additional training.\n"
      ],
      "metadata": {
        "id": "f9X944sKWeo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* * *\n",
        "## **Effect of Removing NSP: Ablation Study on MNLI and QNLI**\n",
        "\n",
        "The **Next Sentence Prediction (NSP)** task was introduced in the original BERT paper ([Devlin et al., 2018](https://arxiv.org/pdf/1810.04805.pdf)) to help the model understand relationships between sentence pairs. However, later studies and experiments questioned its necessity for achieving state-of-the-art results on various downstream tasks.\n",
        "\n",
        "To investigate the contribution of the NSP task, researchers conducted **ablation studies**. An ablation study involves removing a specific component (in this case, the NSP pre-training objective) from a model to assess its impact on performance.\n",
        "\n",
        "The original BERT paper presented results comparing models trained **with** and **without** the NSP objective on several tasks, including **Natural Language Inference (NLI)** tasks like **MNLI** and **QNLI**.\n",
        "\n",
        "### **What are MNLI and QNLI?**\n",
        "\n",
        "*   **MNLI (Multi-Genre Natural Language Inference)**: A dataset for determining the relationship between a premise sentence and a hypothesis sentence. The task is to classify the relationship as **entailment**, **contradiction**, or **neutral**. This requires the model to understand how sentence pairs relate to each other logically.\n",
        "*   **QNLI (Question-answering Natural Language Inference)**: An NLI task derived from question-answering data. Given a question and a potential answer sentence from a text, the model must determine if the sentence contains the answer (entailment) or not (not entailment). This task also relies on understanding the relationship between a question and a potential answer.\n",
        "\n",
        "These tasks are good benchmarks for evaluating a model's ability to understand sentence-pair relationships, which is what the NSP task was designed to improve.\n",
        "\n",
        "### **Ablation Study Results: With vs. Without NSP**\n",
        "\n",
        "The ablation study results on MNLI and QNLI showed:\n",
        "\n",
        "| Model Variant      | MNLI Accuracy (%) | QNLI Accuracy (%) |\n",
        "| ------------------ | ----------------- | ----------------- |\n",
        "| BERT (with NSP)    | 84.0              | 91.0              |\n",
        "| BERT (without NSP) | 83.9              | 90.7              |\n",
        "\n",
        "> *Note: Values are illustrative, based on the original BERT paper's ablation findings.*\n",
        "\n",
        "### **Interpretation and Why NSP is Sometimes Removed**\n",
        "\n",
        "The results indicate that removing the NSP task resulted in a very **marginal decrease** in performance on both MNLI and QNLI. The difference was often less than 0.5%.\n",
        "\n",
        "This led to the conclusion that, while NSP might offer a slight benefit for tasks specifically requiring sentence-pair understanding, its contribution was not as significant as initially hypothesized and was considerably less impactful than the MLM task.\n",
        "\n",
        "Later models, such as **RoBERTa**, built upon this finding by completely removing the NSP pre-training task and focusing on more robust MLM training with larger datasets and longer sequences. RoBERTa achieved state-of-the-art results across many benchmarks without NSP, further supporting the idea that explicit sentence-pair pre-training might not be strictly necessary if the model learns strong contextual representations through MLM.\n",
        "\n",
        "In summary, NSP is sometimes removed because empirical evidence showed its limited impact on overall performance, and focusing solely on a strong MLM objective proved to be a more effective pre-training strategy for many downstream NLP tasks."
      ],
      "metadata": {
        "id": "iyewDAbJzJmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###  **Examples of loading BERT model from HuggingFace and show examples of tasks like MASK filling that BERT is capable of:**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Loading BERT from Hugging Face and Performing MASK Filling**\n",
        "\n",
        "**BERT (Bidirectional Encoder Representations from Transformers)** is a powerful pretrained model developed by Google. One of its core tasks is **Masked Language Modeling (MLM)** ‚Äî predicting the masked word in a sentence.\n",
        "\n",
        "With the Hugging Face ü§ó Transformers library, it's very easy to use BERT for such tasks.\n",
        "\n",
        "---\n",
        "\n",
        "###  Step 1: Install Transformers Library\n",
        "\n",
        "```bash\n",
        "pip install transformers\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  Step 2: Load Pretrained BERT Model and Tokenizer\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load BERT with a fill-mask pipeline\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  Step 3: Example ‚Äì MASK Filling (Masked Language Modeling)\n",
        "\n",
        "```python\n",
        "# Provide a sentence with a [MASK] token\n",
        "result = unmasker(\"The capital of France is [MASK].\")\n",
        "\n",
        "# Print top predictions\n",
        "for res in result:\n",
        "    print(f\"{res['sequence']} (score: {res['score']:.4f})\")\n",
        "```\n",
        "\n",
        "####  Output:\n",
        "\n",
        "```\n",
        "The capital of France is Paris. (score: 0.97)\n",
        "The capital of France is Lyon. (score: 0.01)\n",
        "The capital of France is Marseille. (score: 0.003)\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **What's Going On?**\n",
        "\n",
        "BERT tries to predict the masked word using context from both the **left and right** of the mask token. In this example, it correctly predicts \"**Paris**\" as the missing word.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Other Tasks BERT Can Do (using Hugging Face pipelines)**\n",
        "\n",
        "| Task                           | Pipeline Name           | Example Use                            |\n",
        "| ------------------------------ | ----------------------- | -------------------------------------- |\n",
        "| Masked word filling            | `\"fill-mask\"`           | Predict `[MASK]` in a sentence         |\n",
        "| Text classification            | `\"text-classification\"` | Sentiment analysis or intent detection |\n",
        "| Named Entity Recognition (NER) | `\"ner\"`                 | Identify people, places, etc.          |\n",
        "| Question Answering             | `\"question-answering\"`  | Answer questions from context          |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Conclusion**\n",
        "\n",
        "Hugging Face Transformers make it very easy to experiment with BERT. You can quickly load the model and perform **masked language modeling** and many other NLP tasks without much setup.\n",
        "\n",
        "Let me know if you want to explore another BERT task like sentiment classification or question answering!\n"
      ],
      "metadata": {
        "id": "K9S8Ymwl385Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **Influence of BERT**\n",
        "\n",
        "Since its release, BERT has revolutionized Natural Language Processing (NLP) and inspired many follow-up models and applications.\n",
        "\n",
        "### Popular BERT Variants and Domain Adaptations\n",
        "\n",
        "- **SciBERT**  \n",
        "  Pre-trained on a large scientific publications corpus, SciBERT improves performance on scientific NLP tasks like paper classification and entity recognition.\n",
        "\n",
        "- **BioBERT**  \n",
        "  Trained on vast biomedical texts, BioBERT excels in biomedical named entity recognition, relation extraction, and question answering, outperforming previous models in the biomedical domain.\n",
        "\n",
        "- **RoBERTa**  \n",
        "  A robustly optimized BERT variant that trains longer with more data and removes the Next Sentence Prediction (NSP) task, achieving better results on many benchmarks.\n",
        "\n",
        "- **ALBERT**  \n",
        "  A ‚Äúlite‚Äù version of BERT designed to reduce model size and increase training speed, using parameter-sharing and factorized embeddings.\n",
        "\n",
        "- **DistilBERT**  \n",
        "  A smaller, faster version of BERT that retains much of its performance but is more efficient for deployment.\n",
        "\n",
        "- **ViLBERT**  \n",
        "  Extends BERT to a **multi-modal** model combining image and text understanding with two separate streams, enabling joint reasoning over vision and language.\n",
        "\n",
        "- **ClinicalBERT**  \n",
        "  Adapted for clinical notes and healthcare data, improving medical text understanding.\n"
      ],
      "metadata": {
        "id": "PMGbvJ43fD5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### **Real-World Applications of BERT**\n",
        "\n",
        "- **Search Engines**  \n",
        "  Google uses BERT to better understand search queries, improving the relevance of results.\n",
        "\n",
        "- **Chatbots and Virtual Assistants**  \n",
        "  BERT powers natural and context-aware conversations.\n",
        "\n",
        "- **Text Summarization and Translation**  \n",
        "  BERT-based models help generate more accurate summaries and translations.\n",
        "\n",
        "- **Sentiment Analysis**  \n",
        "  Widely used in social media monitoring, customer feedback analysis, and brand management.\n",
        "\n",
        "\n",
        "\n",
        "###  **Limitations and Challenges**\n",
        "\n",
        "- **Resource Intensive**  \n",
        "  Training BERT requires large computational power and memory.\n",
        "\n",
        "- **Input Length Limit**  \n",
        "  BERT can only process input sequences up to a certain length (usually 512 tokens), which limits handling very long documents.\n",
        "\n",
        "- **Inference Speed**  \n",
        "  Large models can be slow for real-time applications, prompting research into lighter versions.\n",
        "\n",
        "\n",
        "\n",
        "###  **Future Directions**\n",
        "\n",
        "- **Efficient Transformers**  \n",
        "  Models like Longformer and Performer address BERT‚Äôs input length and speed limitations by optimizing attention mechanisms.\n",
        "\n",
        "- **Multimodal Learning**  \n",
        "  Combining text with images, audio, or video to build richer understanding (e.g., extensions of ViLBERT).\n",
        "\n",
        "- **Self-Supervised Learning Advances**  \n",
        "  New pretraining objectives and architectures continue to improve language understanding.\n",
        "\n",
        "---\n",
        "\n",
        "##  **Key Takeaways**\n",
        "\n",
        "- BERT generates **contextual embeddings** for each token by considering the entire input bidirectionally.\n",
        "\n",
        "- It is based on the **Transformer encoder architecture**.\n",
        "\n",
        "- BERT‚Äôs framework has two main phases:\n",
        "  1. **Pretraining** on large unlabeled text corpora using **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**.\n",
        "  2. **Finetuning** on specific downstream NLP tasks by adding small task-specific layers.\n",
        "\n",
        "- Variants like RoBERTa, ALBERT, and DistilBERT improve performance, efficiency, and scalability.\n",
        "\n",
        "- BERT has transformed many real-world applications, including search, chatbots, summarization, and biomedical NLP.\n",
        "\n",
        "- Despite its success, BERT faces challenges like high computational cost and input length limits, which ongoing research aims to solve.\n",
        "\n"
      ],
      "metadata": {
        "id": "ilBf0LaoXSsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# References\n",
        "\n",
        "- Devlin J., Chang M., Lee K., Toutanova K. (2018), BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL, https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "- Vaswani A. et al., (2018), Attention Is All You Need, NIPS, https://arxiv.org/abs/1706.03762\n",
        "    - Check 3. Model Architecture on page 2 to study about encoder.\n",
        "\n",
        "- Nayak P. (2019), Understanding searches better than ever before, https://blog.google/products/search/search-language-understanding-bert/\n"
      ],
      "metadata": {
        "id": "7-goH-wsYVoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suggestion Points:\n",
        "\n",
        "* Use [viz](https://github.com/jessevig/bertviz) to display attention visualization.\n",
        "* More techincal details on MLP and NSP\n",
        "  * How is [MASK] handled during fine-tuning.\n",
        "  * Effect of removing NSP [Compare ablation study with and with out NSP on MNLI and QNLI]\n",
        "* Add some examples of loading BERT model from huggingface show examples of task like MASK filling that BERT is capable of"
      ],
      "metadata": {
        "id": "1brlGRZArUI6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1K-omRqNwwIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "518c7790"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76c123fd"
      },
      "source": []
    }
  ]
}