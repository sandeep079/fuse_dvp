{"cells":[{"cell_type":"markdown","metadata":{"id":"94kSBK7UgGKj"},"source":["# **Variational Inference in VAE**\n","\n","## Prerequisite\n","Before starting this lesson, students should be familiar with the following concepts\n","- Probability Distribution & Bayes Theorem\n","- Intro of VAE\n"]},{"cell_type":"markdown","metadata":{"tags":["image"],"id":"8Vi-s-q-7n-0"},"source":["## **Variational Inference**\n","\n","Variational inference is a technique to deduce parameters of probability distributions in graphical models.\n","\n","*Please forget everything you know about deep learning and neural networks for now. Thinking about the following concepts in isolation from neural networks will clarify things. At the very end, we’ll bring back neural nets.*\n","\n","Graphical models are used in probability and statistics where graphs visually represent conditional dependence between random variables.\n","\n","Suppose a graphical VAE model with data $x$ and latent variable $h$.\n","\n","<center>\n","<figure>\n","<p><img src=\"https://i.postimg.cc/R0GtwfVh/graph-model-for-variational-inference.jpg\"></p>\n","<figcaption align=\"center\">Fig: Graphical Model used for approximating our posterior probability, where solid lines denote the generative model and dotted ones denote the approximate posterior</figcaption>\n","</figure>\n","</center>\n","\n","We assume that the data is generated by some random process involving a continuous random variable $h$. Each latent variable is drawn from a known distribution, prior $P(h)$ (i.e., $h \\sim P(h)$), and the data points are then generated from a likelihood $P(x|h)$ (i.e., $x \\sim P(x|h)$).\n","We can write the joint probability of the model as $P(x, h) = P(x|h) \\cdot P(h)$.\n","The prior $P(h)$ and likelihood $P(x|h)$ are selected to be simple (e.g., Gaussian), from which $h$, $x$ are generated. Gaussian is chosen because input data representation is feasible that way. It reduces computational complexity of approximation and is reasonable for latent attributes. (For a face image, latent features can be a pose, amount of smile, etc.)\n","\n","> **Analogy**: Imagine generating faces by randomly picking features like smile intensity, angle of head, eye size—those are your $h$'s. You pick these from a “bag” (the prior), and a drawing mechanism (the likelihood) turns those features into actual face images.\n","\n","Coming from a machine learning background, we know the same neural network (same function) is used by all data points. But in our graphical VAE model, the latent variable is **local**. This means each data point has a unique latent $h$ and doesn’t share it with another variable.\n","This is very important to note when tackling the problem using a graphical model.\n","\n","> **Analogy:** Suppose you see a face image $x$ and want to figure out which facial features (smile, head tilt, eye size — the latent $h$) created it. Each face has unique features, but directly identifying them all is complicated.\n","\n","\n","The goal of the model is to infer a good value of the latent variable given an observation of data, represented by the posterior $P(h|x)$, denoted by the dotted line.\n","The idea is that inferring $P(h)$ using $P(h|x)$ makes the latent variables more likely under our data. Using Bayes' theorem:\n","\n","$$\n","P(h|x) = \\frac{P(x|h) \\cdot P(h)}{P(x)} \\tag{7}\n","$$\n","\n","But the evidence $P(x)$ in the above equation is **intractable** when the latent space is high-dimensional:\n","\n","$$\n","P(x) = \\underbrace{\\int P(x|h) \\cdot P(h) \\, dh}_{\\text{intractable}} \\tag{8}\n","$$\n","\n","This integral becomes computationally expensive because it requires summing over **all possible values** of the latent variables. As a result, the true posterior is also intractable.\n","\n","> **Analogy**: Imagine trying to figure out how likely a face image is by **checking every possible combination** of facial features—every smile intensity, head angle, eye size. Since the combinations are endless, this quickly becomes impossible.\n","\n","This is where **variational inference** comes in.\n","\n","We approximate the true posterior $P(h|x)$ with a simpler, tractable distribution $q_\\lambda(h|x)$, where $\\lambda$ are parameters (e.g., mean and variance in a Gaussian).\n","\n","> **Analogy**: Instead of checking every combination of facial features, you **pick one combination that seems good enough** based on prior info—like guessing the most likely smile and pose that produced the face you see.\n","\n","Our goal is to **minimize the KL divergence** between $q_\\lambda(h|x)$ and $P(h|x)$:\n","\n","$$\\min \\mathbb{KL}(q_\\lambda(h|x) || P(h|x)) $$ Given as\n","\n","$$\n","\\mathbb{KL}(q_\\lambda(h|x) || P(h|x)) = -\\sum q_\\lambda(h|x)*log\\frac{P(h|x)}{q_\\lambda(h|x)} \\tag{9}\n","$$\n","\n","Now, moving on with the derivation. Substituting the true posterior value from Eqn (7), Eqn (9) converts to\n","\n","\n","$$\n","\\begin{aligned}\n","\\text{or,} \\quad \\mathbb{KL}(q_\\lambda(h|x) \\| P(h|x))\n","&= -\\sum q_\\lambda(h|x) \\cdot \\log \\frac{\\frac{P(x,h)}{P(x)}}{q_\\lambda(h|x)} \\\\\n","&= -\\sum q_\\lambda(h|x) \\cdot \\left[\\log \\frac{P(x,h)}{q_\\lambda(h|x)} - \\log P(x)\\right] \\quad \\text{(log property)}\n","\\end{aligned}\n","$$\n","\n","\n","$$\n","or, \\mathbb{KL}(q_\\lambda(h|x) || P(h|x)) =  -\\sum_h q_\\lambda(h|x)*log\\frac{P(x,h)}{q_\\lambda(h|x)} + \\sum_h q_\\lambda(h|x)*logP(x)\n","$$\n","\n","Since, $\\sum_h q_\\lambda(h|x) = 1$ (summation of probability) and $logP(x)$'s parameter is defined by variable $h$\n","\n","$$\n","or, \\mathbb{KL}(q_\\lambda(h|x) || P(h|x)) = -\\sum q_\\lambda(h|x)*log\\frac{P(x,h)}{q_\\lambda(h|x)} + logP(x) \\tag{10}\n","$$\n","\n","Eqn (10) can be re-written in term of evidence as\n","\n","$$\n","logP(x) =\\mathbb{KL}(q_\\lambda(h|x) || P(h|x)) + \\underbrace{\\sum q_\\lambda(h|x)*log\\frac{P(x,h)}{q_\\lambda(h|x)}}_{\\mathcal{L(\\lambda)}} \\tag{11}\n","$$\n","\n","In the above eqn, the latter term given by $\\mathcal{L(\\lambda)}$ is known as **Variational LowerBound** or **Evidence Lowerbound(ELBO)** (computationally tractable as we will discuss). The KL-divergence term still contains the pesky term $P(h|x)$ which we discussed was intractable which is always greater or equal to zero.\n","\n","Since, $P(x)$ has definite value for particular input data $x$ we can assume $logP(x)$ to be a constant for given $x$. So, Instead of minimizing the KL-Divergence we can maximize $\\mathcal{L(\\lambda)}$ as the operations are equivalent. As we know $\\mathbb{KL} >= 0$, so, variational lowerbound, $\\mathcal{L} <= logP(x)$.\n","\n","<center>\n","<figure>\n","\n","\n","<p><img src=\"https://i.postimg.cc/L6nMSMZb/kl-divergence-minimization.png\"  ></p>\n","<figcaption align=\"center\">Fig: Graphical representation of the KL-divergence minimization, which can be achieved by maximizing variational lowerbound or evidence lowerbound (ELBO) to reach evidence (log P(x))\n","\n","We know that KL>=0. Our target is to reduce KL divergence to zero.</br>\n","i.e $\\mathbb{KL}=0$ or $log P(x)=ELBO$</figcaption>\n","</figure>\n","</center>\n","\n","\n","\n","Also the variational lowerbound term's gradient is computable through which we can later optimize, which can also be written as:\n","\n","$$\n","\\begin{aligned}\n","\\mathcal{L}(\\lambda) &= \\sum q_\\lambda(h|x) * \\log \\frac{P(x,h)}{q_\\lambda(h|x)} \\\\\n","&= \\sum q_\\lambda(h|x) * \\log \\frac{P(x|h) * P(h)}{q_\\lambda(h|x)}\n","\\end{aligned}\n","$$\n","\n","$$\n","or, \\mathcal{L(\\lambda)} = \\sum q_\\lambda(h|x)[logP(x|h)+log\\frac{P(h)}{q_\\lambda(h|x)}]\n","$$\n","\n","$$\n","or, \\mathcal{L(\\lambda)} = \\sum q_\\lambda(h|x)*logP(x|h)+ \\sum q_\\lambda(h|x)*log\\frac{P(h)}{q_\\lambda(h|x)} \\tag{12}\n","$$\n","\n","The 1st term in Eqn (12) is familiar in nature, it's the expectation of log-likelihood, $logP(x|h)$.\n","\n","$$\n","\\mathbb{E}_{q_\\lambda(h|x)} [logP(x|h)] = \\sum q_\\lambda(h|x)*logP(x|h)\n","$$\n","\n","The 2nd term in eqn(12) is the -ve of KL-Divergence of $q_\\lambda(h|x)$ w.r.to prior $P(h)$\n","\n","$$\n","-\\mathbb{KL}(q_\\lambda(h|x)  || P(h)) = \\sum q_\\lambda(h|x)*log\\frac{P(h)}{q_\\lambda(h|x)}\n","$$\n","\n","\n","$$\n","\\mathcal{L(\\lambda)} = \\mathbb{E}_{q_\\lambda(h|x)} [logP(x|h)] - \\mathbb{KL}(q_\\lambda(h|x)  || P(h)) \\tag{13}\n","$$\n","\n","For a single data point $x^{(i)}$, the variational lowerbound is represented as\n","\n","$$\n","\\mathcal{L_i(\\lambda)} = \\mathbb{E}_{q_\\lambda(h|x^{(i)})} [logP(x^{(i)}|h)] - \\mathbb{KL}(q_\\lambda(h|x^{(i)})  || P(h))\n","$$\n","\n","> **Analogy**: You're trying to find the best \"explanation\" (latent variable $h$) for a particular observed event (data point $x$), while also making sure that your explanation is not too far-fetched (i.e., it stays close to what you'd expect a prior).\n","\n","We have to **maximize** the variational lowerbound to make assumed posterior and true posterior similar to each other. This can be achieved by maximizing the likelihood of obtaining points $logP(x^{(i)}|h)$ given $h$ and making our assumed posterior similar to prior $P(h)$ deduced from Eqn (11).\n","\n","The prior is selected to be Gaussian distribution(or Bernoulli distribution) making our approximate posterior $q_\\lambda(h|x)$ to be similar to Gaussian, which has a neat closed-form solution. This enables us to save the mean and variance of the approximate posterior as vectors/matrix in our neural network.\n","\n","Using variational inference, we’ve now solved the problem of not being able to calculate the intractable true posterior. This gives us a mathematically sound and computationally feasible way to proceed—and now we’re ready to bring back **neural networks** to parameterize $q_\\lambda(h|x)$ and $P(x|h)$, which will be discussed in the next notebook.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AsmXBYGpN_J8"},"source":["<!-- $$p\\left( {z|x} \\right) = \\frac{{p\\left( {x|z} \\right)p\\left( z \\right)}}{{p\\left( x \\right)}}$$\n","\n","\n","$$p\\left( x \\right) = \\int {p\\left( {x|z} \\right)p\\left( z \\right)dz}$$\n","\n","\n","\n","$$\\min KL\\left( {q\\left( {z|x} \\right)||p\\left( {z|x} \\right)} \\right)$$\n","\n","\n","$$\\text{log }p(x) =\\text{KL}(q(z|x) || p(z|x)) + {\\sum q(z|x)*\\text{log}\\frac{p(x,z)}{q(z|x)}} $$\n","\n","\n","$${E_{q\\left( {z|x} \\right)}}\\log p\\left( {x|z} \\right) - \\text{KL}\\left( {q\\left( {z|x} \\right)||p\\left( z \\right)} \\right)$$  -->\n","\n","\n","### Key Takeaways\n","\n","1. Variational inference approximates the posterior distribution of the data with the family of distribution.\n","2. We minimize the KL-Divergence between two distribution. It is equivalent to maximizing the evidence lowerbound (ELBO).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G63HOsR9IYRP"},"source":["### Additional Resources\n","\n","* Papers\n","   * Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. https://arxiv.org/pdf/1312.6114.pdf\n","       * Appendix B analytically solves the KL divergence term.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}