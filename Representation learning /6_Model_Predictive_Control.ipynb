{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","---\n","# Module 4: Model Predictive Control (MPC)\n","\n","## Objectives\n","\n","* Understand the principles of **Model Predictive Control** (MPC) for sequential decision-making.\n","* Learn how **optimization** and **prediction models** guide control actions.\n","* Explore how **learned representations** integrate into MPC.\n","\n","---\n","\n","## 1. Introduction to MPC\n","\n","### **What is Model Predictive Control (MPC)?**\n","\n","Model Predictive Control (MPC) is an advanced control strategy that uses a model of the system to predict its future behavior over a defined time horizon. It's particularly well-suited for **sequential decision-making** problems, where a series of actions are taken over time to achieve a goal.\n","\n","At each time step, MPC follows an iterative process:\n","\n","1. **Predict:** Using the system model, MPC predicts the system's future states over a prediction horizon, considering a sequence of potential control actions.\n","2. **Optimize:** Based on these predictions, an optimization problem is solved to find the sequence of control actions that minimizes a cost function (e.g., minimizing error, maximizing performance) over the prediction horizon.\n","3. **Apply:** Only the first action from the optimal sequence is applied to the system.\n","4. **Repeat:** The prediction and optimization process is repeated at the next time step, using the updated system state. This receding horizon approach allows MPC to adapt to changing conditions and disturbances.\n","\n","---\n","MPC is used extensively in **process control**, **robotics**, and **autonomous systems** due to its ability to handle multi-variable systems with constraints.\n","\n","<img src= \"https://i.postimg.cc/d3zWdCD6/anim.webp\">\n","The MPC principle is visualized in the graphic above. The dotted line indicates the current prediction and the solid line represents the realized values. The graphic is generated using the innate plotting capabilities of do-mpc.\n","\n","\n","---\n","\n","## 2. TD-MPC: Model Predictive Control with Latent Representations\n","\n","TD-MPC (Task-Driven Model Predictive Control) introduces a **latent dynamics model** to learn compact representations of high-dimensional environments. All predictions, planning, and optimization are done in **latent space**.\n","\n","To learn a policy $\\pi_\\theta(s) \\rightarrow a$ that maximizes the long-term reward in the given infinite-horizon Markov Decision Process (MDP) with continuous state and action spaces, we need to maximize the expected discounted cumulative reward, defiThe goal is to maximize the expected discounted return:\n","\n","$$\n","J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\right],\n","$$\n","\n","where:\n","\n","* $\\pi_\\theta(s)$ is the parameterized policy (e.g., a neural network),\n","* $\\tau = (s_0, a_0, s_1, a_1, \\dots)$ is a trajectory,\n","* $s_0 \\sim p_0$, the initial state distribution,\n","* $a_t = \\pi_\\theta(s_t)$, the action taken by the policy,\n","* $s_{t+1} \\sim T(\\cdot|s_t, a_t)$, the environment dynamics.\n","\n","---\n","\n","### Common RL Methods for Continuous Control\n","\n","To optimize $J(\\pi_\\theta)$ in continuous domains, we often use **policy gradient methods** or **actor-critic methods**:\n","\n","---\n","\n","#### 1. **Policy Gradient Methods**\n","\n","These directly optimize the policy by estimating the gradient $\\nabla_\\theta J(\\pi_\\theta)$ and applying gradient ascent:\n","\n","$$\n","\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot R_t \\right],\n","$$\n","\n","where $R_t = \\sum_{t'=t}^\\infty \\gamma^{t'-t} R(s_{t'}, a_{t'})$ is the return from time $t$ onward.\n","\n","**Example algorithms**:\n","\n","* **REINFORCE** (Vanilla Policy Gradient)\n","* **Trust Region Policy Optimization (TRPO)**\n","* **Proximal Policy Optimization (PPO)** — very popular due to stability and ease of implementation\n","\n","---\n","\n","#### 2. **Actor-Critic Methods**\n","\n","These use two function approximators:\n","\n","* The **actor** $\\pi_\\theta(a|s)$, which selects actions\n","* The **critic** $V_w(s)$ or $Q_w(s,a)$, which estimates the value function\n","\n","The critic helps reduce variance in the policy gradient:\n","\n","$$\n","\\nabla_\\theta J(\\pi_\\theta) \\approx \\mathbb{E}_{s,a} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A^{\\pi}(s,a) \\right],\n","$$\n","\n","where $A^\\pi(s,a)$ is the advantage function (e.g., $Q(s,a) - V(s)$).\n","\n","**Popular actor-critic methods**:\n","\n","* **Deep Deterministic Policy Gradient (DDPG)** — deterministic policy for continuous actions\n","* **Twin Delayed DDPG (TD3)** — improves over DDPG with less overestimation\n","* **Soft Actor-Critic (SAC)** — adds entropy regularization for better exploration and stability\n","\n","---\n","\n","\n","\n","---\n","\n","Absolutely! Here's a **detailed breakdown** of the components of the **Task-Oriented Latent Dynamics (TOLD) model**, as introduced in the TD-MPC paper. This model forms the **backbone of the TD-MPC architecture**, enabling efficient decision-making in a learned latent space, rather than the high-dimensional raw observation space.\n","\n","---\n","\n","## **Task-Oriented Latent Dynamics (TOLD) Model: In-Depth**\n","\n","---\n","\n","### **Why Learn Latent Dynamics?**\n","\n","In high-dimensional environments (like vision-based RL), it's inefficient and unnecessary to model every detail of the raw observation.\n","Instead, we learn a **latent representation** that:\n","\n","* **Compresses** the observation,\n","* **Focuses** only on task-relevant aspects,\n","* **Supports planning and value estimation** in this compact space.\n","\n","This helps the agent **generalize** better across tasks and reduces sample complexity.\n","\n","---\n","\n","##COMPONENTS OF THE TOLD MODEL\n","\n","Each component is a neural network that plays a role in **modeling and predicting future behavior** in the **latent space**.\n","\n","---\n","\n","### 1. **Representation Function**\n","\n","Encodes an observation $s_t$ into a latent state $z_t$:\n","\n","$$\n","z_t = h_\\theta(s_t)\n","$$\n","\n","* $s_t$: Raw observation (e.g., image, joint state).\n","* $z_t$: Compressed latent representation.\n","* $h_\\theta$: Encoder (typically a CNN for images, MLP for state vectors).\n","\n","**Purpose**: Filter out noise and extract features relevant to the control task.\n","\n","---\n","\n","### 2. **Dynamics Function**\n","\n","Predicts next latent state from current latent and action:\n","\n","$$\n","z_{t+1} = d_\\theta(z_t, a_t)\n","$$\n","\n","* $d_\\theta$: Latent dynamics model (e.g., MLP or recurrent network).\n","* Learns to simulate how the latent state evolves under action $a_t$.\n","\n","**Purpose**: Enable model-based planning entirely in the latent space.\n","\n","---\n","\n","### 3. **Reward Function**\n","\n","$$\n","\\hat{r}_t = R_\\theta(z_t, a_t)\n","$$\n","\n","* Predicts the reward from latent state and action.\n","* Trained to regress to the actual reward $r_t$.\n","\n","**Purpose**: Provide reward signals for imagined trajectories during planning.\n","\n","---\n","\n","### 4. **Value Function**\n","\n","$$\n","\\hat{q}_t = Q_\\theta(z_t, a_t)\n","$$\n","\n","* Approximates the **expected return** (cumulative discounted reward) from a state-action pair.\n","* Used for bootstrapping and value learning.\n","\n","**Purpose**: Guide the agent toward high-return trajectories.\n","\n","---\n","\n","### 5. **Policy Function**\n","\n","$$\n","\\hat{a}_t = \\pi_\\theta(z_t)\n","$$\n","\n","* Outputs the action to take from latent state $z_t$.\n","* Learns a mapping from latent representation to optimal actions.\n","\n","**Purpose**: Infer actions directly from compact features.\n","\n","---\n","\n","## TRAINING OBJECTIVE\n","\n","The TOLD model is trained to **jointly learn** representation, dynamics, reward, value, and policy by minimizing a **composite loss** over imagined rollouts.\n","\n","---\n","\n","### **Total Loss Function**\n","\n","$$\n","J(\\theta) = \\sum_{i=t}^{t+H} \\lambda^{i-t} \\mathcal{L}(\\theta; \\Gamma_i)\n","$$\n","\n","* $H$: Horizon length.\n","* $\\Gamma_i$: Training tuple at time $i$.\n","* $\\lambda$: Discount factor for horizon depth (not to be confused with RL discount $\\gamma$).\n","\n","---\n","\n","### Single-Step Loss: $\\mathcal{L}(\\theta; \\Gamma_i)$\n","\n","$$\n","\\mathcal{L}(\\theta; \\Gamma_i) = c_1 \\ell^r_i + c_2 \\ell^v_i + c_3 \\ell^c_i\n","$$\n","\n","Each term trains a different component:\n","\n","---\n","\n","#### a. **Reward Loss**\n","\n","$$\n","\\ell^r_i = \\| R_\\theta(z_i, a_i) - r_i \\|^2\n","$$\n","\n","* Supervised regression of predicted reward to ground-truth reward.\n","* Encourages accurate predictions for imagined rewards.\n","\n","---\n","\n","#### b. **Value TD Loss**\n","\n","$$\n","\\ell^v_i = \\left\\| Q_\\theta(z_i, a_i) - \\left(r_i + \\gamma Q_{\\theta^-}(z_{i+1}, \\pi_\\theta(z_{i+1}))\\right) \\right\\|^2\n","$$\n","\n","* A **temporal difference (TD)** loss using bootstrapped targets.\n","* $\\gamma$: RL discount factor.\n","* $\\theta^-$: Target network (slow-moving version of $\\theta$).\n","\n","Promotes stability by not updating the target and prediction networks simultaneously.\n","\n","---\n","\n","#### c. **Consistency Loss**\n","\n","$$\n","\\ell^c_i = \\| d_\\theta(z_i, a_i) - h_{\\theta^-}(s_{i+1}) \\|^2\n","$$\n","\n","* Ensures that the predicted next latent state matches the encoder output of the next true observation.\n","* Encourages the model to learn **consistent latent transitions**.\n","\n","---\n","\n","## Target Network Update\n","\n","To improve training stability, target networks (used in value and consistency loss) are updated slowly via:\n","\n","$$\n","\\theta^-_{t+1} = (1 - \\zeta) \\theta^-_t + \\zeta \\theta_t\n","$$\n","\n","* $\\zeta \\in (0, 1)$: Smoothing factor (e.g., 0.01).\n","* Prevents value overestimation and training oscillations.\n","\n","---\n","\n","\n","\n","\n","---\n","\n","## 2. System Modelling and Prediction Horizons\n","\n","---\n","\n","### **System Modelling in MPC**\n","\n","In **Model Predictive Control (MPC)**, the controller **predicts the future behavior of a system** using a **mathematical model** of its dynamics. At each time step, MPC solves an optimization problem to determine the best control inputs by simulating how the system will evolve in the future. This prediction is based entirely on the model.\n","\n","---\n","\n","####  What is a System Model?\n","\n","A **system model** is a set of equations that describe how the internal state of a system evolves in response to inputs. It allows us to forecast future states and outputs.\n","\n","In its **linear time-invariant (LTI)** form (a simplified case often used in control), the model looks like this:\n","\n","$$\n","x(t+1) = A x(t) + B u(t) \\\\\n","y(t) = C x(t)\n","$$\n","\n","---\n","\n","#### Breakdown of the Terms\n","\n","* **$x(t)$**: The state vector at time $t$. This represents the internal configuration of the system (e.g., position and velocity in a robot).\n","\n","* **$u(t)$**: The control input at time $t$. This is the decision the controller makes (e.g., force applied by motors).\n","\n","* **$y(t)$**: The system output (what we observe or want to control, such as robot position).\n","\n","* **$A, B, C$**: Matrices that define how states evolve:\n","\n","  * $A$: State transition matrix — how the system moves on its own.\n","  * $B$: Control matrix — how the input affects the state.\n","  * $C$: Output matrix — how the state is mapped to measurable outputs.\n","\n","> In practice, especially for complex or nonlinear systems, these matrices aren’t known ahead of time. Instead, they are:\n",">\n","> * **Identified** from physical modeling (e.g., physics-based equations), or\n","> * **Learned** from data (using system identification or neural networks).\n","\n","---\n","---\n","\n","#### Optimization Problem in MPC\n","\n","The optimization problem typically looks like:\n","\n","$$\n","\\min_{u_0, ..., u_{N-1}} \\sum_{k=0}^{N-1} \\ell(x_k, u_k) + \\ell_f(x_N)\n","$$\n","\n","subject to:\n","\n","$$\n","x_{k+1} = Ax_k + Bu_k \\quad (\\text{System dynamics}) \\\\\n","x_k \\in \\mathcal{X}, \\quad u_k \\in \\mathcal{U} \\quad (\\text{State/input constraints})\n","$$\n","\n","Where:\n","\n","* $\\ell(x_k, u_k)$: Stage cost at each time step (e.g., penalize deviation from a desired state)\n","* $\\ell_f(x_N)$: Terminal cost at the horizon end\n","* $\\mathcal{X}, \\mathcal{U}$: Constraint sets on states and controls\n","\n","---\n","\n","#### Receding Horizon Control\n","\n","Only the **first control input** $u(t)$ is applied to the real system. Then the system moves to a new state $x(t+1)$, and the process repeats with a new optimization problem. This is called **receding horizon control** and provides **feedback**, making the system robust to disturbances or model errors.\n","\n","---\n","\n","### Summary\n","\n","| Concept                      | Description                                                                                   |\n","| ---------------------------- | --------------------------------------------------------------------------------------------- |\n","| **System Model**             | Describes how the system state evolves in time using equations like $x(t+1) = Ax(t) + Bu(t)$. |\n","| **Prediction Horizon ($N$)** | Number of future steps MPC uses to simulate and optimize control decisions.                   |\n","| **Control Input ($u(t)$)**   | The decision made by the MPC at time $t$.                                                     |\n","| **Receding Horizon**         | Only the first control input is used, then re-optimization happens at the next step.          |\n","\n","---\n","\n","### If the system is nonlinear or unknown?\n","\n","In modern settings (like autonomous driving or robotics), we often replace the linear model with a **learned nonlinear model**, such as:\n","\n","* Neural networks\n","* Gaussian processes\n","* Koopman operator models\n","\n","This leads to **Learning-based MPC** or **Model-based Reinforcement Learning**, where a learned model is used within the MPC framework.\n","\n","---\n","\n","\n","\n","\n","### **Prediction Horizon**\n","\n","The **prediction horizon** is a key parameter in MPC. It defines the number of future time steps over which the system's behavior is predicted and the optimization problem is solved. A longer prediction horizon allows MPC to anticipate future events and plan actions accordingly, potentially leading to better long-term performance. However, it also increases the computational complexity of the optimization problem.\n","\n","In MPC, predictions are made over a finite horizon `N`. The optimization problem is solved at each time step to determine the sequence of control actions that minimizes the cost function over the next `N` steps.\n","\n","---\n","\n","\n","---\n","\n","## 3. Optimization-Based Action Selection in MPC\n","\n","---\n","\n","### **What Is It?**\n","\n","At every time step, **Model Predictive Control (MPC)** computes the best control input by solving an optimization problem. This problem:\n","\n","* Predicts how the system will behave over a **future time window** (the prediction horizon),\n","* Chooses control inputs that **minimize a cost function**,\n","* Respects **constraints** on the system and controls.\n","\n","This makes MPC **explicitly goal-driven**: it chooses actions that optimize future performance based on predictions from a system model.\n","\n","---\n","\n","### Optimization Problem in MPC\n","\n","A common form of the MPC optimization problem is:\n","\n","$$\n","\\min_{\\mathbf{U}} \\ \\| \\mathbf{Y} - \\mathbf{Y}_{\\text{ref}} \\|^2 + \\| \\Delta \\mathbf{U} \\|^2\n","$$\n","\n","**Subject to:**\n","\n","* Dynamics constraints (i.e., how $Y$ depends on inputs and the model),\n","* State and input constraints (e.g., actuator limits, safety constraints).\n","\n","---\n","\n","### Terms Explained\n","\n","| Term                      | Description                                                                       |\n","| ------------------------- | --------------------------------------------------------------------------------- |\n","| $\\mathbf{Y}$              | Vector of **predicted outputs** over the horizon: $[y(t+1), y(t+2), ..., y(t+N)]$ |\n","| $\\mathbf{Y}_{\\text{ref}}$ | **Reference trajectory** (desired future outputs to follow)                       |\n","| $\\Delta \\mathbf{U}$       | Change in control inputs over time: $\\Delta u(t) = u(t) - u(t-1)$                 |\n","| $\\mathbf{U}$              | Control input sequence: $[u(t), u(t+1), ..., u(t+N-1)]$                           |\n","\n","---\n","\n","### Why These Cost Terms?\n","\n","* **$\\|\\mathbf{Y} - \\mathbf{Y}_{\\text{ref}}\\|^2$**:\n","\n","  * Penalizes deviation from the target trajectory.\n","  * Drives the system toward goals (e.g., desired position, speed, temperature).\n","\n","* **$\\|\\Delta \\mathbf{U}\\|^2$**:\n","\n","  * Penalizes rapid changes in control inputs (e.g., jerky movements).\n","  * Encourages **smooth, stable control behavior**.\n","\n","---\n","\n","### Constraints\n","\n","Constraints ensure physical feasibility and safety:\n","\n","* **Input constraints**: $u_{\\text{min}} \\le u(t) \\le u_{\\text{max}}$\n","* **State constraints**: $x(t) \\in \\mathcal{X}$ (e.g., position must stay inside boundaries)\n","* **Output constraints**: $y(t) \\in \\mathcal{Y}$\n","\n","The optimization must choose control actions that satisfy all of these.\n","\n","---\n","\n","### Receding Horizon Control\n","\n","Once the optimal control sequence $\\mathbf{U}^* = [u^*_0, u^*_1, \\dots, u^*_{N-1}]$ is found:\n","\n","* **Only the first input** $u^*_0$ is applied to the system.\n","* The system moves forward to the next state.\n","* A **new optimization** is solved at the next time step with updated state info.\n","\n","This loop gives MPC its **feedback** and **adaptive behavior**.\n","\n","---\n","\n","### What If the Model or Cost Is Complex?\n","\n","In practice, the dynamics model might be:\n","\n","* **Nonlinear** (e.g., robot arm, drone),\n","* **Learned from data** (e.g., neural networks),\n","* **Implicit** (e.g., in black-box simulators).\n","\n","These introduce challenges:\n","\n","* The optimization may become **non-convex**, requiring nonlinear or gradient-based solvers.\n","* If a neural network is used to model dynamics or output predictions, automatic differentiation (e.g., using PyTorch or TensorFlow) is used to compute gradients.\n","\n","---\n","\n","### Example (High-Level)\n","\n","Let’s say a drone wants to follow a path at a constant altitude and avoid obstacles.\n","\n","The MPC optimization problem could be:\n","\n","$$\n","\\min_{\\mathbf{U}} \\sum_{k=0}^{N-1} \\left[ \\| y_k - y_k^{\\text{ref}} \\|^2 + \\lambda \\| \\Delta u_k \\|^2 \\right]\n","$$\n","\n","**Subject to:**\n","\n","* Drone dynamics (e.g., learned or physics-based model)\n","* Constraints on thrust, pitch, yaw\n","* Obstacle avoidance regions\n","\n","Here, the optimization must **balance trajectory tracking**, **smooth motion**, and **constraint satisfaction**.\n","\n","---\n","\n","### Summary Table\n","\n","| Component                 | Role                                                                  |\n","| ------------------------- | --------------------------------------------------------------------- |\n","| **Cost Function**         | Guides the system toward desired behavior (e.g., trajectory tracking) |\n","| **Optimization Problem**  | Computes best future control actions over prediction horizon          |\n","| **Constraints**           | Enforce physical limits and safety                                    |\n","| **Receding Horizon**      | Only apply the first control input; replan at next step               |\n","| **Learning-Based Models** | Allow MPC in systems where physical models are unavailable or complex |\n","\n","---\n","\n","\n","---\n","\n","## 4. State-Based MPC Formulation\n","\n","### **State-Based MPC – Concept**\n","\n","State-based MPC refers to the use of **optimal control synthesis** for solving the MPC problem over a finite horizon. This approach involves solving the system's dynamics iteratively to find the optimal control inputs that minimize the cost function.\n","\n","An infinite-horizon optimal control problem can be approximated as a **N-step horizon** problem, leading to a state-space MPC formulation:\n","\n","```math\n","minimize J = Σ_{τ=t}^{t+N} [(y(τ) - r(τ))² + u(τ)²]  \n","subject to x(τ+1) = Ax(τ) + Bu(τ)\n","```\n","\n","Where:\n","\n","* `J` is the cost function.\n","* `r(τ)` is the reference trajectory.\n","* `x(τ)` is the system state at time `τ`.\n","* `u(τ)` is the control action at time `τ`.\n","\n","This formulation uses **receding horizon control** (RHC), where the optimization is recalculated at each time step, always looking ahead over a fixed horizon.\n","\n","---\n","\n","## 5. Integration with Learned Representations\n","\n","The paper explores the **integration of learned representations** into the MPC framework. Traditionally, MPC relies on hand-designed or identified system models and cost functions based on raw sensor data. However, learned representations, often derived from deep learning models, can provide a more abstract and potentially more informative state representation. Integrating these learned representations into MPC can offer several advantages:\n","\n","* **Improved System Modelling**: Learned representations can capture complex, non-linear system dynamics more effectively than traditional models, leading to more accurate predictions.\n","* **Enhanced Optimization**: Optimization can be performed in a lower-dimensional or more meaningful latent space defined by the learned representations, potentially simplifying the optimization problem and improving efficiency.\n","* **Handling High-Dimensional Data**: Learned representations can effectively process high-dimensional sensor data (e.g., images) into a compact representation suitable for MPC.\n","\n","The paper likely details specific methods for integrating these learned representations, such as using the learned representation as the state for the MPC, or using the learned representation to improve the system model or the cost function used in the MPC optimization.\n","\n","---\n","\n","\n","##  References\n","\n","* Joe Qin's Survey on Industrial MPC: [Qin MPC Resource](http://www.che.utexas.edu/~qin/cpcv/cpcv14.html)\n","* Bemporad et al., 1994 - Stability of Constrained MPC\n","* Mayne et al., 2000 - MPC Stability Theory\n","\n","---\n","\n","This version includes clear definitions of key concepts and explanations, providing a comprehensive guide to the lecture material. Would you like me to export this as a `.md` file or convert it to another format?\n"],"metadata":{"id":"nPwH9hM3tMjZ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"3_tK-KiI3Rz6"}},{"cell_type":"code","source":[],"metadata":{"id":"rLFAKOButNDk"},"execution_count":null,"outputs":[]}]}