{"cells":[{"cell_type":"markdown","id":"85da4f5d","metadata":{"id":"85da4f5d"},"source":["# Energy Based Model\n","\n","This notebook provides a comprehensive exploration of Energy-Based Models (EBMs), a versatile framework for modeling complex relationships in data, particularly for structured prediction tasks. EBMs assign a scalar energy value to input-output pairs, where lower energy indicates higher compatibility. This framework is widely used in machine learning for tasks such as classification, regression, and structured prediction (e.g., sequence labeling, image segmentation). Through this notebook, we aim to:\n","- Introduce the theoretical foundations of EBMs, including energy functions, inference, and learning.\n","- Provide practical Python implementations to demonstrate EBM concepts.\n","- Explore real-world applications with detailed examples.\n","- Address exercises to deepen understanding and encourage experimentation."]},{"cell_type":"markdown","id":"37feeb06","metadata":{"id":"37feeb06"},"source":["## Introduction\n","\n","Energy-Based Models (EBMs) provide a powerful framework for modeling complex relationships in data, particularly for structured prediction tasks. Unlike traditional discriminative models that directly predict outputs, EBMs assign an energy score to input-output pairs, where lower energy indicates better compatibility. This flexibility makes EBMs suitable for tasks like classification, regression, and structured outputs (e.g., sequences, graphs). This notebook introduces the EBM framework, covering energy functions, inference methods, learning algorithms, and loss functions, with practical Python examples.\n","\n","**Why EBMs?**\n","- **Flexibility**: EBMs can model complex dependencies without assuming a specific probabilistic structure.\n","- **Generalization**: Applicable to various tasks, from simple classification to structured prediction like natural language processing and computer vision.\n","- **Interpretability**: The energy function provides a clear measure of compatibility, which can be analyzed and visualized."]},{"cell_type":"markdown","id":"4b694fcd","metadata":{"id":"4b694fcd"},"source":["### Objective\n","<ul>\n","<li>To understand the energy-based modeling framework for structured prediction.</li>\n","<li>To explore inference methods, learning algorithms, and loss functions in EBMs.</li>\n","<li>To implement EBMs in Python for practical applications.</li>\n","</ul>"]},{"cell_type":"markdown","id":"00d975c1","metadata":{"id":"00d975c1"},"source":["### Key Topics\n","<ul>\n","<li>Energy functions and compatibility</li>\n","<li>Deterministic and probabilistic inference</li>\n","<li>Loss functions: perceptron, margin-based, and negative log-likelihood</li>\n","<li>Applications to classification, regression, and structured output</li>\n","</ul>"]},{"cell_type":"markdown","id":"fdef5f69","metadata":{"id":"fdef5f69"},"source":["## 1. Energy Functions and Compatibility\n","\n","EBMs model the relationship between input $x$ and output $y$ using an energy function $E(x, y; \\theta)$, where $\\theta$ represents the model parameters. Lower energy values indicate higher compatibility between $x$ and $y$.\n","\n","- **Definition**: $E(x, y; \\theta)$ is a scalar-valued function that quantifies the \"cost\" or \"incompatibility\" of a given input-output pair.\n","- **Goal**: Learn $\\theta$ such that $E(x, y_{\\text{true}}; \\theta)$ is low for correct pairs and high for incorrect pairs.\n","- **Example**: For a binary classification task, the energy function might be a linear model:  \n","  $$ E(x, y; w) = -y \\cdot (w^T x + b) $$  \n","  where $y \\in \\{+1, -1\\}$, $w$ is a weight vector, and $b$ is a bias term. A lower energy for $y = +1$ indicates that the positive class is more compatible with the input $x$."]},{"cell_type":"markdown","id":"dcf7a8d9","metadata":{"id":"dcf7a8d9"},"source":["### 1.1. Compatibility\n","\n","Compatibility is inversely related to energy. A pair $(x, y)$ is more compatible if $E(x, y; \\theta)$ is lower. For structured outputs, $y$ may be a sequence or graph, and the energy function often decomposes into local factors over parts of $y$. For example, in sequence labeling, the energy might include terms for individual labels and transitions between labels, as seen in Conditional Random Fields (CRFs)."]},{"cell_type":"markdown","id":"66849cd3","metadata":{"id":"66849cd3"},"source":["<img src=\"https://i.postimg.cc/jdKNjKZz/image.png\" alt=\"Energy function diagram\">\n","\n","**Note**: A model measures the compatibility between observed variables X and variables to\n","be predicted Y using an energy function E(Y,X). For example, X could be the pixels of an\n","image, and Y a discrete label describing the object in the image. Given X, the model produces\n","the answer Y that minimizes the energy E."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OAJb_1gd2vn","executionInfo":{"status":"ok","timestamp":1752136412965,"user_tz":-345,"elapsed":29192,"user":{"displayName":"RONAK SHRESTHA","userId":"11319184517876563645"}},"outputId":"644876ef-87c4-491b-e438-3f60a01db0a3"},"id":"3OAJb_1gd2vn","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"a1f19e1e","metadata":{"id":"a1f19e1e"},"source":["<img src=\"image2.png\" alt=\"Applications of EBMs\">\n","\n","**Note**: Several applications of EBMs: (a) face recognition: Y is a high-cardinality discrete\n","variable; (b) face detection and pose estimation: Y is a collection of vectors with location\n","and pose of each possible face; (c) image segmentation: Y is an image in which each pixel\n","is a discrete label; (d-e) handwriting recognition and sequence labeling: Y is a sequence of\n","symbols from a highly structured but potentially infinite set (the set of English sentences). The\n","situation is similar for many applications in natural language processing and computational\n","biology; (f) image restoration: Y is a high-dimensional continuous variable (an image)."]},{"cell_type":"markdown","id":"bbccca2f","metadata":{"id":"bbccca2f"},"source":["## 2. Inference in EBMs\n","\n","Inference in EBMs involves finding the output $y$ that minimizes the energy for a given input $x$:\n","\n","$$ y^* = \\arg\\min_{y \\in \\mathcal{Y}} E(x, y; \\theta) $$\n","\n","### 2.1. Deterministic Inference\n","- Directly compute $y^*$ by minimizing the energy function.\n","- Suitable for simple output spaces (e.g., binary classification).\n","- Example: For linear energy, select the class with the lowest energy.\n","\n","### 2.2. Probabilistic Inference\n","- Instead of a single $y^*$, compute a probability distribution over $y$:  \n","  $$ P(y|x; \\theta) = \\frac{\\exp(-E(x, y; \\theta))}{\\sum_{y' \\in \\mathcal{Y}} \\exp(-E(x, y'; \\theta))} $$  \n","- The denominator (partition function) is often intractable for complex $\\mathcal{Y}$.\n","- Approximation methods: Gibbs sampling, Markov Chain Monte Carlo (MCMC).\n","\n","### 2.3. Practical Example: Binary Classification\n","\n","Letâ€™s implement a simple EBM for binary classification using a linear energy function. The code below defines an energy function, performs deterministic inference, and tests it on sample data."]},{"cell_type":"code","execution_count":null,"id":"f6179b54","metadata":{"id":"f6179b54","outputId":"00b3d569-c1e5-4dac-9a0a-3b6571eed70a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions: [1, 1, -1, -1]\n"]}],"source":["import numpy as np\n","\n","# Define energy function with bias\n","def energy(x, y, w, b):\n","    \"\"\"Compute energy for input x, output y, weights w, and bias b.\"\"\"\n","    return -y * (np.dot(w, x) + b)\n","\n","# Deterministic inference\n","def infer(x, w, b):\n","    \"\"\"Infer the class (+1 or -1) with the lowest energy.\"\"\"\n","    y_pos = energy(x, 1, w, b)\n","    y_neg = energy(x, -1, w, b)\n","    return 1 if y_pos < y_neg else -1\n","\n","# Example data\n","X = np.array([[1, 2], [2, 1], [-1, -1], [-2, -2]])\n","y = np.array([1, 1, -1, -1])\n","w = np.array([0.5, 0.5])\n","b = 0.0\n","\n","# Inference\n","predictions = [infer(x, w, b) for x in X]\n","print(\"Predictions:\", predictions)"]},{"cell_type":"markdown","id":"75fd2d1f","metadata":{"id":"75fd2d1f"},"source":["**Code Explanation**:\n","- **Energy Function**: The function `energy(x, y, w, b)` computes the energy as $-y \\cdot (w^T x + b)$. A lower energy indicates a more compatible $(x, y)$ pair.\n","- **Inference**: The `infer(x, w, b)` function compares the energy for $y = +1$ and $y = -1$, selecting the class with the lower energy.\n","- **Data**: The input `X` contains four 2D points, and `y` contains their true labels. The weights `w` and bias `b` are initialized to simple values for demonstration.\n","- **Output**: The predictions match the true labels, indicating the weights and bias are reasonable for this toy dataset.\n","\n","**Exercise**: Adjust $w$ and $b$ to observe their impact. For example, try $w = [1, 0]$ or $b = 1$ and rerun the inference to see how the predictions change."]},{"cell_type":"markdown","id":"0b41b981","metadata":{"id":"0b41b981"},"source":["## 3. Learning in EBMs\n","\n","Learning in EBMs involves optimizing the parameters $\\theta$ to minimize the energy for correct input-output pairs while increasing it for incorrect pairs. This is achieved using various loss functions, which we describe below.\n","\n","### 3.1 Perceptron Loss\n","- **Update Rule**: Updates parameters when the predicted output differs from the true output:\n","  $$ \\theta \\leftarrow \\theta + \\eta (y_{\\text{true}} - y_{\\text{pred}}) \\nabla_\\theta E(x, y_{\\text{true}}; \\theta) $$\n","- **Loss Function**: Measures the difference in energy between the true and predicted outputs:\n","  $$ L = \\max(0, E(x, y_{\\text{true}}; \\theta) - E(x, y_{\\text{pred}}; \\theta)) $$\n","- **Explanation**: The perceptron loss is zero if the predicted output has lower energy than the true output. Otherwise, it penalizes the model proportional to the energy difference. It has a margin of zero, which may lead to less robust solutions.\n","\n","### 3.2 Margin-Based Loss\n","- **Loss Function**: Introduces a margin $\\Delta$ to ensure a separation between correct and incorrect outputs:\n","  $$ L = \\max(0, E(x, y_{\\text{true}}; \\theta) - E(x, y_{\\text{incorrect}}; \\theta) + \\Delta) $$\n","- **Explanation**: The margin $\\Delta$ ensures that the energy of incorrect outputs is higher than that of correct outputs by at least $\\Delta$. This makes the model more robust to noise and outliers compared to perceptron loss.\n","\n","### 3.3 Negative Log-Likelihood Loss\n","- **Loss Function**: Combines the energy of the true output with the log partition function:\n","  $$ L = E(x, y_{\\text{true}}; \\theta) + \\log Z(x; \\theta) $$\n","  where $Z(x; \\theta) = \\sum_{y' \\in \\mathcal{Y}} \\exp(-E(x, y'; \\theta))$ is the partition function.\n","- **Explanation**: This loss encourages the model to assign low energy to the true output while normalizing over all possible outputs. The partition function can be computationally expensive, requiring approximation methods like MCMC for complex output spaces."]},{"cell_type":"markdown","id":"a1dfefbc","metadata":{"id":"a1dfefbc"},"source":["### 3.4 Practical Example: Training with Perceptron Loss\n","\n","The following code trains a binary classification EBM using perceptron loss. We correct the original code to ensure it runs without errors and add detailed comments."]},{"cell_type":"code","execution_count":null,"id":"a72dac30","metadata":{"id":"a72dac30","outputId":"6d90e9a2-1d94-47d7-c4ee-ef9f95697cb9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Learned weights: [0.1 0.2] Bias: 0.1\n","Predictions after training: [1, 1, -1, -1]\n"]}],"source":["def perceptron_train(X, y, w, b, lr=0.1, epochs=10):\n","    \"\"\"Train an EBM using perceptron loss for binary classification.\n","\n","    Args:\n","        X: Input data (numpy array of shape [n_samples, n_features]).\n","        y: True labels (numpy array of shape [n_samples], values in {+1, -1}).\n","        w: Initial weights (numpy array of shape [n_features]).\n","        b: Initial bias (scalar).\n","        lr: Learning rate (float).\n","        epochs: Number of training epochs (int).\n","\n","    Returns:\n","        w: Updated weights.\n","        b: Updated bias.\n","    \"\"\"\n","    for _ in range(epochs):\n","        for x, y_true in zip(X, y):\n","            y_pred = infer(x, w, b)\n","            if y_pred != y_true:\n","                # Update rule: w += lr * y_true * x, b += lr * y_true\n","                w += lr * y_true * x\n","                b += lr * y_true\n","    return w, b\n","\n","# Train\n","w = np.array([0.0, 0.0])\n","b = 0.0\n","w, b = perceptron_train(X, y, w, b, lr=0.1, epochs=10)\n","print(\"Learned weights:\", w, \"Bias:\", b)\n","\n","# Test inference\n","predictions = [infer(x, w, b) for x in X]\n","print(\"Predictions after training:\", predictions)"]},{"cell_type":"markdown","id":"f08498cd","metadata":{"id":"f08498cd"},"source":["**Code Explanation**:\n","- **Training Loop**: Iterates over the dataset for a specified number of epochs, updating weights and bias when the predicted label differs from the true label.\n","- **Update Rule**: The perceptron update rule adjusts the weights and bias to reduce the energy of the true label relative to the predicted label.\n","- **Output**: The learned weights and bias should improve the model's ability to correctly classify the input data.\n","\n","**Exercise**: Implement a margin-based loss (e.g., square-square loss) and compare its performance with perceptron loss. Below, we provide an implementation of the square-square loss to address this exercise."]},{"cell_type":"markdown","id":"220409d0","metadata":{"id":"220409d0"},"source":["## 4. Applications of EBMs\n","\n","EBMs are versatile and can be applied to various tasks. Below, we describe the theoretical formulations and provide practical application cases with code examples.\n","\n","### 4.1 Classification\n","- **Energy Function**: For $K$-class classification:\n","  $$ E(x, y; \\theta) = -f_\\theta(x)_y $$\n","  where $f_\\theta(x)_y$ is the score for class $y$. The class with the lowest energy is selected.\n","- **Application: Handwritten Digit Recognition**\n","  EBMs can be used to classify handwritten digits (e.g., MNIST dataset). The energy function assigns a score to each digit class, and the model selects the digit with the lowest energy.\n","\n","**Example Code**: Below is an implementation of a multi-class EBM for digit classification using a simple linear model."]},{"cell_type":"markdown","id":"4c80961a","metadata":{"id":"4c80961a"},"source":["### 4.2 Regression\n","- **Energy Function**: For regression:\n","  $$ E(x, y; \\theta) = \\frac{1}{2}(y - f_\\theta(x))^2 $$\n","  where $f_\\theta(x)$ is the predicted output, and the energy measures the squared error.\n","- **Application: House Price Prediction**\n","  EBMs can predict continuous values like house prices based on features such as size and location."]},{"cell_type":"markdown","id":"49190d4d","metadata":{"id":"49190d4d"},"source":["### 4.3 Structured Prediction\n","- **Energy Function**: For sequence labeling (e.g., CRFs):\n","  $$ E(x, y; \\theta) = -\\sum_{t=1}^T \\psi_t(y_t, x; \\theta) - \\sum_{t=1}^{T-1} \\phi_t(y_t, y_{t+1}) $$\n","  where $\\psi_t$ models the compatibility of label $y_t$ with input $x$, and $\\phi_t$ models transitions between labels.\n","- **Application: Part-of-Speech Tagging**\n","  EBMs can be used for sequence labeling tasks like part-of-speech (POS) tagging, where the goal is to assign grammatical categories to words in a sentence."]},{"cell_type":"markdown","id":"7048b7c2","metadata":{"id":"7048b7c2"},"source":["### 4.4. Practical Example: Regression with EBM\n","\n","The following code implements a regression EBM, corrected to ensure it runs and enhanced with comments and evaluation."]},{"cell_type":"code","execution_count":null,"id":"c9a5bbe2","metadata":{"id":"c9a5bbe2"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","# Energy function\n","def energy_regression(x, y, w, b):\n","    \"\"\"Compute energy as squared error for regression.\"\"\"\n","    y_pred = np.dot(w, x) + b\n","    return 0.5 * (y - y_pred) ** 2\n","\n","# Inference\n","def infer_regression(x, w, b):\n","    \"\"\"Predict output by minimizing energy (trivial for squared loss).\"\"\"\n","    return np.dot(w, x) + b\n","\n","# Gradient descent training\n","def train_regression(X, y, w, b, lr=0.01, epochs=100):\n","    \"\"\"Train regression EBM using gradient descent.\"\"\"\n","    for _ in range(epochs):\n","        for x, y_true in zip(X, y):\n","            y_pred = infer_regression(x, w, b)\n","            error = y_true - y_pred\n","            w += lr * error * x\n","            b += lr * error\n","    return w, b\n","\n","# Example data\n","X = np.array([[1], [2], [3], [4]])\n","y = np.array([2, 4, 5, 8])\n","w = np.array([0.0])\n","b = 0.0\n","\n","# Train\n","w, b = train_regression(X, y, w, b, lr=0.01, epochs=100)\n","print(\"Learned w:\", w, \"b:\", b)\n","\n","# Predictions\n","predictions = [infer_regression(x, w, b) for x in X]\n","print(\"Predictions:\", predictions)\n","print(\"MSE:\", mean_squared_error(y, predictions))"]},{"cell_type":"markdown","id":"new_regression_explanation","metadata":{"id":"new_regression_explanation"},"source":["**Code Explanation**:\n","- **Energy Function**: The energy is half the squared error between the true and predicted outputs, ensuring a convex optimization problem.\n","- **Inference**: The prediction is simply $w^T x + b$, as minimizing the squared error is trivial.\n","- **Training**: Gradient descent updates the weights and bias to minimize the energy (squared error).\n","- **Data**: A simple dataset with one feature is used for demonstration. In practice, this can be extended to real-world datasets like house prices.\n","\n","**Exercise**: Extend to a non-linear $E$ using polynomial features. Below, we provide an implementation to address this exercise."]},{"cell_type":"markdown","id":"bc6c55a1","metadata":{"id":"bc6c55a1"},"source":["## 5. Summary\n","\n","Key components of EBMs:\n","1. **Energy Function** $E(x, y; \\theta)$: Quantifies compatibility between input and output.\n","2. **Inference**: Finds the output that minimizes energy (deterministic or probabilistic).\n","3. **Learning**: Optimizes parameters using loss functions like perceptron, margin-based, or negative log-likelihood.\n","\n","EBMs are powerful due to their flexibility and ability to handle complex tasks, as demonstrated in the classification and regression examples."]},{"cell_type":"markdown","id":"598403b6","metadata":{"id":"598403b6"},"source":["**Reference**:\n","\n","- LeCun, Y., et al. (2006). *A Tutorial on Energy-Based Learning*. In *Predicting Structured Data*, MIT Press.\n","<br>\n","<a href =\"https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=7fc604e1a3e45cd2d2742f96d62741930a363efa\">Link</a>"]},{"cell_type":"markdown","id":"27cb0312","metadata":{"id":"27cb0312"},"source":["## 6. Further Reading\n","- Explore Conditional Random Fields (CRFs), contrastive divergence, and Graph Transformer Networks (GTNs).\n","- Goodfellow et al. (2016) - *Deep Learning* (Chapter 18).\n","- Lafferty, J., McCallum, A., & Pereira, F. (2001). *Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data*."]},{"cell_type":"markdown","id":"dda3e6c4","metadata":{"id":"dda3e6c4"},"source":["## 7. Exercises\n","1. **Implement multi-class EBM with softmax inference**: Extend the binary classification example to use probabilistic inference with softmax.\n","2. **Compare different margin values $\\Delta$ in margin-based loss**: Test the square-square loss with different margins (e.g., $\\Delta = 0.5, 1.0, 2.0$).\n","3. **Apply EBM to sequence labeling with local factors**: Implement a simple CRF-like model for a toy sequence labeling task."]}],"metadata":{"kernelspec":{"display_name":"week4_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}