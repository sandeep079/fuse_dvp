{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MoCo: Momentum Contrast for Unsupervised Visual Representation Learning\n","\n","### Pre-requisites\n","\n","* Understanding of contrastive learning principles\n","* Familiarity with convolutional neural networks (CNNs)\n","* Prior knowledge of SimCLR helps (especially issues with large batch sizes)"],"metadata":{"id":"hyCOip_IMLqK"}},{"cell_type":"markdown","source":["\n","In contrastive learning, SimCLR requires large batch sizes to provide enough negative examples, which can be computationally expensive. **MoCo (Momentum Contrast)** offers an alternative approach that enables learning strong representations with **smaller batches** by maintaining a **dynamic memory bank (queue)** of negative samples.\n","\n","> **Analogy**: Imagine you're learning to recognize a celebrity's face. Instead of comparing with only the people around you (like SimCLR), you carry a photo album of many people you've seen before and use that for comparison. MoCo builds this \"album\" dynamically and keeps it consistent using momentum updates.\n","\n","This **analogy** highlights MoCo's efficiency it allows for a rich and diverse set of comparisons (negative samples) without the need to process all of them within a single, massive batch, which is crucial for practical representation learning."],"metadata":{"id":"77QW_3KGMRiC"}},{"cell_type":"markdown","source":["\n","## **Core Idea**\n","\n","<center>\n","  <img src=\"\n","https://i.postimg.cc/85VSr8x5/MoCo.gif\" width=50%>\n","</center>\n","\n","MoCo replaces the need for large in-batch negatives by maintaining a **large dictionary (queue)** of past embeddings. Instead of sampling negatives from the current mini-batch, MoCo keeps a **memory queue of encoded keys from previous batches**.\n","\n","> It uses two encoders:\n",">\n","> * A **query encoder** updated via backpropagation\n","> * A **key encoder** updated slowly by momentum from the query encoder\n","\n","This ensures that the dictionary remains **consistent** across training steps, which is critical for contrastive learning.\n","\n","This consistency is vital because if the negative samples in the queue were too stale (i.e., encoded by an outdated key encoder), they wouldn't provide reliable gradients for the query encoder, hindering effective representation learning."],"metadata":{"id":"Mh0sA93mMUIn"}},{"cell_type":"markdown","source":["\n","\n","## **Architecture Overview**\n","\n","<center>\n","  <img src=\"https://i.postimg.cc/cLZ4PnFC/image.png\" width=45%>\n","</center>\n","\n","### **1. Data Augmentation**\n","\n","Like SimCLR, MoCo uses **stochastic data augmentation** to create two correlated views:\n","\n","* $x_q$: the query image (input to query encoder)\n","* $x_k$: the key image (input to key encoder)\n","\n","These are generated from the same image using augmentations such as:\n","\n","* Random cropping and resizing\n","* Color jittering\n","* Gaussian blur\n","\n","### **2. Dual Encoders**\n","\n","MoCo maintains **two neural network encoders**:\n","\n","* **Query encoder** $f_q(\\cdot)$: learns through backpropagation\n","* **Key encoder** $f_k(\\cdot)$: updated by momentum from the query encoder\n","\n","Instead of training both encoders directly, the key encoder is updated with a **momentum update rule**:\n","\n","$$\n","\\theta_k \\leftarrow m \\cdot \\theta_k + (1 - m) \\cdot \\theta_q\n","$$\n","\n","Where:\n","\n","* $\\theta_k$ and $\\theta_q$ are the parameters of the key and query encoders.\n","* $m \\in [0, 1)$ is the momentum coefficient (typically 0.999).\n","\n","> This slow-moving average update makes the key encoder's output more consistent across training iterations, leading to a more stable contrastive dictionary.\n","\n","The momentum update ensures that the key encoder's representations evolve smoothly. This stability means that the negative samples in the queue remain relevant over time, even as the query encoder (and thus the learned representations) are constantly improving.\n","\n","### **3. Memory Bank / Queue**\n","\n","MoCo maintains a **FIFO (First-In-First-Out) queue** of encoded keys from previous batches, acting as a dictionary of negative samples.\n","\n","* When a new key $k$ is computed, it is enqueued.\n","* The oldest key in the queue is dequeued.\n","\n","> This allows MoCo to compare the query with **thousands of negatives** without needing large batch sizes while still achieving effective contrastive learning.\n","\n","The large number of negative samples in the queue is crucial for effective contrastive learning, as it helps the model learn to distinguish between very similar and very different inputs, thereby learning more discriminative representations.\n","\n","### **4. Contrastive Loss (InfoNCE)**\n","\n","MoCo uses a **contrastive loss** similar to SimCLR:\n","\n","Given a query $q$ and its positive key $k^+$, and many negative keys $k^- \\in \\mathcal{K}$ from the queue:\n","\n","$$\n","\\mathcal{L}_q = -\\log \\frac{\\exp(q \\cdot k^+ / \\tau)}{\\exp(q \\cdot k^+ / \\tau) + \\sum_{k^- \\in \\mathcal{K}} \\exp(q \\cdot k^- / \\tau)}\n","$$\n","\n","Where:\n","\n","* $q$ is the query embedding from $f_q$\n","* $k^+$ is the positive key embedding from $f_k$\n","* $\\mathcal{K}$ is the set of negative keys in the memory queue\n","* $\\tau$ is the temperature hyperparameter (e.g. 0.07)\n","\n","This loss pushes $q$ and $k^+$ closer while pushing away all $k^-$.\n","\n","This loss function is the engine of representation learning in MoCo. By minimizing it, the model learns to map semantically similar inputs (positive pairs) to nearby points in the embedding space, while mapping dissimilar inputs (negative pairs) to distant points. This forces the encoders to learn meaningful, robust, and transferable features."],"metadata":{"id":"iKL53dMKMZe9"}},{"cell_type":"markdown","source":["## **Key Advantages Over SimCLR**\n","\n","<center>\n","\n","| Feature                | SimCLR             | MoCo                           |\n","| ---------------------- | ------------------ | ------------------------------ |\n","| Negative Samples       | In-batch           | From memory queue              |\n","| Batch Size Requirement | Large (e.g., 4096) | Small (e.g., 256)              |\n","| Encoder Updates        | Single encoder     | Momentum-updated dual encoders |\n","| Dictionary Consistency | Inconsistent       | Slowly evolving via momentum   |\n","\n","</center>\n","\n","> MoCo's use of a dynamic and consistent memory dictionary makes it more scalable and stable under limited resources.\n","\n","The ability to use smaller batch sizes is a significant practical advantage, making MoCo accessible for training on more modest hardware while still achieving high-quality representation learning.\n","\n"],"metadata":{"id":"30gVX6gsMh0G"}},{"cell_type":"markdown","source":["## **Downstream Usage**\n","\n","\n","After pretraining, the **projection head is discarded**, and the query encoder is used to generate representations for downstream tasks like:\n","\n","- **Image classification**\n","\n","- **Object detection**\n","\n","- **Semantic segmentation**\n","\n","Evaluation protocols:\n","\n","- **Linear evaluation**: Train a simple classifier on frozen encoder features\n","\n","- **Fine-tuning**: Update encoder and task-specific layers jointly\n","\n","The effectiveness of these learned representations on various downstream tasks is the ultimate measure of successful representation learning. If the features extracted by MoCo lead to high performance in these tasks, it indicates that the model has learned generalizable and meaningful representations of the visual world."],"metadata":{"id":"01dnaI3yRKxK"}},{"cell_type":"markdown","source":["## **Challenges and Considerations**\n","\n","* **Momentum coefficient**: Needs careful tuning; too low causes instability, too high slows adaptation\n","* **Queue size**: A small memory queue reduces negative diversity; large size increases memory usage\n","* **Representation drift**: If the key encoder lags too far behind the query encoder, learning slows\n","\n","> Despite these challenges, MoCo offers a robust framework for unsupervised visual representation learning."],"metadata":{"id":"8q9nKRqNQ5TR"}},{"cell_type":"markdown","source":["\n","## **Conclusion**\n","\n","**MoCo** is a powerful contrastive learning framework that addresses SimCLR’s dependency on large batch sizes by using a **momentum encoder** and a **memory queue** of negative examples.\n","\n","Its innovative architecture allows effective and scalable training of self-supervised models with limited computational resources.\n","\n","MoCo has influenced several advancements in the field, including:\n","\n","* **MoCo v2**: which incorporates stronger data augmentation and an MLP projection head\n","* **MoCo v3**: which adapts MoCo for transformer-based architectures\n","\n","MoCo remains a foundational technique in the landscape of contrastive learning and self-supervised learning.\n","\n","Its success underscores the importance of carefully designed architectural components and training strategies in pushing the boundaries of unsupervised representation learning, allowing models to learn powerful features from raw, unlabeled data."],"metadata":{"id":"4-eqjC9nQ-b-"}},{"cell_type":"markdown","source":["## **References**\n","\n","* He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). *Momentum Contrast for Unsupervised Visual Representation Learning*. arXiv. [https://arxiv.org/abs/1911.05722](https://arxiv.org/abs/1911.05722)\n","\n","* db (2022, July 1). *Contrastive Learning: Comparison Between Architectures of MoCo and SimCLR*. Medium. [https://medium.com/@dbaofd/contrastive-learning-comparison-between-moco-and-simclr-52918fedaddd](https://medium.com/@dbaofd/contrastive-learning-comparison-between-moco-and-simclr-52918fedaddd)\n","\n","* Tsang, S.-H. (2022, February 6). *Review — MoCo: Momentum Contrast for Unsupervised Visual Representation Learning*. Medium. [https://sh-tsang.medium.com/review-moco-momentum-contrast-for-unsupervised-visual-representation-learning-99b590c042a9](https://sh-tsang.medium.com/review-moco-momentum-contrast-for-unsupervised-visual-representation-learning-99b590c042a9)\n","\n","\n","## **Code Tutorial Reference**\n","\n","* MyScale Team. (2024, September 16). [*An In-Depth Guide to Contrastive Learning: Techniques, Models, and Applications*](https://myscale.com/blog/what-is-contrastive-learning/#code-example)"],"metadata":{"id":"p8j_mw0FTKQb"}}]}