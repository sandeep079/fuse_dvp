{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SimCLR: A Simple Framework for Contrastive Learning of Visual Representations\n","\n","### Pre-requistes\n","\n","- Understanding of representational learning\n","- Familiarity with Computer Vision Basics\n","- Knowledge of ResNets or CNN architectures"],"metadata":{"id":"Go8AiplSgt2a"}},{"cell_type":"markdown","source":["In supervised learning, labeled datasets guide the model to learn meaningful representations, but labeling is often costly and time consuming. **SimCLR (A Simple Framework for Contrastive Learning of Visual Representations)** aims to bypass this by learning powerful visual representations **without labels**, purely from **data augmentations**. The key idea is that two augmented views of the same image should have similar representations, while views from different images should differ.\n","\n","> **Analogy**: Imagine you're identifying your friend in a crowd. Whether your friend wears a hat or glasses, or even if they change clothes, you still recognize them. These variations are like data augmentations. SimCLR learns a representation that is invariant to such changes."],"metadata":{"id":"VUWmCgOAg5hf"}},{"cell_type":"markdown","source":["\n","## **Core Idea**\n","\n","<center>\n","\n","<img src=\"https://i.postimg.cc/3NPT6pPS/SimCLR.gif\" width=50%>\n","</center>\n","\n","SimCLR learns by **maximizing the similarity between augmented views of the same image** (positive pairs) while **minimizing similarity between different images** (negative pairs). This process creates a feature space where similar images cluster together, and dissimilar ones are far apart.\n","\n","### **Process Overview:**\n","\n","* For each image in the batch, generate **two different augmented views**. (example one cropped and one color-distorted version).\n","* Pass all augmented views through a **shared encoder network** (example ResNet) to obtain their representations.\n","* Apply a **contrastive loss (NT-Xent)** to pull positive pairs (augmented views of the same image) closer together and **pushes negative pairs** (views of different images) apart.\n"],"metadata":{"id":"aGEuO_jmg2k-"}},{"cell_type":"markdown","source":["\n","## **Major Components of SimCLR**\n","\n","<center>\n","\n","<img src=\"https://i.postimg.cc/4xwSb8D2/simclr-general-architecture.png\" width=75%>\n","</center>\n","\n","### **1. Data Augmentation**\n","\n","The data augmentation module generates two different augmented views of the same image. These views are meant to be semantically similar, so the model learns to map them close together in the representation space.\n","\n","For a given image, two stochastic augmentations are applied to produce a pair of correlated views:\n","\n","* \\$\\hat{x}\\_i\\$: first augmented view\n","* \\$\\hat{x}\\_j\\$: second augmented view\n","\n","The following augmentations are applied sequentially:\n","\n","* **Random cropping and resizing** to the original size, simulating different perspectives.\n","* **Random color distortion** (adjusting brightness, contrast, saturation, and hue) to mimic lighting changes.\n","* **Random Gaussian blur** to introduce slight blurriness, encouraging robustness.\n","\n","> The combination of random cropping and color distortion has been shown to be particularly effective for learning useful representations.\n","\n","### **2. Base Encoder**\n","\n","A deep neural network encoder \\$f(\\cdot)\\$ is used to extract high-level feature representations from the augmented images.\n","\n","A commonly used encoder is **ResNet**, applied as follows:\n","\n","$$\n","\\mathbf{h}_i = f(\\hat{x}_i) = \\text{ResNet}(\\hat{x}_i)\n","$$\n","\n","Here:\n","\n","* \\$\\hat{x}\\_i\\$ is the augmented input image.\n","* \\$\\mathbf{h}\\_i\\$ is the resulting **\\$d\\$-dimensional feature vector**, typically taken from the output of the average pooling layer of ResNet.\n","* \\$\\mathbf{h}\\_i\\$ and \\$\\mathbf{h}\\_j\\$ (from \\$\\hat{x}\\_j\\$) are expected to be close in feature space for positive pairs.\n","\n","### **3. Projection Head**\n","\n","The projection head \\$g(\\cdot)\\$ is a small neural network that maps the encoder’s output \\$\\mathbf{h}\\_i\\$ to a new space where the contrastive loss is applied. This helps improve the quality of learned representations.\n","\n","It is typically a **Multi-Layer Perceptron (MLP)** with one hidden layer:\n","\n","$$\n","\\mathbf{z}_i = g(\\mathbf{h}_i) = W^{(2)} \\sigma(W^{(1)} \\mathbf{h}_i)\n","$$\n","\n","Where:\n","\n","* \\$\\sigma\\$ is a ReLU activation function.\n","* \\$\\mathbf{z}\\_i\\$ is the final vector used in the contrastive loss computation.\n","* This step improves learning but \\$\\mathbf{h}\\_i\\$ (not \\$\\mathbf{z}\\_i\\$) is used as the final representation for downstream tasks.\n","\n","### **4. Contrastive Loss**\n","\n","SimCLR uses a **contrastive loss function** to learn meaningful representations by bringing embeddings of similar (positive) pairs closer together, while pushing embeddings of dissimilar (negative) pairs farther apart in the projected space.\n","\n","> #### **Setup**\n",">\n","> * Let the mini-batch contain **\\$N\\$ original images**.\n","> * After augmentation (two views per image), the batch size becomes **\\$2N\\$ samples**.\n","> * For each positive pair \\$(i, j)\\$ (two augmented views of the same image), the remaining \\$2(N-1)\\$ augmented examples serve as **negative samples**.\n","\n","\n","#### **4.1 Cosine Similarity**\n","\n","For two embeddings \\$\\mathbf{z}\\_i\\$ and \\$\\mathbf{z}\\_j\\$, the **cosine similarity** is calculated as:\n","\n","$$\n","\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j) = \\frac{\\mathbf{z}_i \\cdot \\mathbf{z}_j}{\\|\\mathbf{z}_i\\| \\|\\mathbf{z}_j\\|}\n","$$\n","\n","* Here, \\$\\cdot\\$ denotes the dot product.\n","* \\$|\\mathbf{z}\\_i|\\$ and \\$|\\mathbf{z}\\_j|\\$ are the vector norms (magnitudes).\n","* The similarity ranges from -1 (opposite) to 1 (identical direction).\n","\n","A **temperature parameter** \\$\\tau\\$ is used to scale these similarities before applying the loss, controlling how \"soft\" or \"hard\" the penalty for mismatches is.\n","\n","\n","#### **4.2 NT-Xent Loss**\n","\n","The **Normalized Temperature-Scaled Cross-Entropy (NT-Xent) loss** for a positive pair \\$(i, j)\\$ is defined as:\n","\n","$$\n","\\ell_{i,j} = -\\log \\frac{\n","\\exp\\left(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j)/\\tau\\right)\n","}{\n","\\sum_{k=1}^{2N} \\mathbb{I}_{[k \\neq i]} \\exp\\left(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k)/\\tau\\right)\n","}\n","$$\n","\n","Where:\n","\n","* The numerator is the scaled similarity between the positive pair \\$(i, j)\\$.\n","* The denominator sums over all other embeddings in the batch **except** the anchor \\$\\mathbf{z}\\_i\\$, treating them as negative pairs.\n","* \\$\\mathbb{I}\\_{\\[k \\neq i]}\\$ is an indicator function that excludes the anchor itself from the denominator.\n","\n","#### **4.3 Symmetric Loss**\n","\n","We calculate this loss twice for each positive pair:\n","\n","* Once treating \\$\\mathbf{z}\\_i\\$ as the anchor and \\$\\mathbf{z}\\_j\\$ as the positive.\n","* Once with roles reversed (anchor \\$\\mathbf{z}\\_j\\$ and positive \\$\\mathbf{z}\\_i\\$).\n","\n","\n","#### **4.4 Final Loss**\n","\n","The total loss is the average over **all positive pairs** in the batch and both directions.\n","\n","> This loss encourages the model to:\n",">\n","> * **Attract** embeddings of augmented views of the *same* image (positive pairs).\n","> * **Repel** embeddings of views from *different* images (negative pairs).\n","\n","Thus, the model learns a representation space where similar images are close and different images are far apart.\n"],"metadata":{"id":"Zcvq8Nm6KC1a"}},{"cell_type":"markdown","source":["\n","### **Key Insights:**\n","\n","* **Large batch size** helps the model see more negative samples, which improves performance.\n","* **Longer training** also leads to better representations.\n"],"metadata":{"id":"5_-Wq8XjJ_JB"}},{"cell_type":"markdown","source":["## **Downstream Tasks**\n","\n","After training, the SimCLR model can be used for **transfer learning** on downstream tasks like classification, object detection, or segmentation:\n","\n","* The **projection head** is discarded.\n","* The **representations from the base encoder** (i.e., the output before the projection head) are used as learned features.\n","* These features serve as input to a simple classifier or other task-specific models, often resulting in improved performance thanks to the rich representations learned during contrastive training.\n","\n","Example: A SimCLR model trained on ImageNet can be fine tuned for a medical imaging task, using the learned features to classify X-ray images with minimal labeled data."],"metadata":{"id":"kRrQYwCTVCOd"}},{"cell_type":"markdown","source":["## **Evaluating SimCLR Models**\n","\n","To assess the quality of SimCLR’s representations, common evaluation protocols include:\n","\n","*   **Linear Evaluation**: Train a linear classifier on the frozen encoder outputs ($\\mathbf{h}_i$) and evaluate on a labeled dataset (example ImageNet). High accuracy indicates strong representations.\n","\n","*  **K-Nearest Neighbors (k-NN)**: Use the feature space to find the k-nearest neighbors for a test image and predict its class based on majority voting.\n","\n","*   **Downstream Task Performance**: Fine-tune the model on tasks like object detection or semantic segmentation and compare performance to supervised baselines."],"metadata":{"id":"kJjQgUp5Pk4P"}},{"cell_type":"markdown","source":["## **Challenges and Limitations**\n","SimCLR is powerful but has challenges:\n","\n","\n","\n","*   **Large Batch Sizes**: SimCLR relies on large batches (4096 images) for sufficient negative samples, requiring significant computational resources.\n","*  **Augmentation Sensitivity:** The choice and strength of augmentations heavily impact performance. Poor augmentations may lead to trivial representations.\n","*   **False Negatives**: Random negative sampling may include images that are semantically similar to the anchor, confusing the model.\n","*   **Computational Cost**: Training with large batches and deep encoders like ResNet is resource-intensive.\n","\n","**Solutions:** Methods like **MoCo** use a memory bank to reduce batch size dependence, while **BYOL** avoids negative pairs entirely."],"metadata":{"id":"GFvGkGQXSAHJ"}},{"cell_type":"markdown","source":["\n","## **References**\n","\n","* Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020, February). [*A simple framework for contrastive learning of visual representations*. arXiv](https://arxiv.org/abs/2002.05709)\n","\n","* Chaudhary, A. (2020, March 4). [*The illustrated SimCLR framework*. Amitness.](https://amitness.com/posts/simclr)\n","\n","* Tsang, S.-H. (2020, August 20). [*Review — SimCLR: A simple framework for contrastive learning of visual representations*. Medium.](https://sh-tsang.medium.com/review-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-5de42ba0bc66)\n","\n","## **Code Tutorial Reference**\n","\n","* Lippe, P. (2025, May 1). [*Tutorial 13: Self-Supervised Contrastive Learning with SimCLR*. PyTorch Lightning (Lightning AI)](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/13-contrastive-learning.html) *(CC BY-SA License)*"],"metadata":{"id":"BwqYzD-BbLvw"}}]}