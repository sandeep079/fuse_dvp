{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Key Components of Diffusion Models\n",
        "\n",
        "## Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "\n",
        "- Identify and describe the core components of diffusion models.\n",
        "- Understand the role of the denoising neural network, noise schedule, and sampling process.\n",
        "- Visualize and explain how each component contributes to generating realistic samples.\n",
        "- Compare architecture types like U-Net and Transformer-based models.\n",
        "- Connect theoretical understanding with mathematical intuition and visuals.\n"
      ],
      "metadata": {
        "id": "0mUlxiEfboL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "## Core Components of Diffusion Models\n",
        "\n",
        "Diffusion models are built upon several core components that define how they function and are trained. Understanding these components will help clarify how diffusion transforms noise into meaningful data, especially images."
      ],
      "metadata": {
        "id": "_1UEy73Wdp0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1. Neural Network (Denoising Network)\n",
        "\n",
        "###  Purpose\n",
        "\n",
        "The neural network in diffusion models is trained to predict the noise component added at each timestep during the forward diffusion process.\n",
        "\n",
        "Mathematically, at each timestep \\( t \\), the model learns to estimate:\n",
        "\n",
        "$$\n",
        "\\epsilon_\\theta(x_t, t)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "-  \\(x_t) \\: Noisy image at time step \\( t \\)\n",
        "- t \\: Current timestep\n",
        "- \\$ \\epsilon_\\theta \\$: Noise predicted by the network with parameters \\( \\theta\\)\n",
        "\n",
        "---\n",
        "\n",
        "###  Common Architectures\n",
        "\n",
        "####  U-Net\n",
        "\n",
        "- Popular in image-based diffusion models.\n",
        "- Has a symmetric encoder-decoder structure with skip connections.\n",
        "- Enables the network to capture both global context and fine details.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"Figure 1: Architecture of U-Net\" width=\"600\"/>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.postimg.cc/bNWRMfN1/U-net-Architecture.png\" alt=\"Figure 1: Architecture of U-Net\" width=\"600\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.postimg.cc/FFfpxKhh/Description.png\" alt=\"Figure 1: Architecture of U-Net\" width=\"300\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"center\"><b>Figure 1:</b> Architecture of U-Net</p>\n",
        "\n",
        "\n",
        "\n",
        "####  Transformer-based Networks\n",
        "\n",
        "- Used for text, image, and multimodal diffusion models.\n",
        "- Leverages self-attention to model long-range dependencies.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://erdem.pl/static/2f26fadad0f8290c51b1b8579c008aeb/41d3c/attention.png\" alt=\"Figure 1: Architecture of U-Net\" width=\"400\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"center\"><b>Figure 2:</b> Self-Attention block</p>\n",
        "\n",
        "---\n",
        "\n",
        "###  Function\n",
        "\n",
        "At every step \\( t \\), the denoising network receives:\n",
        "\n",
        "- The noisy input image: \\( x_t \\)\n",
        "- The current timestep: \\( t \\)\n",
        "\n",
        "The model predicts the noise component:\n",
        "\n",
        "$$\n",
        "\\epsilon_\\theta(x_t, t)\n",
        "$$\n",
        "\n",
        "Then the sample for the previous step \\( x_{t-1} \\) is computed as:\n",
        "\n",
        "$$\n",
        "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{1 - \\alpha_t}} \\cdot \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t \\cdot z\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\alpha_t \\text{ and } \\bar{\\alpha}_t$ are noise schedule coefficients.\n",
        "- $\\sigma_t$ is the standard deviation of the added noise at timestep $t$.\n",
        "- $z \\sim \\mathcal{N}(0, I)$ represents standard Gaussian noise.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "\n",
        ">  Think of the denoising network as a *noise cleaner*. Each step, it learns to reverse a tiny part of the corruption caused by noise — gradually revealing the final image.\n",
        "\n",
        "---\n",
        "\n",
        "| Concept                      | Suggested Image Description                                    |\n",
        "|------------------------------|----------------------------------------------------------------|\n",
        "| U-Net Architecture           | Show a U-Net diagram with encoder, decoder, and skip paths    |\n",
        "| Transformer Model (optional) | Display attention layers of Transformer blocks                |\n",
        "| Denoising Process Flow       | Diagram of $$x_t \\rightarrow \\epsilon_\\theta(x_t, t) \\rightarrow x_{t-1}$$ |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yj0bGxfTeIUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Noise Schedule (β Schedule)\n",
        "\n",
        "###  Purpose\n",
        "\n",
        "The **noise schedule** defines how much noise is added to the data at each timestep during the forward diffusion process. It also controls the reverse process by influencing the denoising steps.\n",
        "\n",
        "It is essentially a sequence of variance values \\( \\beta_1, \\beta_2, \\ldots, \\beta_T \\) over \\( T \\) timesteps, determining how rapidly or gradually the data is destroyed.\n",
        "\n",
        "---\n",
        "\n",
        "###  Mathematical Formulation\n",
        "\n",
        "At each step \\( t \\), the noisy image is generated as:\n",
        "\n",
        "$$\n",
        "x_t = \\sqrt{\\bar{\\alpha}_t} \\cdot x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "\\begin{align*}\n",
        "\\alpha_t &= 1 - \\beta_t &&\\text{(signal retention at timestep } t\\text{)} \\\\\n",
        "\\bar{\\alpha}_t &= \\prod_{s=1}^t \\alpha_s &&\\text{(cumulative product over time)} \\\\\n",
        "\\epsilon &\\sim \\mathcal{N}(0, I) &&\\text{(standard Gaussian noise)}\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Types of Noise Schedules\n",
        "\n",
        "#### 1. **Linear Schedule**\n",
        "- $\n",
        "\\beta_t \\text{ increases linearly from a small value to a larger one.}\n",
        "$\n",
        "\n",
        "- Simple and widely used in early diffusion models.\n",
        "- Leads to a uniform degradation of the image.\n",
        "\n",
        "#### 2. **Cosine Schedule**\n",
        "- Introduced in *Improved Denoising Diffusion Probabilistic Models* (Nichol & Dhariwal, 2021).\n",
        "- Begins slowly and increases rapidly.\n",
        "- Produces sharper images and improves training stability.\n",
        "\n",
        "#### 3. **Quadratic / Sigmoid Schedules**\n",
        "- Non-linear increase or S-shaped growth in noise.\n",
        "- Useful for adjusting training dynamics or optimizing model quality.\n",
        "\n",
        "---\n",
        "\n",
        "###  Intuition\n",
        "\n",
        "> The noise schedule acts like a **corruption controller**. In the forward process, it determines **how aggressively** the image gets noised. In the reverse process, it **guides the denoiser** in how much correction to apply.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W9LAIhDymszW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Sampling Procedure in Diffusion Models\n",
        "\n",
        "## Purpose\n",
        "\n",
        "The sampling process is where diffusion models generate data (e.g., images) from pure noise by running the reverse diffusion process. It begins with a randomly sampled noisy input:\n",
        "\n",
        "$$\n",
        "x_T \\sim \\mathcal{N}(0, I)\n",
        "$$\n",
        "\n",
        "Then it gradually denoises it using a trained neural network to reach the final output:\n",
        "\n",
        "$$\n",
        "x_0\n",
        "$$\n",
        "\n",
        "This is how the model synthesizes realistic samples without direct data input—essentially learning to \"imagine from noise.\"\n",
        "\n",
        "## Mathematical Formulation\n",
        "\n",
        "The reverse denoising step at timestep \\( t \\) is usually defined as:\n",
        "\n",
        "$$\n",
        "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\cdot \\epsilon_\\theta(x_t, t) \\right) + \\sigma_t \\cdot z\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $x_t$: Noisy sample at timestep $t$\n",
        "- $\\epsilon_\\theta(x_t, t)$: Noise predicted by the denoising network\n",
        "- $\\alpha_t = 1 - \\beta_t$: Signal retention at timestep $t$\n",
        "- $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$: Cumulative product over timesteps\n",
        "\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "PfDk8jtdo6Br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 4. Training Objective in Diffusion Models\n",
        "\n",
        "### Purpose\n",
        "\n",
        "The goal of training a diffusion model is to teach the neural network to **predict the noise** added to a sample at each timestep during the forward diffusion process.\n",
        "\n",
        "At each timestep $t$, the model tries to estimate the noise $\\epsilon$ such that:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t, t) \\right\\|^2 \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x_0$: Original clean data sample\n",
        "* $\\epsilon$: Actual Gaussian noise added\n",
        "* $x_t$: Noisy version of $x_0$ at timestep $t$\n",
        "* $\\epsilon_\\theta(x_t, t)$: Noise predicted by the neural network with parameters $\\theta$\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.postimg.cc/s215N1dj/Screenshot-2025-06-18-161207.png\" alt=\"Figure 1: Architecture of U-Net\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"center\"><b>Figure 3:</b> The training and sampling algorithms </p>\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "> The model is trained to reverse the noise addition process by minimizing the **Mean Squared Error (MSE)** between the actual noise and the predicted noise.\n",
        "\n",
        "\n",
        "\n",
        "### Variants of the Loss\n",
        "\n",
        "There are multiple formulations of the training objective, including:\n",
        "\n",
        "1. **Predicting the noise** $\\epsilon$:\n",
        "   This is the most common and simple approach.\n",
        "\n",
        "2. **Predicting the original image** $x_0$:\n",
        "   Loss is based on the difference between the true and reconstructed image.\n",
        "\n",
        "3. **Predicting the mean of** $x_{t-1}$:\n",
        "   Involves estimating parameters of the reverse distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PE_rCY3tiCV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "#  Connection to Score-Based Generative Models (SGMs) in Diffusion Models\n",
        "\n",
        "Diffusion models and **Score-Based Generative Models (SGMs)** are two families of generative models that are **closely related**. In fact, diffusion models can be seen as a special case of SGMs. Here's a breakdown of the connection:\n",
        "\n",
        "\n",
        "\n",
        "###  What are Score-Based Generative Models?\n",
        "\n",
        "Score-Based Generative Models learn a **score function**, which is the gradient of the log-probability density of data:\n",
        "\n",
        "$$\n",
        "\\nabla_{x} \\log p(x)\n",
        "$$\n",
        "\n",
        "This score tells us **which direction to move a data point to make it more likely** under the true data distribution.\n",
        "Since the true distribution $p(x)$ is unknown, SGMs learn to approximate this score from noisy versions of the data.\n",
        "\n",
        "\n",
        "\n",
        "###  How this connects to Diffusion Models\n",
        "\n",
        "In **diffusion models**, we gradually add noise to data through a forward process, producing a sequence $x_0 \\rightarrow x_1 \\rightarrow \\cdots \\rightarrow x_T$.\n",
        "At each timestep $t$, the model tries to reverse this process.\n",
        "\n",
        "Diffusion models learn to predict the **noise** added to a data sample $x_t$, using a model $\\epsilon_\\theta(x_t, t)$.\n",
        "It turns out this noise prediction is **mathematically related** to the score function:\n",
        "\n",
        "> The predicted noise is **proportional** to the negative score:\n",
        ">\n",
        "> $$\n",
        "> \\epsilon_\\theta(x_t, t) \\propto -\\nabla_{x_t} \\log p(x_t)\n",
        "> $$\n",
        "\n",
        "This means that **diffusion models are implicitly learning the score function**, just like SGMs.\n",
        "\n",
        "\n",
        "\n",
        "###  Summary of the Connection\n",
        "\n",
        "| Aspect            | Score-Based Models                         | Diffusion Models                    |\n",
        "| ----------------- | ------------------------------------------ | ----------------------------------- |\n",
        "| Goal              | Learn score function: $\\nabla_x \\log p(x)$ | Learn to predict noise $\\epsilon$   |\n",
        "| Method            | Train on noisy data using score matching   | Train on noisy data using MSE loss  |\n",
        "| Generation method | Reverse SDE or Langevin Dynamics           | Reverse diffusion process           |\n",
        "| Noise estimation  | Explicit score learning                    | Implicit score via noise prediction |\n",
        "| Common Ground     | Both learn how to denoise or reverse noise |                                     |\n",
        "\n",
        "\n",
        "\n",
        "###  Intuition\n",
        "\n",
        "Imagine you're climbing a hill (i.e., generating a sample). The **score function** tells you which direction is uphill (toward high data likelihood).\n",
        "By learning to **undo the noise step-by-step**, diffusion models are effectively **learning how to climb back up the hill** from pure noise to realistic data — just like SGMs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dl4sEeapl5tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Diffusion Models: Advantages, Disadvantages & Applications\n",
        "\n",
        "---\n",
        "\n",
        "##  Advantages of Diffusion Models\n",
        "\n",
        "### 1. High-Quality Sample Generation\n",
        "- Achieve **state-of-the-art performance** in image and audio synthesis.\n",
        "- Often surpass GANs in **visual fidelity and realism**.\n",
        "- **Examples**: `DALL·E 2`, `Imagen`, `Stable Diffusion`.\n",
        "\n",
        "\n",
        "\n",
        "### 2. Training Stability\n",
        "- Based on a **well-defined and simple objective** (e.g., Mean Squared Error).\n",
        "- Avoids **adversarial instability** seen in GANs.\n",
        "- Results in **more predictable and stable training**.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Diverse Sample Generation\n",
        "- Less prone to **mode collapse** (a problem in GANs where outputs lack variety).\n",
        "- Generates **rich and varied samples** even from similar noise inputs.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Flexible Conditioning\n",
        "- Can be easily **conditioned** on:\n",
        "  - Text prompts\n",
        "  - Class labels\n",
        "  - Images\n",
        "  - Audio\n",
        "- Enables powerful tasks like **text-to-image generation** and **guided image editing**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Disadvantages of Diffusion Models\n",
        "\n",
        "### 1. Computational Cost\n",
        "- **Slow inference** due to hundreds/thousands of denoising steps.\n",
        "- Much **slower than VAEs or GANs** which use one-shot generation.\n",
        "\n",
        "\n",
        "\n",
        "### 2. High Memory Usage\n",
        "- Architectures like large U-Nets or Transformers require:\n",
        "  - **High GPU memory**\n",
        "  - **Longer training time**\n",
        "- Can be challenging for deployment on **resource-constrained systems**.\n",
        "\n",
        "\n",
        "\n",
        "##  Applications of Diffusion Models\n",
        "\n",
        "### 1. High-Fidelity Image Generation\n",
        "- Generate photorealistic and artistic images from random noise.\n",
        "- Used in systems like:\n",
        "  - `DALL·E 2`\n",
        "  - `Midjourney`\n",
        "  - `Stable Diffusion`\n",
        "\n",
        "\n",
        "\n",
        "### 2. Text-to-Image Synthesis\n",
        "- Convert **natural language prompts** into detailed images.\n",
        "- Example prompt: _“A cat wearing sunglasses in space”_\n",
        "\n",
        "\n",
        "\n",
        "### 3. Image Editing & Inpainting\n",
        "- **Modify or restore images** by:\n",
        "  - Filling missing parts\n",
        "  - Replacing specific regions\n",
        "- Useful in **photo restoration**, **inpainting**, and **creative editing**.\n",
        "\n",
        "\n",
        "\n",
        "### 4. Audio Generation\n",
        "- Generate **music**, **speech**, and **sound effects**.\n",
        "- Common models:\n",
        "  - `DiffWave`\n",
        "  - `AudioLDM`\n",
        "\n",
        "\n",
        "\n",
        "### 5. Video Generation *(Emerging)*\n",
        "- Generate **short videos** with consistent motion and style.\n",
        "- Applications include:\n",
        "  - Prompt-to-video generation\n",
        "  - Frame interpolation\n",
        "  - Motion-guided synthesis\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ETjf56jOmYOD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCEvaP6abAN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}