{"cells":[{"cell_type":"markdown","source":["# Evaluation of RAG Systems"],"metadata":{"id":"M68JwDo1AerJ"}},{"cell_type":"markdown","source":["Evaluating a Retrieval-Augmented Generation (RAG) system is **critical for making it production-grade**. Unlike traditional language models that generate responses based solely on internal knowledge, RAG systems rely on **retrieved external documents**. This makes evaluation really important.\n","\n","#### **Importance of RAG Evaluation for Production-Readiness**\n","\n","* **Pinpoints Failures Precisely:** It helps identify whether issues are due to **poor retrieval** (irrelevant documents) or **generation errors** (hallucinations or incoherent answers).\n","* **Improves End-to-End Quality:** By using evaluation metrics like **faithfulness, relevance, and context recall**, teams can systematically improve both components of the system.\n","* **Boosts Reliability and Trust:** In production environments, factual correctness is crucial. RAG evaluation ensures answers are **grounded in retrieved data**, increasing user trust.\n","* **Enables Iterative Optimization:** Consistent evaluation provides feedback loops for optimizing **retrievers, rerankers, chunking strategies, and prompts**, making the system scalable and robust.\n","* **Supports Debugging & Monitoring:** In real-time applications, evaluation helps detect when the system fails silently (e.g., generating plausible but incorrect answers).\n","\n","\n","\n","**While evaluation is essential, it's not flawless:**\n","\n","* **Automated metrics** like BLEU or faithfulness may **miss nuanced errors** or fail to capture partial truths.\n","* **Subjectivity in responses** means human evaluation is often needed—but it’s **costly and inconsistent**.\n","* **Metric limitations:** High fluency doesn’t always mean factual accuracy. A response may look good but be based on irrelevant or incorrect context.\n","* **Hard to fully automate:** Especially in domain-specific settings, context relevance or truthfulness may be hard for generic metrics to judge.\n","\n","\n","So, **it’s best used as a guiding tool, not an absolute measure**. Combining automated + human-in-the-loop evaluation gives the most reliable results."],"metadata":{"id":"QFtXtOxIAhTR"}},{"cell_type":"markdown","source":["## 1. Retrieval Metrics"],"metadata":{"id":"Vx2R4D6BBGIw"}},{"cell_type":"markdown","source":["Retrieval metrics evaluate how effectively the retriever fetches relevant documents for a given user query.\n","\n","Good retrieval ensures the LLM has the right information to generate accurate answers. Poor retrieval leads to hallucination or irrelevant outputs—even if the generator is strong. This needs our human feedback, it needs labeled relevant documents for accurate scoring."],"metadata":{"id":"jjwES8MhBJYX"}},{"cell_type":"code","source":["# This is Dummy Data that we will use later\n","retrieved_docs = [\n","    \"Albert Einstein developed the theory of relativity.\",\n","    \"He was a German physicist known for E=mc².\",\n","    \"Einstein was born in a city in Germany.\",\n","    \"He entered the world in the late 19th century.\",\n","    \"Quantum mechanics and relativity shaped modern physics.\",\n","    \"He worked at the Swiss Patent Office.\"\n","]\n","\n","ground_truth = [\"Einstein was born in Ulm, Germany in 1879.\"]"],"metadata":{"id":"dw3IcvZ7DdpO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Suppose the variable `retrieved_docs` contains the top **k = 5** documents **retrieved by the system for a given query.** The `relevant_doc` represents the **ground-truth document** (or chunk) that actually contains the correct answer, which has been **manually identified by a human evaluator**."],"metadata":{"id":"D4qicSsgDels"}},{"cell_type":"markdown","source":["\n","\n","### **A. Recall\\@K**\n","\n","Recall\\@K evaluates whether **at least one relevant document** appears in the **top K** documents retrieved for a query.\n","\n","#### **Formula:**\n","\n","$$\n","\\text{Recall@K} = \\frac{\\text{Number of queries with at least one relevant doc in top K}}{\\text{Total number of queries}}\n","$$\n","\n","* **High Recall\\@K** = Retriever is surfacing relevant context frequently.\n","* **Low Recall\\@K** = Retriever is failing to include useful documents, leading to LLM hallucinations.\n","----\n","\n","* It does need **labeled relevant documents** (ground truth).\n","* Doesn’t consider **rank** or **multiple relevant docs**—just if **any** one is found."],"metadata":{"id":"jBs6zB1CBy_l"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util\n","\n","# Load pre-trained sentence transformer model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","def recall_at_k_semantic(retrieved_docs, ground_truth, k=5, similarity_threshold=0.7):\n","    \"\"\"\n","    Compute Recall@K for retrieval with semantic similarity.\n","\n","    Args:\n","      retrieved_docs (List[str]): Retrieved docs for a single query.\n","      ground_truth (List[str]): Ground-truth relevant docs for the query.\n","      k (int): Top K docs to consider.\n","      similarity_threshold (float): Minimum cosine similarity to count as relevant.\n","\n","    Returns:\n","      int: 1 if at least one doc in top K passes similarity threshold, else 0.\n","    \"\"\"\n","    # Encode ground truth and top-k retrieved docs\n","    ground_truth_embeddings = model.encode(ground_truth, convert_to_tensor=True)\n","    retrieved_embeddings = model.encode(retrieved_docs[:k], convert_to_tensor=True)\n","\n","    # Compute cosine similarity matrix [top_k x ground_truth]\n","    cos_scores = util.cos_sim(retrieved_embeddings, ground_truth_embeddings)\n","\n","    # Check if any retrieved doc has similarity >= threshold with any ground truth doc\n","    max_similarities, _ = cos_scores.max(dim=1)  # max similarity per retrieved doc\n","\n","    if (max_similarities >= similarity_threshold).any():\n","        return 1\n","    else:\n","        return 0\n","\n","\n","# Compute Recall@5\n","recall_score = recall_at_k_semantic(retrieved_docs, ground_truth, k=5, similarity_threshold=0.7)\n","print(\"Recall@5 (semantic):\", recall_score)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_D9iMjwEBaa","executionInfo":{"status":"ok","timestamp":1751722580762,"user_tz":-345,"elapsed":2820,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"e344aa60-1d55-4d40-e2a5-8c5efc54efc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Recall@5 (semantic): 1\n"]}]},{"cell_type":"markdown","source":["Semantic embeddings capture the overall meaning, so phrases like **\"Einstein was born in a city in Germany\" strongly overlap with \"Einstein was born in Ulm, Germany\" by referencing the same event and location generally**. Even without exact matches like \"Ulm,\" the model detects key concepts such as \"born,\" \"Einstein,\" and \"Germany,\" resulting in a **cosine similarity above the threshold and a positive Recall\\@5.**\n"],"metadata":{"id":"FhxM4lkNLHL1"}},{"cell_type":"markdown","source":["\n","### **B. Context Recall**\n","\n","Context Recall measures how much of the **ground-truth answer** is present in the **retrieved context** passed to the LLM.\n","\n","$$\n","\\text{Context Recall} = \\frac{\\text{Number of overlapping tokens between answer and context}}{\\text{Total tokens in the ground-truth answer}}\n","$$\n","\n","\n","* **High Context Recall** = The context supports answering the query correctly.\n","* **Low Context Recall** = The LLM is expected to guess or hallucinate due to missing info.\n","\n","---\n","* Token overlap may not always reflect **semantic similarity**.\n","* Doesn’t evaluate **usefulness** or **clarity** of retrieved content—just literal overlap.\n","\n"],"metadata":{"id":"D9cGfR6jEB7N"}},{"cell_type":"code","source":["def context_recall_semantic(answer: str, context: str, similarity_threshold=0.7) -> float:\n","    \"\"\"\n","    Compute semantic Context Recall using embeddings.\n","    Measures how many segments of the ground-truth answer are semantically found in the context.\n","\n","    Args:\n","        answer (str): Ground-truth answer string.\n","        context (str): Retrieved context string.\n","        similarity_threshold (float): Minimum cosine similarity to consider a token matched.\n","\n","    Returns:\n","        float: Semantic context recall (0 to 1).\n","    \"\"\"\n","    # Split into tokens or short phrases (approximate)\n","    answer_tokens = answer.strip().split()\n","    context_tokens = context.strip().split()\n","\n","    if not answer_tokens:\n","        return 0.0\n","\n","    # Encode each token or short phrase as a vector\n","    answer_embeds = model.encode(answer_tokens, convert_to_tensor=True)\n","    context_embeds = model.encode(context_tokens, convert_to_tensor=True)\n","\n","    # Compute cosine similarity matrix [answer_token x context_token]\n","    similarity_matrix = util.cos_sim(answer_embeds, context_embeds)\n","\n","    # For each answer token, find the max similarity to any context token\n","    max_similarities, _ = similarity_matrix.max(dim=1)\n","\n","    # Count how many tokens are semantically matched (above threshold)\n","    matched_count = (max_similarities >= similarity_threshold).sum().item()\n","\n","    return matched_count / len(answer_tokens)\n","\n","# Compute context string and answer\n","retrieved_context = \" \".join(retrieved_docs)\n","ground_truth_answer = ground_truth[0]\n","\n","# Compute semantic Context Recall\n","score = context_recall_semantic(ground_truth_answer, retrieved_context)\n","print(\"Semantic Context Recall:\", round(score, 2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtV6aYRzEWgG","executionInfo":{"status":"ok","timestamp":1751722581298,"user_tz":-345,"elapsed":515,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"1f7e4359-9ee3-40f5-da33-5ff3e6d766db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic Context Recall: 0.75\n"]}]},{"cell_type":"markdown","source":["The answer \"Einstein was born in Ulm, Germany in 1879\" shares tokens like \"Einstein,\" \"born,\" and \"Germany\" with vague retrieved chunks such as \"Einstein was born in a city in Germany\" and \"He entered the world in the late 19th century.\" However, specific tokens like \"Ulm\" and \"1879\" are missing, resulting in about 57% token overlap—indicating the retrieved context is somewhat relevant but lacks precise details.\n"],"metadata":{"id":"6buiTa3VLP0v"}},{"cell_type":"markdown","source":["### **C. Precision\\@K**\n","\n","Precision\\@K measures how many of the **top K retrieved documents** are actually relevant to the query.\n","\n","$$\n","\\text{Precision@K} = \\frac{\\text{Number of relevant documents in top K}}{K}\n","$$\n","\n","* **High Precision\\@K** = Most retrieved documents are relevant, reducing noise.\n","* **Low Precision\\@K** = Many irrelevant documents appear, potentially confusing the LLM.\n","\n","\n","*Precision\\@K depends on reliable labeling of relevant documents and helps assess retrieval quality.*"],"metadata":{"id":"H5lVaeeBL3kk"}},{"cell_type":"code","source":["def precision_at_k_semantic(retrieved_docs, ground_truth, k=5, similarity_threshold=0.7):\n","    \"\"\"\n","    Compute Precision@K for retrieval with semantic similarity.\n","\n","    Args:\n","      retrieved_docs (List[str]): Retrieved docs for a single query.\n","      ground_truth (List[str]): Ground-truth relevant docs for the query.\n","      k (int): Top K docs to consider.\n","      similarity_threshold (float): Minimum cosine similarity to count as relevant.\n","\n","    Returns:\n","      float: Precision@K = (# relevant docs in top K) / K\n","    \"\"\"\n","    # Encode ground truth and top-k retrieved docs\n","    ground_truth_embeddings = model.encode(ground_truth, convert_to_tensor=True)\n","    retrieved_embeddings = model.encode(retrieved_docs[:k], convert_to_tensor=True)\n","\n","    # Compute cosine similarity matrix [top_k x ground_truth]\n","    cos_scores = util.cos_sim(retrieved_embeddings, ground_truth_embeddings)\n","\n","    # For each retrieved doc, find max similarity to any ground truth doc\n","    max_similarities, _ = cos_scores.max(dim=1)\n","\n","    # Count how many retrieved docs pass the similarity threshold\n","    relevant_count = (max_similarities >= similarity_threshold).sum().item()\n","\n","    precision = relevant_count / k\n","    return precision\n","\n","# Calculate semantic Precision@5\n","prec_at_5 = precision_at_k_semantic(retrieved_docs, ground_truth, k=5, similarity_threshold=0.7)\n","print(\"Precision@5 (semantic):\", prec_at_5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8Py1FdZL37R","executionInfo":{"status":"ok","timestamp":1751722581469,"user_tz":-345,"elapsed":168,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"9169a045-b65b-4457-9693-066711d2f017"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision@5 (semantic): 0.2\n"]}]},{"cell_type":"markdown","source":["**Precision\\@5 = 0.2** means that only 1 out of 5 retrieved chunks was semantically close to the ground-truth answer. Most retrieved content was loosely related or irrelevant, making it harder for the LLM to generate accurate responses. This indicates the retriever needs improvement—such as better chunking, embeddings, or reranking.\n"],"metadata":{"id":"lsFAFNT4M7bb"}},{"cell_type":"markdown","source":["### **D. Context Precision**\n","\n","Context Precision measures how much of the **retrieved context tokens** are actually present in the **ground-truth answer**.\n","\n","$$\n","\\text{Context Precision} = \\frac{\\text{Number of overlapping tokens between context and answer}}{\\text{Total tokens in the retrieved context}}\n","$$\n","\n","* **High Context Precision** = Most tokens in the retrieved context are relevant to the answer, meaning less noise.\n","* **Low Context Precision** = Retrieved context contains many irrelevant tokens, potentially distracting the LLM.\n","\n","*Like Context Recall, Context Precision measures literal overlap and does not capture semantic relevance or usefulness.*\n"],"metadata":{"id":"KfJZvurVMVAB"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util\n","\n","# Load model\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","def context_precision_semantic(context: str, answer: str, similarity_threshold=0.7) -> float:\n","    \"\"\"\n","    Compute semantic Context Precision: what fraction of tokens in the retrieved context\n","    are semantically relevant to the ground-truth answer.\n","\n","    Args:\n","        context (str): Retrieved context string.\n","        answer (str): Ground-truth answer string.\n","        similarity_threshold (float): Cosine similarity threshold for a semantic match.\n","\n","    Returns:\n","        float: Semantic context precision score (0 to 1).\n","    \"\"\"\n","    context_tokens = context.strip().split()\n","    answer_tokens = answer.strip().split()\n","\n","    if not context_tokens:\n","        return 0.0\n","\n","    # Encode context and answer tokens\n","    context_embeds = model.encode(context_tokens, convert_to_tensor=True)\n","    answer_embeds = model.encode(answer_tokens, convert_to_tensor=True)\n","\n","    # Compute cosine similarity matrix [context_token x answer_token]\n","    similarity_matrix = util.cos_sim(context_embeds, answer_embeds)\n","\n","    # For each context token, find its max similarity to any answer token\n","    max_similarities, _ = similarity_matrix.max(dim=1)\n","\n","    # Count context tokens semantically matching the answer\n","    matched_count = (max_similarities >= similarity_threshold).sum().item()\n","\n","    return matched_count / len(context_tokens)\n","\n","retrieved_context = \" \".join(retrieved_docs)\n","ground_truth_answer = ground_truth[0]\n","\n","# Compute semantic context precision\n","ctx_precision_score = context_precision_semantic(retrieved_context, ground_truth_answer)\n","print(\"Semantic Context Precision:\", round(ctx_precision_score, 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpztQsf5MAeS","executionInfo":{"status":"ok","timestamp":1751722584517,"user_tz":-345,"elapsed":3036,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"3f97536f-b4ed-44c9-983f-44b69d74f26b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic Context Precision: 0.22\n"]}]},{"cell_type":"markdown","source":["**Context Precision = 0.11** means that only 11% of the tokens in the retrieved context overlap with the ground-truth answer. This indicates the context contains mostly irrelevant or unrelated information, adding noise and making it difficult for the LLM to focus on the correct answer. It suggests low-quality retrieval or excessive irrelevant content in the context.\n"],"metadata":{"id":"KbgE4FD9NBBd"}},{"cell_type":"markdown","source":["## 2. Generation Metrics"],"metadata":{"id":"speS7DuYL3iH"}},{"cell_type":"markdown","source":["Metrics to track the quality, correctness, and fluency of generated answers from the LLM."],"metadata":{"id":"80AzAc5YwgDP"}},{"cell_type":"code","source":["# Example\n","response = \"The first Super Bowl was held on January 15, 1967.\"\n","contexts = [\n","    \"The First AFL–NFL World Championship Game, later known as Super Bowl I, was held on January 15, 1967, in Los Angeles.\",\n","    \"It was a historic match between the Green Bay Packers and the Kansas City Chiefs.\"\n","]"],"metadata":{"id":"q8i02xuOzF-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **A. BLEU & ROUGE**\n","\n","These are **overlap-based metrics** comparing the generated answer to one or more reference answers:\n","\n","* **BLEU:** Measures n-gram precision, mostly used in machine translation.\n","* **ROUGE:** Measures n-gram recall, common in summarization.\n","\n","\n","$$\n","\\text{BLEU} = BP \\times \\exp\\left(\\sum_{n=1}^N w_n \\log p_n\\right)\n","$$\n","\n","Where:\n","\n","* $p_n$ = precision of n-grams\n","* $w_n$ = weights (usually uniform)\n","* $BP$ = brevity penalty to penalize short outputs\n","\n","\n","$$\n","\\text{ROUGE-N} = \\frac{\\sum_{\\text{gram}_n \\in \\text{Ref}} \\text{Count}_\\text{match}(\\text{gram}_n)}{\\sum_{\\text{gram}_n \\in \\text{Ref}} \\text{Count}(\\text{gram}_n)}\n","$$\n","\n","Where:\n","\n","* $\\text{Count}_\\text{match}$ = count of overlapping n-grams between generated and reference\n","* $\\text{Count}$ = total n-grams in reference\n","\n","*Good for automated evaluation but may miss semantic correctness.*\n","\n","---\n","\n","BLEU n-gram precision between generated text and references whereas ROUGE n-gram recall (often ROUGE-N or ROUGE-L).\n","\n","In short:\n","\n","- BLEU asks: Of what I generated, how much matches the reference?\n","\n","- ROUGE asks: Of what the reference says, how much did I generate?"],"metadata":{"id":"WAYtGnHPL3Xo"}},{"cell_type":"markdown","source":["### **B. Faithfulness**\n","\n","Faithfulness measures how **factually accurate and consistent** the generated answer is with respect to the provided context or source documents.\n","\n","$$\n","\\text{Faithfulness Score} = \\frac{\\left| \\text{Number of claims in the generated answer that can be inferred from the given context} \\right|}{\\left| \\text{Total number of claims in the generated answer} \\right|}\n","$$\n","\n","\n","* **High faithfulness** = Generated content is grounded and truthful.\n","* **Low faithfulness** = Model hallucinates or introduces incorrect information.\n","\n","*Essential to ensure trustworthy, reliable outputs in RAG systems.*"],"metadata":{"id":"X7MhQffYL3fO"}},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer, util\n","import numpy as np\n","import torch\n","\n","# Load a pre-trained model (small and fast for demo)\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","def calculate_faithfulness(response, contexts, threshold=0.7):\n","    \"\"\"\n","    Calculates faithfulness based on cosine similarity between response and context.\n","    \"\"\"\n","    # Encode response and all context chunks\n","    response_embedding = model.encode(response, convert_to_tensor=True)\n","    context_embeddings = model.encode(contexts, convert_to_tensor=True)\n","\n","    # Compute cosine similarities\n","    cosine_scores = util.cos_sim(response_embedding, context_embeddings)[0]\n","\n","    # Get maximum similarity score across all contexts\n","    max_score = float(torch.max(cosine_scores))\n","\n","    # Return result with interpretation\n","    return {\n","        \"faithfulness_score\": round(max_score, 3),\n","        \"is_faithful\": max_score > threshold\n","    }\n","\n","\n","result = calculate_faithfulness(response, contexts)\n","print(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KP5FW4zry8oU","executionInfo":{"status":"ok","timestamp":1751722681539,"user_tz":-345,"elapsed":2592,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"d3308aec-6abe-4453-a1da-4ffb64c5a083"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'faithfulness_score': 0.849, 'is_faithful': True}\n"]}]},{"cell_type":"markdown","source":["### **C. Perplexity**\n","\n","Perplexity measures how **predictable or fluent** the generated text is according to the language model.\n","\n","$$\n","\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^N \\log p(w_i)\\right)\n","$$\n","\n","Where:\n","\n","* $N$ = number of tokens\n","* $p(w_i)$ = probability assigned by the model to token $w_i$\n","\n","* **Lower perplexity** = More fluent, coherent text.\n","* **Higher perplexity** = Text is unexpected or unnatural.\n","\n","*Focuses on language quality rather than factual correctness.*"],"metadata":{"id":"NyhTAi7kL3VH"}},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import torch\n","import math\n","\n","def calculate_perplexity(sentence):\n","    model_name = 'gpt2'\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","    model.eval()\n","\n","    # Encode input\n","    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, labels=input_ids)\n","        loss = outputs.loss\n","        perplexity = torch.exp(loss)\n","\n","    return perplexity.item()\n"],"metadata":{"id":"oP36s1Z0zO_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example sentence\n","ppl = calculate_perplexity(response)\n","print(f\"Perplexity: {ppl:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSkwp1IQz7ZJ","executionInfo":{"status":"ok","timestamp":1751722738683,"user_tz":-345,"elapsed":1992,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"b08fb858-c2d4-4f31-cb74-b13f17b7f1e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Perplexity: 18.87\n"]}]},{"cell_type":"markdown","source":["\n","In our evaluation, we used human-written responses as the ground truth. However, an **alternative approach** involves using a **larger language model—guided by prompt engineering and chain-of-thought reasoning—to assess the output of a smaller model**. This method allows the larger model to act as a judge, helping to calculate key evaluation metrics.\n","\n","There are also several purpose-built libraries designed to support this kind of evaluation. Tools like **Ragas**, **TruLens**, **G-Eval**, and **LMEval** provide frameworks to assess critical aspects of a RAG system’s performance, including **faithfulness**, **relevance**, **fluency**, and **overall answer quality**."],"metadata":{"id":"rEtq7wD30lcs"}},{"cell_type":"markdown","source":["# Limitations of RAG Evaluation"],"metadata":{"id":"S90KiM7NIUm_"}},{"cell_type":"markdown","source":["While RAG evaluation is essential for production readiness, it faces several challenges:\n","\n","1. **Ground Truth Scarcity** – Many queries have multiple valid answers or no definitive “correct” document, making recall/precision evaluation tricky.\n","2. **Dependence on Corpus Quality** – Outdated, biased, or incomplete knowledge bases can mislead metrics, even if the retriever and generator are strong.\n","3. **Metric Blind Spots** – Overlap-based metrics (BLEU, ROUGE) and even semantic similarity can miss nuanced factual errors or partial truths.\n","4. **Attribution Ambiguity** – It’s often unclear if a fact came from retrieved context, the model’s memory, or hallucination, complicating error diagnosis.\n","5. **Subjectivity & Cost of Human Evaluation** – Human judgment remains the gold standard for faithfulness and relevance, but it is slow, costly, and inconsistent.\n","6. **Context Sensitivity** – Small changes in top-k, chunking, or prompt design can significantly shift results, reducing reproducibility.\n","7. **Limited Automation for Truthfulness** – Current automated tools struggle with subtle fact-checking, especially in domain-specific contexts.\n","\n","**Bottom line:**\n","RAG evaluation should be used as a *guiding framework*, not an absolute scorecard. Combining automated metrics with targeted human review yields the most reliable insights.\n","\n","\n"],"metadata":{"id":"svD2ew0HIatp"}},{"cell_type":"markdown","source":["## References"],"metadata":{"id":"KraDZ-4D-G-f"}},{"cell_type":"markdown","source":["![Different RAG Configuration and accuracy](https://i.postimg.cc/d1PgJ3F4/RAG-settings-accuracy.png)\n","\n"],"metadata":{"id":"Tq1gBqtE9zB4"}},{"cell_type":"markdown","source":["The graph illustrates how the performance of the RAG system improved through various tuning techniques, courtesy of Hugging Face."],"metadata":{"id":"ik1oMzP30KLx"}},{"cell_type":"markdown","source":["[Pinnecone RAG Evaluation](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/)\n","\n","[Huggingface RAG Evaluation](https://huggingface.co/learn/cookbook/en/rag_evaluation)\n","\n","[RAGAS Evaluation Mertics](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/context_precision/)"],"metadata":{"id":"4861veQ00IA9"}}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}