{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPmXfscOVJqx0RxEWoABZnT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Retrieval-Augmented Generation (RAG)"],"metadata":{"id":"_-aWFFiRvsJs"}},{"cell_type":"markdown","source":["Retrieval-Augmented Generation (RAG) is a powerful framework that **combines information retrieval with text generation** , significantly improving the performance of language models, especially in tasks that require up-to-date or factual information.\n","\n","Retrieval-Augmented Generation (RAG) is a hybrid approach that:\n","\n","* **Retrieves** relevant documents or passages from a knowledge base.\n","\n","* **Augments** the input query with this retrieved content.\n","\n","* **Generates** a final response using a generative model (e.g., BERT2GPT, T5, LLaMA).\n","\n","It's like giving a language model access to a search engine + memory.\n","\n"],"metadata":{"id":"MQXDWkdGvtot"}},{"cell_type":"markdown","source":["## Benefits of Using RAG:\n","\n","| Challenge                 | How RAG Helps                             |\n","| ------------------------- | ----------------------------------------- |\n","| Hallucinated facts        | Grounds output in retrieved documents     |\n","| Outdated knowledge        | Retrieval updates automatically           |\n","| Domain-specific knowledge | Uses custom, private knowledge bases      |\n","| Lack of explainability    | Retrieved documents can be shown to users |"],"metadata":{"id":"aSWvhRXr6vlG"}},{"cell_type":"markdown","source":["## **Quick Scenario**\n","\n","\"Large language models often produce hallucinated or outdated responses when answering domain-specific or fact-based questions, especially when the knowledge required is not part of their training data.\"\n","\n","**Model forgets content in the middle of its contextual window.**\n","\n","\n","This becomes a serious issue in:\n","\n","* **Legal or medical domains** (accuracy is critical)\n","* **Customer support** (product info changes frequently)\n","* **Education or research** (answers must be grounded in facts)\n","* **Enterprise applications** (data is private and not in public models)"],"metadata":{"id":"TrNxRZMrwRzF"}},{"cell_type":"markdown","source":["### How RAG Works:\n","\n","**Retrieval-Augmented Generation** enhances factuality and relevance by combining **external document retrieval** with **natural language generation**.\n","\n","1. **Input Query**:\n","   A user asks a question:\n","   *What are the main benefits of using RAG in legal tech?*\n","\n","2. **Retrieval**:\n","\n","   * The query is embedded and matched with a **vector database** (like FAISS, Pinecone).\n","   * The system retrieves the top-k relevant documents from a **legal corpus**.\n","\n","3. **Augmentation**:\n","\n","   * The retrieved documents are **concatenated with the user query** to form an augmented input.\n","\n","4. **Generation**:\n","\n","   * A language model (e.g., T5, BART, or LLaMA) uses this augmented input to **generate a context-aware answer** grounded in the documents.\n","\n","5. **Output**:\n","   The model generates:\n","   *RAG improves legal AI tools by providing real-time answers based on up-to-date laws and case precedents, reducing hallucinations common in static LLMs.*\n"],"metadata":{"id":"xSEy7YKK7Qfa"}},{"cell_type":"markdown","source":["## RAG vs Finetuining\n","\n","\n","### Use **RAG** when:\n","\n","If we need accurate, **up-to-date answers grounded in external documents without retraining the model**. It's ideal for dynamic domains like legal, medical, or enterprise support where knowledge changes frequently and explainability is important. For example: A legal assistant chatbot answering queries about company policies.\n","\n","> \"Let the model look things up instead of remembering everything.\"\n","\n","\n","### Use **Fine-Tuning** when:\n","\n","**Fine-tuning** is the process of training a pre-trained model further on a specific dataset to adapt it to a particular task, domain, or style.\n","\n","If we want the **model to learn specific behavior, language style, or task format from data**. It's best for stable domains where low-latency, task-specific performance is needed and you have the resources to train and deploy a custom model. For example: A customer support chatbot trained specifically for a telecom company.\n","\n","> \"Teach the model everything it needs to know beforehand.\"\n"],"metadata":{"id":"93lfDSdmw_mh"}},{"cell_type":"markdown","source":["## RAG vs LLM"],"metadata":{"id":"Ad3lR25EbgE1"}},{"cell_type":"markdown","source":["**RAG (Retrieval-Augmented Generation)** is a technique that combines a **Large Language Model (LLM)** with **Information Retrieval**. An LLM is a pretrained model trained on vast amounts of text data to understand and generate human-like language. In RAG, the LLM is enhanced by retrieving relevant external documents at query time, allowing it to generate more accurate, up-to-date, and context-aware responses.\n","\n","> RAG = LLM + Information Retrieval\n","\n"],"metadata":{"id":"BZKvf-Svb_po"}},{"cell_type":"markdown","source":["|**Aspect**              |**LLM (Large Language Model)**           |**RAG (Retrieval-Augmented Generation)**                                |\n","| ----------------------- | ---------------------------------------- | ----------------------------------------------------------------------- |\n","| **Architecture**        | Just the language model                  | Retriever + Language Model (Generator)                                  |\n","| **Data Source**         | Pretrained on static datasets            | Dynamically retrieves from external/custom knowledge sources            |\n","| **Factual Accuracy**    | Can hallucinate or provide outdated info | More accurate and grounded (uses real documents)                        |\n","| **Updatability**        | Requires retraining or fine-tuning       | Easily updatable by modifying the document store                        |\n","| **Knowledge Freshness** | Frozen at time of training               | Can use the most recent or real-time data                               |\n","| **Memory of Knowledge** | Static (training-time knowledge only)    | Dynamic (uses updated retrieved content)                                |\n","| **Complexity**          | Simpler (single model)                   | More complex (retriever + generator, sometimes reranker)                |\n","| **Latency**             | Faster (just generation)                 | Slightly slower due to retrieval step                                   |\n","| **Scalability**         | Scales with model size                   | Scales with both model and retrieval system                             |\n","| **Best For**            | General tasks, storytelling, basic Q\\&A  | Domain-specific Q\\&A, legal, medical, internal tools, enterprise search |\n","| **Examples**            | ChatGPT, Claude, Gemini (standalone)     | Chatbots that read your PDFs, enterprise assistants, open-book QA       |\n"],"metadata":{"id":"6KL0-FFmcFw3"}},{"cell_type":"code","source":[],"metadata":{"id":"Sh-pXM9acJnk"},"execution_count":null,"outputs":[]}]}