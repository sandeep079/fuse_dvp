{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNG+UJvQIjl9zdEUEMq9JzY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Generation Configuration and Sampling Strategies**\n","The output from the transformers softmax layer is a probability distribution across the entire dictionary of words the model uses.\n","\n","Generation parameters are settings that control the behavior of text generation in large language models (LLMs). These parameters determine how the model selects tokens from its prediction probabilities to form coherent outputs. By adjusting these configurations, users can influence the **randomness, length, style, creativity, and factual accuracy** of the model’s responses.\n","\n","These parameters are especially important when designing prompts for creative tasks (e.g., poetry, dialogue), fact-based Q&A, or programming outputs where precision varies in importance.\n","\n","Below only four words are shown and this is the table with the list of all dictionaries.\n","\n","<div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/7P7S0zSZ/prompt-engineering-2.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1v5QynU5F69FojwrZY7kTJ06-qGzUVRnK/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: List of all dictionaries\n"," </div>\n","\n","## **Core Generation Parameters**\n","Generation parameters are essential levers that influence how a large language model produces text. These parameters guide the model's decision-making during generation, affecting everything from the tone and length of the output to how repetitive or creative it is. Proper understanding and tuning of these parameters allow users to adapt model behavior to various contexts like summarization, creative writing, programming, or technical explanation.\n","\n","### **1. Top-p (Nucleus Sampling)**\n","\n","Top-p sampling is a dynamic filtering technique that selects the next token from a subset of the vocabulary whose cumulative probability exceeds a certain threshold p.\n","\n","- If top_p = 0.9, only the most probable tokens that together make up 90% of the probability mass are retained for sampling.\n","\n","- This method allows the model to adjust the token pool based on the context: smaller pools for predictable moments, larger pools for ambiguous ones.\n","\n","<div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/x8RGwkGm/prompt-engineering-4.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1lQDMZp4_3J-oCVJUgv3wCFlF-w1mjeK2/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Top p sampling\n"," </div>\n","\n","Compared to top_k, top-p offers more flexibility and tends to produce more natural-sounding language.\n","\n","### **2. Top-k Sampling**\n","Top-k sampling is a static cutoff where the model samples from only the top k most probable tokens at each step.\n","\n","- If k = 50, the model considers only the top 50 tokens for selection, ignoring the rest.\n","\n","- Lower k values restrict randomness, often improving focus, while higher values allow more creativity.\n","\n","<div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/Vv1BZ9t1/prompt-engineering-3.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1xuxZWLU2rZqKseCgqIgOGBkSaX28GHSi/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Top k sampling\n"," </div>\n","\n","Though top-k is more rigid than top-p, it can be useful when you want strict control over vocabulary choices (e.g., in constrained-generation tasks).\n","\n","### **3. Temperature**\n","Temperature controls the randomness of token selection. It modifies the model’s probability distribution over the vocabulary, essentially determining how \"bold\" or \"safe\" its choices are.\n","\n","While Top-k and Top-p operate directly on the output probabilities, temperature affects the softmax function itself. Temperature influences the shape of probability distribution the model calculates for the next token.\n","\n","The softmax function takes in a vector of n real numbers, and then normalizes it into a discrete probability distribution across those n elements whose probabilities will sum to 1.\n","\n","$$  \\sigma (x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n}{e^{x_j}}} $$\n","\n","Temperature is the parameter of the softmax function, which is the last layer in the network. This value affects whether the difference between input values is to be amplified or reduced.\n","\n","$$ \\sigma (x)_i = \\frac{e^{\\frac{x_i}{T}}}{\\sum_{j=1}^{n} e^{\\frac{x_j}{T}}} $$\n","\n","> If 0<T<1, then the $x_i$ input values get pushed further away from 0 and differences are amplified.\n","\n","> If T>1, then the $x_i$ input values get pushed toward 0 and differences are reduced.\n","\n","A lower temperature sharpens the distribution, making the model more confident and likely to pick the highest-probability item. Conversely, a higher temperature smoothens the distribution, allowing less probable options to have a higher chance of being selected. This allows for more creative or diverse outputs.\n","\n","<div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/9XtBRS3f/prompt-engineering-6.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1mTL3cgg7IOchiWWtlUBQdxZQDXs1v6A3/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Effects of temperature\n"," </div>\n","\n"," <div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/nLjkXhZ8/prompt-engineering-5.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1OhGOpbeAawV_5XMZMtZ1wx0OUUCQdOup/view?usp=sharing\"> -->\n","    </figure>\n"," </div>\n","\n"," The x-axis in all four graphs represents output indices ranging from 0 to 9. The y-axis shows raw logits in the top-left graph, while in the other three, it shows softmax probabilities affected by different temperature settings.\n","\n","A temperature of 0 activates greedy decoding, where the model always chooses the token with the highest probability.\n","\n","### **4. Max Tokens**\n","max_tokens limits the total number of tokens the model is allowed to generate. This is not about randomness but about length control.\n","\n","- In chat-based systems or APIs, it helps manage cost, latency, and verbosity.\n","\n","- It ensures the model doesn’t ramble in longer outputs or cut off prematurely in shorter ones.\n","\n","Choosing the right max token length depends on the prompt type: shorter for instructions, longer for narratives or documents.\n","\n","**Example:**\n","\n","Prompt:\n","\"*Summarize the plot of Romeo and Juliet.*\"\n","\n","- max_tokens = 20\n","\n","  ```\n","  \"Romeo and Juliet fall in love despite their families’ feud. They both die in the end.\"\n","  ```\n","\n","- max_tokens = 10\n","```\n","  \"Romeo and Juliet are star-crossed lovers who die.\"\n","```\n","- max_tokens = 5\n","```\n","  \"Tragic love story ending in death.\"\n","```\n","### **5. Frequency Penalty**\n","This parameter discourages the model from repeating the same tokens by penalizing based on how often a token has already been generated.\n","\n","- Higher frequency penalty values reduce repetition.\n","\n","- It is especially helpful in long-form text to avoid word loops like “very very very…”\n","\n","**Example:**\n","\n","Prompt:\n","\"*Describe the forest in detail.*\"\n","\n","- frequency_penalty = 0\n","```\n","  \"The forest was dark and deep. The forest had tall trees. The forest echoed with sounds of animals.\"\n","```\n","  *(Repetitive use of \"the forest\")*\n","\n","- frequency_penalty = 1.5\n","```\n","  \"A dense, shadowy place with towering pines and scattered undergrowth. Sounds of birds and rustling leaves filled the air.\"\n","```\n","  *(Avoids repeating \"forest\" excessively)*\n","\n","### **6. Presence Penalty**\n","Unlike the frequency penalty (which is repetition-based), the presence penalty penalizes tokens that already appeared, regardless of how many times.\n","\n","- It nudges the model to introduce new ideas or vocabulary, fostering novelty.\n","\n","- Useful in brainstorming or ideation contexts where variety is more important than precision.\n","\n","Together, frequency and presence penalties help tune the freshness vs. consistency trade-off in output.\n","\n","**Example:**\n","\n","Prompt:\n","\"*List some animals found in the rainforest.*\"\n","\n","- presence_penalty = 0\n","```\n","  \"Jaguar, monkey, toucan, jaguar, sloth, toucan.\"\n","```\n","  (Repeats previously mentioned animals)\n","\n","- presence_penalty = 1.2\n","```\n","  \"Jaguar, monkey, toucan, sloth, poison dart frog, harpy eagle.\"\n","```\n","  (Avoids repeating previously used animals)\n","\n","#### **7. Stop_sequence/token**\n","The special symbol or sequence of characters used to indicate the end of a sequence or sentence is a stop sequence. If the model produces the stop token even before reaching the max_new_token limit, the generation is terminated.\n"],"metadata":{"id":"SHTxJ8zKZ0LB"}},{"cell_type":"markdown","source":["## **Decoding Strategies**\n","Decoding strategies are algorithms the model uses to convert its probability distribution into coherent text. Each strategy offers a different balance between determinism, diversity, speed, and accuracy.\n","\n","### **1. Greedy Decoding**\n","Greedy decoding selects the most probable token at every step of generation without considering future consequences.\n","- It is fast and reliable for factual or rule-based tasks.\n","- However, it can lead to bland, repetitive, or incomplete answers, as it lacks a global view of sentence quality.\n","\n","<div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/VNLCKYZd/prompt-engineering-7.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1Xrr0WtQO-KjnyOWhEQ_iLgySAoZoqzAO/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Greedy Decoding\n"," </div>\n","\n"," <div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/xd19BdzG/greedy-search.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1pUczX-G0lCP-WGmCbp6nB6oKPjwJK2ua/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Greedy Method\n"," </div>\n","\n","### **2. Random Sampling**\n","Here, the model randomly samples a token from the entire probability distribution, influenced by the temperature.\n","\n","- At higher temperatures, the randomness increases, which can lead to creative but inconsistent outputs.\n","\n","- It works well for fiction, poetry, dialogue writing, or anytime you want variety over accuracy.\n","\n","- Unlike greedy decoding, it does not always pick the top-ranked choice, promoting surprise.\n","\n","<div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/Ssnzpgh4/prompt-engineering-8.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/131UEkaH-dZ9Y17_Iz4OfcKFoqQ2UrtAm/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Random Sampling\n"," </div>\n","\n"," <div align=\"center\">\n"," <figure>\n","     <img src=\"https://i.postimg.cc/mrpT4GYv/beam-search.png\">\n","     <!-- <img src=\"https://drive.google.com/file/d/1w2VapgrvIcRX2uK0uFQd7naxJRZHEotH/view?usp=sharing\"> -->\n","    </figure>\n","    Fig: Random Method\n"," </div>\n","\n"," There is a 20% chance that ‘cake’ will be selected and 2% chance that the word ‘banana’ will be selected. Top-k and Top-p further limit the random sampling to make the output more sensible allowing variability.\n","\n","### **3. Beam Search**\n","Beam Search is a decoding algorithm that maintains multiple candidate sequences (called “beams”) while generating tokens. Instead of choosing the highest-probability token at each step (like greedy decoding), Beam Search explores **several high-probability sequences in parallel** and selects the most promising complete one.\n","\n","- Useful in translation, summarization, and structured prediction, where quality matters more than variety.\n","\n","- It sacrifices diversity for higher output coherence.\n","\n","However, it is computationally more expensive and may lead to generic responses without additional diversity enhancements.\n","\n","[References](https://huggingface.co/blog/how-to-generate)"],"metadata":{"id":"Hf0Vhpq4eugI"}}]}