{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Zre08ve4IUwOaK11ol3ptJ8qSd9hSdEE","timestamp":1752471640742}],"toc_visible":true,"authorship_tag":"ABX9TyNt6C0k+fBGhse0p6kLFmWJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In this section, we will implement a Retrieval-Augmented Generation (RAG) system. To proceed with the demo, a **Google API key** is required.\n","\n","If you haven't set it up in Google Colab yet, follow these video tutorials for step-by-step guidance:\n","\n","* [How to Set Up Google API Key](https://www.youtube.com/watch?v=brCkpzAD0gc)\n","* [How to Set Up Google API Key in Colab](https://www.youtube.com/watch?v=3qYm-S2NDDI)\n","\n","Make sure the API key is properly configured before moving on to the next steps.\n"],"metadata":{"id":"_DfYFJ73c_TH"}},{"cell_type":"code","source":["from google.colab import userdata\n","api_key = userdata.get('GOOGLE_API_KEY')\n","\n","assert api_key is not None and api_key != \"\", \"API key not found in userdata!\""],"metadata":{"id":"yt_oas5-YfiF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optinally, you could download the model locally and use it for inference. It's needed for generating the answer basde on given chunks which you will learn later."],"metadata":{"id":"v5fWotM0OrFO"}},{"cell_type":"markdown","source":["# Importing the Necessary Libraries"],"metadata":{"id":"8rJeRpVEeK7F"}},{"cell_type":"code","source":["# !pip install langchain faiss-cpu sentence-transformers google-generativeai --quiet\n","# Install the above library if not installed"],"metadata":{"id":"CgQoEyAzqLmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import numpy as np\n","import google.generativeai as genai"],"metadata":{"id":"3PIxCiQvYXqP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RAG Implementations"],"metadata":{"id":"Upcttzg9ejXF"}},{"cell_type":"markdown","source":["## 1. Chunking"],"metadata":{"id":"0iSz8jSllie9"}},{"cell_type":"markdown","source":["Here, we are working with certain portion of text from the \"Attention is All You Need\" paper. Typically, we would be using PDFs or other document formats, but in this case, we’ve already extracted the content and are now breaking it into smaller chunks for processing."],"metadata":{"id":"GRkjOmCkk6m1"}},{"cell_type":"code","source":["\n","\n","\n","text = \"\"\"\n","The paper \"Attention Is All You Need\" by Vaswani et al. (2017) introduced the Transformer architecture, which revolutionized natural language processing by entirely removing recurrence and convolutions from the model architecture, relying solely on attention mechanisms. This innovation enabled models to handle long-range dependencies in sequence data more effectively and with greater parallelism than previous architectures like RNNs or LSTMs.\n","\n","The Transformer is based on an encoder-decoder structure. Both the encoder and decoder are composed of a stack of identical layers: the encoder has six layers, and the decoder has six layers as well in the original configuration. Each encoder layer consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder has an additional third sub-layer that performs multi-head attention over the output of the encoder stack.\n","\n","A key innovation is the self-attention mechanism, which computes attention scores between all pairs of tokens in a sequence. This allows the model to dynamically focus on different parts of the input for each output, enabling it to capture relationships and dependencies across any distance. The scaled dot-product attention mechanism is used to calculate attention weights, and multiple attention heads (multi-head attention) allow the model to jointly attend to information from different representation subspaces.\n","\n","To retain the positional information that is lost when removing recurrence, the Transformer uses positional encoding, which injects information about the position of each token in the sequence into the input embeddings using sinusoidal functions. This helps the model understand the order and structure of the sequence.\n","\n","Residual connections and layer normalization are used around each sub-layer to facilitate training and ensure better gradient flow. Dropout is also employed as a regularization technique.\n","\n","The Transformer was evaluated on machine translation tasks, such as English-to-German and English-to-French translation, where it achieved state-of-the-art results while being significantly faster to train due to its highly parallelizable design. The model's effectiveness and efficiency have made it the foundation of many subsequent models like BERT, GPT, T5, and others that have driven progress in NLP and beyond.\n","\n","Overall, the Transformer represents a paradigm shift in sequence modeling, proving that attention mechanisms alone, without the need for recurrence or convolution, are sufficient for powerful and scalable sequence transduction.\n","\"\"\"\n","\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","chunks = text_splitter.split_text(text)\n","print(f\"Chunks: {len(chunks)} \\n\")\n","\n","for i, chunk in enumerate(chunks[:5], start=1):  # Simply Print the chunks\n","    print(f\"Chunk {i}: {chunk}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WyrXGW59aV0Y","executionInfo":{"status":"ok","timestamp":1751365420635,"user_tz":-345,"elapsed":5,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"9c1537fd-9b08-46bf-b722-091f4edfac1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunks: 8 \n","\n","Chunk 1: The paper \"Attention Is All You Need\" by Vaswani et al. (2017) introduced the Transformer architecture, which revolutionized natural language processing by entirely removing recurrence and convolutions from the model architecture, relying solely on attention mechanisms. This innovation enabled models to handle long-range dependencies in sequence data more effectively and with greater parallelism than previous architectures like RNNs or LSTMs.\n","\n","Chunk 2: The Transformer is based on an encoder-decoder structure. Both the encoder and decoder are composed of a stack of identical layers: the encoder has six layers, and the decoder has six layers as well in the original configuration. Each encoder layer consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder has an additional third sub-layer that performs multi-head attention over the output of the encoder stack.\n","\n","Chunk 3: A key innovation is the self-attention mechanism, which computes attention scores between all pairs of tokens in a sequence. This allows the model to dynamically focus on different parts of the input for each output, enabling it to capture relationships and dependencies across any distance. The scaled dot-product attention mechanism is used to calculate attention weights, and multiple attention heads (multi-head attention) allow the model to jointly attend to information from different\n","\n","Chunk 4: to jointly attend to information from different representation subspaces.\n","\n","Chunk 5: To retain the positional information that is lost when removing recurrence, the Transformer uses positional encoding, which injects information about the position of each token in the sequence into the input embeddings using sinusoidal functions. This helps the model understand the order and structure of the sequence.\n","\n"]}]},{"cell_type":"markdown","source":["*Note: Here we are just using recursive character splitter for simplicity and ease of understanding in real world Token Budget Aware Chunking is used.*"],"metadata":{"id":"XExKuq70pk5g"}},{"cell_type":"markdown","source":["## 2. Embeddings\n"],"metadata":{"id":"vqj9dTgatTLA"}},{"cell_type":"markdown","source":["We load a pre-trained SentenceTransformer model called \"all-MiniLM-L6-v2\" to convert text chunks into numerical vector embeddings.\n","\n","Each text segment in the list chunks is encoded into a high-dimensional vector representation, output as a NumPy array for compatibility with the FAISS similarity search library.\n","\n","The code then retrieves the dimensionality of these embeddings, which is essential for initializing the FAISS index that will store and search these vectors efficiently."],"metadata":{"id":"8vC09zTFl2mx"}},{"cell_type":"code","source":["# Load a pre-trained SentenceTransformer model for embedding text.\n","# \"all-MiniLM-L6-v2\" is a lightweight and efficient model that converts text into numerical vector representations.\n","embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","\n","# Use the model to encode (embed) each text chunk into a high-dimensional vector.\n","# `chunks` is a list of text segments, and this function converts each one into a vector (numpy array).\n","# Setting convert_to_numpy=True ensures the output is a NumPy array for easy handling with FAISS.\n","chunk_embeddings = embedder.encode(chunks, convert_to_numpy=True)\n","\n","# Get the dimension (i.e., number of features) of each embedding vector.\n","# This value is needed to initialize the FAISS index, which must know how many features each vector has.\n","embedding_dim = chunk_embeddings.shape[1]"],"metadata":{"id":"HIdxuvBVtbR8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show first 5 values of top 5 embeddings\n","for i, emb in enumerate(chunk_embeddings[:5]):\n","    print(f\"Embedding {i+1} (first 5 dimensions):\")\n","    print(emb[:5])\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nr9WiDlVTfoV","executionInfo":{"status":"ok","timestamp":1751365422047,"user_tz":-345,"elapsed":24,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"4f8a8125-4b8b-415b-e877-17cb493a6086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding 1 (first 5 dimensions):\n","[-0.07092351 -0.07223532  0.03069665  0.01608307  0.00732267]\n","\n","Embedding 2 (first 5 dimensions):\n","[-0.08164732 -0.02074689 -0.02790144 -0.09548715  0.01759948]\n","\n","Embedding 3 (first 5 dimensions):\n","[-0.00218242 -0.0573061  -0.06513862 -0.03470995  0.0239263 ]\n","\n","Embedding 4 (first 5 dimensions):\n","[-0.0093211   0.00057174 -0.00996628  0.04694532 -0.02662582]\n","\n","Embedding 5 (first 5 dimensions):\n","[-0.16550334  0.00069517  0.01096417 -0.03958251 -0.04955875]\n","\n"]}]},{"cell_type":"markdown","source":["## 3. Indexing & Vector Database\n","\n"],"metadata":{"id":"ajhsnxjLN2Mr"}},{"cell_type":"markdown","source":["We index the embeddings and store them in a vector database for efficient retrieval and similarity search."],"metadata":{"id":"v85gr6RSlnTz"}},{"cell_type":"code","source":["# Create a FAISS index to store the vector embeddings for similarity search.\n","# IndexFlatL2 means the index will use L2 (Euclidean) distance to compare vectors.\n","# 'embedding_dim' tells FAISS how many dimensions each embedding vector has (must match the embeddings).\n","index = faiss.IndexFlatL2(embedding_dim)\n","\n","# Add the embedding vectors of the text chunks to the FAISS index.\n","# This allows us to later search for the most similar chunks given a new query vector.\n","index.add(chunk_embeddings)\n"],"metadata":{"id":"ssELAbsjdY2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We’ll use FAISS (Facebook AI Similarity Search) as our vector database for this demo. It’s a fast, in-memory library for similarity search and doesn’t require any setup. Embeddings are stored in RAM by default. While FAISS can be saved to disk or scaled with external storage, for our simple use case, in-memory indexing is sufficient."],"metadata":{"id":"kzNQTjYeakRY"}},{"cell_type":"markdown","source":["## 4. Retrieval"],"metadata":{"id":"oEG8v84ab2iQ"}},{"cell_type":"markdown","source":["This function takes a user query, converts it into an embedding vector using the same model *'all-MiniLM-L6-v2'*, and searches a pre-built FAISS index to find the top `k` most similar text chunks.\n","\n","It returns these relevant chunks along with their similarity scores. The example demonstrates retrieving and printing the closest chunks to a specific question, showing both the similarity scores and a preview of each matched text segment for context."],"metadata":{"id":"1B-7MxK-mIrJ"}},{"cell_type":"code","source":["def retrieve_similar_chunks(query, k=3):\n","    # Convert the query text into an embedding vector using the embedder\n","    query_embedding = embedder.encode([query], convert_to_numpy=True)\n","\n","    # Search the index for the top k most similar chunks to the query embedding\n","    distances, indices = index.search(query_embedding, k)\n","\n","    # Return the list of matching chunks and their similarity distances\n","    return [chunks[i] for i in indices[0]], distances[0]\n","\n","question = \"What is the purpose of positional encoding in the Transformer? Keep it short\"\n","relevant_chunks, distances = retrieve_similar_chunks(question)\n","\n","print(\"Relevant Chunks and Their Similarity Scores:\")\n","for i, (chunk, distance) in enumerate(zip(relevant_chunks, distances), 1):\n","    print(f\"{i}. (Score: {distance:.4f}) {chunk[:200]}...\")  # Print first 200 chars of chunk for brevity\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCs9PxEPZD0J","executionInfo":{"status":"ok","timestamp":1751365422064,"user_tz":-345,"elapsed":11,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"31e550b6-27e1-473e-a2a3-5caa8b043481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Relevant Chunks and Their Similarity Scores:\n","1. (Score: 0.5205) To retain the positional information that is lost when removing recurrence, the Transformer uses positional encoding, which injects information about the position of each token in the sequence into th...\n","2. (Score: 0.7556) The Transformer is based on an encoder-decoder structure. Both the encoder and decoder are composed of a stack of identical layers: the encoder has six layers, and the decoder has six layers as well i...\n","3. (Score: 0.9567) Overall, the Transformer represents a paradigm shift in sequence modeling, proving that attention mechanisms alone, without the need for recurrence or convolution, are sufficient for powerful and scal...\n"]}]},{"cell_type":"markdown","source":["## 5. Generation"],"metadata":{"id":"YXTjIEMWcigt"}},{"cell_type":"markdown","source":["We then configure the GenAI client with an API key and initializes the Gemini generative model.\n","\n","The function `answer_question_with_gemini` retrieves relevant text chunks using FAISS, combines them into a context, and crafts a prompt to instruct the model to answer the user’s question based on that context. It then generates and returns a concise, context-aware answer using the Gemini model.\n"],"metadata":{"id":"HwFfgPPymXR4"}},{"cell_type":"code","source":["genai.configure(api_key=api_key)  # Set up the GenAI client with the provided API key\n","\n","model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")  # Initialize the Gemini generative model\n","\n","# Function to answer a question using FAISS retrieval and the Gemini model\n","def answer_question_with_gemini(question, k=3):\n","    # Retrieve the top k relevant text chunks related to the question\n","    relevant_chunks, _ = retrieve_similar_chunks(question, k)\n","\n","    # Combine the retrieved chunks into a single context string separated by newlines\n","    combined_context = \"\\n\\n\".join(relevant_chunks)\n","\n","    # Create a prompt that instructs the AI to answer based on the provided context\n","    prompt = f\"\"\"\n","    You are a helpful AI assistant. Use the following context to answer the user's question accurately.\n","\n","    ### Context:\n","    {combined_context}\n","\n","    ### Question:\n","    {question}\n","\n","    ### Answer:\n","    \"\"\"\n","    # Generate the answer using the Gemini model with the crafted prompt\n","    response = model.generate_content(prompt)\n","\n","    # Return the generated answer text, stripped of leading/trailing whitespace\n","    return response.text.strip()\n"],"metadata":{"id":"gH338XOwZKVI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"What is the purpose of positional encoding in the Transformer? Keep it short\"\n","answer = answer_question_with_gemini(question)\n","print(f\"\\nQuestion: {question}\\n\\nAnswer:\\n{answer}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"FspXdjDIZMda","executionInfo":{"status":"ok","timestamp":1751365424824,"user_tz":-345,"elapsed":2756,"user":{"displayName":"Sandesh Shrestha","userId":"08864029907362368592"}},"outputId":"da638d15-07d6-49cf-a7cd-172fea31b2a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Question: What is the purpose of positional encoding in the Transformer? Keep it short\n","\n","Answer:\n","Positional encoding injects positional information into the input embeddings, allowing the Transformer to understand word order despite lacking recurrence.\n"]}]}]}