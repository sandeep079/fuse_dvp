{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation"
      ],
      "metadata": {
        "id": "goQSNj6kPCPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Distillation, often abbreviated as KD, is a powerful technique in the field of machine learning and artificial intelligence. It's a method that allows us to compress and transfer the knowledge learned by a large, complex model to a smaller and more efficient one. The core idea behind Knowledge Distillation is to take the extensive expertise of a large \"teacher\" model and distill it into a more compact \"student\" model."
      ],
      "metadata": {
        "id": "ogSt-J4T1k2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Teacher-Student Paradigm**: Knowledge Distillation leverages a teacher-student paradigm. The \"teacher\" model is a large, well-performing model, while the \"student\" model is a smaller, more lightweight model. You can 'distill' the large and complex network into a much smaller network which does as good a job of approximating the original function learned by the deep network.\n",
        "\n",
        "**Goal**: The primary goal of knowledge distillation is to train the student model to not only predict the target labels but also to mimic the knowledge contained within the teacher model."
      ],
      "metadata": {
        "id": "CSoDP8gPc6nU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formal Definition:\n",
        "\n",
        "Neural networks use a softmax function to generate the logits $z_i$ to class probabilities $p(z_i, T) = \\frac{exp(z_i/T)}{∑_jexp(z_j/T)}$. Here, i, j = 0, 1, 2, ..., C-1, where C is the number of classes. T is the temperature, which is normally set to 1."
      ],
      "metadata": {
        "id": "TQGfZISwfZJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Alternately, the **goal** of knowledge distillation is to align the class probability distributions from teacher and student networks."
      ],
      "metadata": {
        "id": "0kmZVZgJc34A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge"
      ],
      "metadata": {
        "id": "q3YEz39kkggB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of knowledge distillation, a vanilla knowledge distillation primarily leverages the logits of a large deep model as the source of teacher knowledge. The activations, neurons, or features of intermediate layers can be used as the knowledge to guide the learning of the student model. The relationships between different activations, neurons or pairs of samples contain rich information learned by the teacher model. Furthermore, the parameters of the teacher model (or the connections between layers) also contain another knowledge. We will discuss different forms of knowledge such as: response-based knowledge, feature-based knowledge,and relation-based knowledge."
      ],
      "metadata": {
        "id": "Ho4OexLzOpo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Response-based knowledge"
      ],
      "metadata": {
        "id": "Y8DysDbMkhyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response-based knowledge is centered around the final output layer of the teacher model. It aims to train the student model to replicate the teacher's predictions. A loss function, known as the distillation loss, quantifies the difference between the student's logits and the teacher's logits. As the distillation loss is minimized during training, the student model learns to make similar predictions as the teacher model."
      ],
      "metadata": {
        "id": "IHmEtjpfksfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![response_based_knowledge](https://drive.google.com/uc?id=1vybYI95A6-BJ3ZA2Jz18_NohlmUwO427) -->\n",
        "![response_based_knowledge](https://i.postimg.cc/J4wSWc7X/image.png)"
      ],
      "metadata": {
        "id": "W2MrY4RGSuRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Feature-based knowledge"
      ],
      "metadata": {
        "id": "YXcjvFi0kh8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature-based knowledge involves the intermediate layers of the teacher model, where valuable data features are captured. This type of knowledge is relevant, particularly in deep neural networks. Feature-based knowledge distillation involves training the student model to learn the same feature activations as the teacher model. The distillation loss function is used to minimize the disparity between the feature activations of the teacher and student models."
      ],
      "metadata": {
        "id": "rPlXLXzWks5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![feature_based_knowledge](https://drive.google.com/uc?id=1kWU2DXTTchQaPj_aPvqW1ZXAJDUdiB2b) -->\n",
        "![feature_based_knowledge](https://i.postimg.cc/6qxcFcV5/image.png)"
      ],
      "metadata": {
        "id": "KjQKoINRSutz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Relation-based knowledge"
      ],
      "metadata": {
        "id": "0Kzor2hdkh--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to knowledge in output and intermediate layers, neural networks can contain knowledge about relationships between feature maps or data representations. This knowledge, referred to as relation-based knowledge, captures correlations, graphs, similarity matrices, feature embeddings, or probabilistic distributions based on feature representations. Relation-based knowledge is used to train a student model by modeling the relationships between feature maps or data representations. The distillation process incorporates these relationships to guide the student model's learning."
      ],
      "metadata": {
        "id": "SgCPFjr0ktYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![relation_based_knowledge](https://drive.google.com/uc?id=14bSslYG4QRzq4XOSMYWjkJl5auKYtigj) -->\n",
        "![relation_based_knowledge](https://i.postimg.cc/L85tCGd3/image.png)"
      ],
      "metadata": {
        "id": "yHgat4uISvSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training/Distillation Schemes"
      ],
      "metadata": {
        "id": "Mz-T2ItckeYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on whether the teacher model is\n",
        "updated simultaneously with the student model or\n",
        "not, the learning schemes of knowledge distillation\n",
        "can be divided into three main categories:\n",
        "**offline distillation**, **online distillation** and **self-distillation**.\n"
      ],
      "metadata": {
        "id": "7oehetFZhg2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![dist_scheme](https://drive.google.com/uc?id=1yldXqGD-Ze9elPOCINhKm3w067GgfQKl) -->\n",
        "![dist_scheme](https://i.postimg.cc/MG8GMjYf/image.png)"
      ],
      "metadata": {
        "id": "x4nMug1FIWe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Offline Distillation"
      ],
      "metadata": {
        "id": "kJXtWWiZIWhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In offline distillation, the teacher model is pre-trained and remains fixed during the distillation process. There is no further training or fine-tuning of the teacher model.\n",
        "\n",
        "**Training Process:**\n",
        "The training process is divided into two stages:\n",
        "- The large teacher model is first trained independently on a set of training samples before the distillation process begins.\n",
        "- The teacher model is then used to extract knowledge, such as logits or intermediate features, which are used to guide the training of the student model during distillation.\n",
        "\n",
        "**Focus:** Offline distillation methods mainly focus on the design of knowledge transfer, including knowledge representation and loss functions for matching features or distributions.\n",
        "\n",
        "**Advantages:** Offline methods are simple and easy to implement. They work well when the teacher model is pre-defined and known in advance."
      ],
      "metadata": {
        "id": "C3dDkJZ4ZQHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Online Distillation"
      ],
      "metadata": {
        "id": "egLptVpDZQKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A large pre-trained model as the teacher may not always be available as Offline Distillation methods assume. Thus, in such scenarios, the teacher and student networks are trained simultaneously — which is called Online Distillation. The entire distillation process is end-to-end trainable.\n",
        "\n",
        "**Training Process:**\n",
        "- Various online knowledge distillation methods have been proposed in recent years, including deep mutual learning, ensemble learning, and feature fusion.\n",
        "- The teacher-student interaction is more dynamic, allowing for improved adaptation and performance of the student model.\n",
        "\n",
        "**Advantages:** Online distillation is a one-phase end-to-end training scheme with efficient parallel computing. It is particularly useful when a high-capacity teacher model is not available, and it allows the student model to adapt to new data and domains."
      ],
      "metadata": {
        "id": "sNj4SEcaZQMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Self Distillation"
      ],
      "metadata": {
        "id": "8ZEX8mnqZQPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In self-distillation, the teacher and student models are the same network, meaning that the teacher and student are essentially the same model. There is no external teacher model.\n",
        "\n",
        "**Training Process:**\n",
        "- Knowledge from deeper sections of the network is distilled into shallower sections of the same network.\n",
        "- Self-attention distillation methods and snapshot distillation are examples of self-distillation techniques.\n",
        "- Self-distillation is used to optimize deep models with the same architecture one by one.\n",
        "\n",
        "**Advantages:** Self-distillation simplifies the distillation process, and the network learns from its own predictions and feature representations."
      ],
      "metadata": {
        "id": "JeXyo7nvZQR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is knowledge transferred from teacher to student?"
      ],
      "metadata": {
        "id": "ef-L9xfF3dYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following is the setup of the training process of the student model:"
      ],
      "metadata": {
        "id": "oBwvztRW6Oqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![know_dist](https://drive.google.com/uc?id=1QwDc9NFW8YRENWxwsDR7bvJhvM8kctjX) -->\n",
        "![know_dist](https://i.postimg.cc/nLZHcfHv/image.png)"
      ],
      "metadata": {
        "id": "bX5KyEniPCSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transfer of the generalization ability of teacher model to student model is done by the use of **soft targets.**\n",
        "- There are two loss terms. The soft labels (from Teacher) and soft predictions (from Student) are used in the first loss term, and the hard prediction (from Student) and hard labels are used in the second loss term. These two terms can always be configured for their contribution.\n",
        "- If teacher model is an ensemble of simpler models, we use arithmetic or geometric mean of individual predictive distributions as soft targets.\n",
        "- When high entropy is present in the soft targets, much more information per training case is provided than in the case of hard targets, and much less variance in the gradient between training cases is observed. As a result, the small model can often be trained on much less data than the original cumbersome model while using a much higher learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "LSKPCBRF5ElC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Temperature Parameter:\n",
        "The temperature parameter, often denoted as $T$, is a crucial element in the Knowledge Distillation process. It controls the softening of the teacher's predictions, making them more informative for the student. The temperature parameter is incorporated through a softmax function."
      ],
      "metadata": {
        "id": "DjrezvbEIWOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Function:\n",
        "The softmax function is typically used to convert logits (real-valued scores) into probabilities. In the context of Knowledge Distillation, it is modified to include the temperature parameter:\n",
        "\n",
        "The standard softmax function is defined as:\n",
        "$P_i=\\frac{e^{z_i}}{∑_je^{z_j}}$\n",
        "where,\n",
        "- $P_i$ is the probability of class $i$.\n",
        "- $z_i$ is the unnormalized logit of class $i$.\n",
        "- The denominator sums over all logits."
      ],
      "metadata": {
        "id": "MrbWCMSzIWRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, with the temperature parameter:\n",
        "\n",
        "$p_i = \\frac{e^{(z_i/T)}}{∑_je^{(z_j/T)}}$"
      ],
      "metadata": {
        "id": "QdXHMWHnIWTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature parameter $T$ controls the balance between two key aspects in Knowledge Distillation:\n",
        "\n",
        "1. **Hard Targets**: When $T=1$, the softmax function behaves like the standard softmax, resulting in hard targets. In this case, the student tries to match the exact class probabilities predicted by the teacher, aiming for a highly accurate but potentially overparameterized model.\n",
        "\n",
        "2. **Soft Targets**: When $T>1$, the softened probabilities are used as \"soft targets.\" This allows the student to focus on capturing the relative importance of different classes without necessarily matching the exact teacher's predictions. Soft targets help in reducing overfitting and generalizing better."
      ],
      "metadata": {
        "id": "xFikezVMIWZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distillation\n",
        "\n",
        "We use Logits(inputs to the final softmax) for distilling the learned knowledge. The student model can be trained using logits, which is achieved by minimizing the squared difference between the logits produced by the teacher model and the logits produced by the student model.\n",
        "\n",
        "$$p_i = \\frac{e^{(z_i/T)}}{∑_je^{(z_j/T)}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "xNBZC7HF8ce6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How Temperature Affects Knowledge Distillation:**\n",
        "- Raising Temperature = Making logits smaller\n",
        "- As we increase T, the logits are smoother (closer to each other)\n",
        "\n",
        "For high temperatures (T -> inf), nearly the same probability is assigned to all actions, and at lower temperatures (T -> 0), the probability is influenced more by the expected rewards. For low temperatures, the probability of the action with the highest expected reward tends to approach 1.\n",
        "\n",
        "In distillation, the temperature of the final softmax is raised until a suitably soft set of targets is generated by the cumbersome model. The same high temperature is then employed when training the small model to match these soft targets."
      ],
      "metadata": {
        "id": "VGAp_9QGjVWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective Function"
      ],
      "metadata": {
        "id": "c0mDS6Xl8chl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The initial objective function involves calculating the cross-entropy using soft targets, where the cross-entropy is determined using the high temperature employed in the softmax of the distilled model, which matches the temperature used for generating the soft targets from the larger model.\n",
        "- The second objective function pertains to the cross-entropy with the correct labels, and it is computed by utilizing the identical logits in the softmax of the distilled model, but this time, the temperature is set to 1.\n",
        "\n",
        "The Teacher model is trained using the categorical cross-entropy applied to the one-hot labels. When applying knowledge distillation the Student model is trained using a mix between the Kullback Leibler divergence and the MAE loss on the soft labels predicted by the Teacher model as target. The Kullback Leibler divergence measures the difference between two probability distributions, so our objective is to make the distribution predicted by the Student as close as possible to the distribution of the Teacher."
      ],
      "metadata": {
        "id": "P5oOiW078c4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation in NLP"
      ],
      "metadata": {
        "id": "NmrJQy4wvo-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Conventional language models like BERT are complex and resource-intensive. Knowledge distillation has emerged as a powerful approach to create lightweight and efficient language models in the field of NLP.\n",
        "\n",
        "- Numerous knowledge distillation (KD) methods have been proposed to address NLP tasks like neural machine translation (NMT), text generation, question-answering systems, event detection, document retrieval, text recognition, etc.\n",
        "\n",
        "- Most KD methods for NLP fall into the category of Natural Language Understanding (NLU). They are designed as **task-specific distillation** or **multi-task distillation**.\n",
        "\n",
        "- NMT stands out as a prominent application in NLP, and KD has been instrumental in creating lightweight NMT models. Various KD techniques have been employed to make NMT models more efficient and compact.\n",
        "\n",
        "- For instance, non-autoregressive machine translation (NAT) models have benefited from KD-based approaches, where the capacity of the student model and the knowledge transfer play a vital role. KD methods in the context of NMT have also leveraged sequence-level distillation, data augmentation, and regularization techniques to achieve good performance.\n",
        "\n",
        "- On the other hand, in NLU, BERT models have been a focal point. These deep models are effective but resource-intensive. To make them more lightweight and deployable, several lightweight BERT model variations have been created using knowledge distillation. These models, referred to as BERT model compression, aim to maintain the efficiency of the original BERT models while reducing their complexity.\n",
        "\n",
        "- As an example, DistilBERT represents a more compact, quicker, cost-effective, and less resource-intensive variant of the BERT model, developed by Hugging Face. During the pre-training phase, knowledge distillation was utilized to produce a distilled BERT model that is 40% smaller (66 million parameters compared to 110 million parameters) and 60% faster, all while preserving approximately 97% of the original BERT model's accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N--jT7ThvpBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distillation Algorithms"
      ],
      "metadata": {
        "id": "njqq25FlvpDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Adversarial Distillation"
      ],
      "metadata": {
        "id": "aZvXK1TCnDb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial distillation, inspired by generative adversarial networks (GANs), trains a generator to create synthetic data resembling the true distribution and a discriminator to differentiate genuine from synthetic data. This concept extends to knowledge distillation, improving both teacher and student models' understanding of the true data distribution. It can be implemented by training a generator for synthetic data, using a discriminator to help the student mimic the teacher, or by jointly optimizing the student and teacher models online."
      ],
      "metadata": {
        "id": "sXgrRAEXnDee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![GAN_KD](https://drive.google.com/uc?id=1MCdWB6F1VWu4DGetSauDQCECQl6qLqsa) -->\n",
        "![GAN_KD](https://i.postimg.cc/kgy5SgYj/image.png)"
      ],
      "metadata": {
        "id": "0-WzsA0jzJL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Multi-Teacher Distillation"
      ],
      "metadata": {
        "id": "h68dFDainDgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-teacher distillation involves a student model learning from multiple teacher models, providing diverse knowledge. The knowledge from different teachers, typically based on logits and feature representations, can be combined by averaging their responses. This approach offers advantages over learning from a single teacher."
      ],
      "metadata": {
        "id": "phJ6_VABnDjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![MulTeacher_KD](https://drive.google.com/uc?id=1wrebtycIoQTUa4elKUot_8KON4Bgrm1l) -->\n",
        "![MulTeacher_KD](https://i.postimg.cc/fTnWg962/image.png)"
      ],
      "metadata": {
        "id": "N69ajj-h1cl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Cross-Modal Distillation"
      ],
      "metadata": {
        "id": "cdzOdu6gnDmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-modal distillation transfers knowledge between modalities. It's particularly useful in the visual domain, where a teacher trained on labeled image data imparts knowledge to a student with an unlabeled input domain like optical flow, text, or audio. The teacher's image features guide the supervised training of the student. This approach is applied in applications such as visual question answering and image captioning."
      ],
      "metadata": {
        "id": "msmvpgUlnDpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![CrossModel_KD](https://drive.google.com/uc?id=17PTckHIxI3Y4xaOmiSjFEUI3_9CA_zsL) -->\n",
        "![CrossModel_KD](https://i.postimg.cc/fyh45xJY/image.png)"
      ],
      "metadata": {
        "id": "Fw6goPoV303b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Graph-Based Distillation"
      ],
      "metadata": {
        "id": "Q3YD3d1RnDsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph-based distillation methods aim to use the graph as a carrier of teacher knowledge or to control the message passing of teacher knowledge. These methods often involve constructing graphs based on teacher knowledge and using them to facilitate knowledge transfer. Various techniques have been proposed, including using locality-preserving loss functions, multi-head graphs, mutual relations of data samples, similarity matrices, and instance features and relationships modeled as graph vertices and edges. Some methods also control knowledge transfer using graphs by considering modality discrepancies, privileged information, and diverse knowledge transfer patterns. Constructing graphs to model the structure knowledge of data remains a challenging research area in graph-based distillation."
      ],
      "metadata": {
        "id": "RReVcud3nOP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![GraphBased_KD](https://drive.google.com/uc?id=1dn4Ky0y-M0-Cr_ZAxwUbdvznlYDw-C39) -->\n",
        "![GraphBased_KD](https://i.postimg.cc/L88cy46Q/image.png)"
      ],
      "metadata": {
        "id": "QC9dDWpOIjqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Attention-Based Distillation"
      ],
      "metadata": {
        "id": "38dXtjIlnOSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention mechanisms are employed in knowledge distillation to improve the performance of student networks by reflecting neuron activations in convolutional neural networks. Different attention-based knowledge distillation methods define various attention transfer mechanisms to distill knowledge from the teacher network to the student network. These methods revolve around defining attention maps for feature embeddings in neural network layers, effectively transferring knowledge about feature embeddings using attention map functions. Additionally, one approach utilizes an attention mechanism to assign different confidence rules for knowledge distillation."
      ],
      "metadata": {
        "id": "2wO4w00fnOVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data-Free Distillation"
      ],
      "metadata": {
        "id": "j6slbdfqnUMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data-free knowledge distillation methods address issues related to unavailability of data due to privacy, legal, security, or confidentiality concerns. These methods do not rely on existing training data but instead generate synthetic data. Several approaches have been proposed, including the use of Generative Adversarial Networks (GANs) to create transfer data, reconstructing transfer data from teacher network activations or spectral activations, and techniques that model the softmax space. Additionally, some methods explore knowledge distillation with few-shot learning, where the teacher uses limited labeled data. Data distillation, a related concept, employs new training annotations from unlabeled data generated by the teacher model to train the student."
      ],
      "metadata": {
        "id": "hJ6WhakDnUOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![DataFree_KD](https://drive.google.com/uc?id=1_WlZZCHUo-MTGpB4QvdzpKeRTytokwqq) -->\n",
        "![DataFree_KD](https://i.postimg.cc/yxK50sbP/image.png)"
      ],
      "metadata": {
        "id": "3DEGGoHzEADK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Quantized Distillation"
      ],
      "metadata": {
        "id": "kLRutWTbnURZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantized distillation combines network quantization, which reduces neural network complexity by converting high-precision networks into low-precision ones, with knowledge distillation. Several methods in this category aim to transfer knowledge from full-precision teacher networks to small, quantized student networks. This involves weight quantization, co-studying teacher and student networks, and the self-study of quantized student networks. Some recent approaches also incorporate self-distillation to improve the performance of quantized deep models, where teachers share model parameters with students."
      ],
      "metadata": {
        "id": "3hIqnR4CnUUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![Quantized_KD](https://drive.google.com/uc?id=1IZSTGqhK36YmkYQA2Zz0YqQFUqzcJJJ-) -->\n",
        "![Quantized_KD](https://i.postimg.cc/d1PSKJwY/image.png)"
      ],
      "metadata": {
        "id": "zNySVsMxGJ1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Lifelong Distillation"
      ],
      "metadata": {
        "id": "NkB0cgYUnbSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lifelong learning aims to accumulate and transfer knowledge over time. Knowledge distillation, a tool for preserving and transferring knowledge, has given rise to variants based on lifelong learning. These include meta-transfer networks, the Leap framework for meta-learning, and methods like semantic-aware knowledge preservation and global distillation. These approaches mitigate catastrophic forgetting in lifelong learning scenarios."
      ],
      "metadata": {
        "id": "T9hjoYS2nbVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. NAS-Based Distillation"
      ],
      "metadata": {
        "id": "-Z0vGZ6_nfX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural architecture search (NAS) automates the discovery of deep neural models. Knowledge distillation is integrated into NAS to address the challenge of transferring knowledge from large teacher models to smaller student models. This includes techniques like AdaNAS, NAS with distilled architecture knowledge, teacher-guided search for architectures (TGSA), and one-shot NAS. TGSA, for instance, guides architecture searches to mimic the teacher's intermediate feature representations for effective and efficient knowledge transfer."
      ],
      "metadata": {
        "id": "_XULxDWinfaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Example:"
      ],
      "metadata": {
        "id": "xRrY9KmC-zrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see an example of knowledge distillation using the student-teacher architecture on the **MNIST dataset**. Let us start by importing the necessary libraries."
      ],
      "metadata": {
        "id": "7hll3dd9bh0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Weq7xfDhnoNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create a teacher and smaller student model both of which are convolutional neural networks. The student model takes grayscale images (3 channels) as input and outputs class predictions (10 classes). The teacher model is similar in architecture to the student model but has more complex and larger layers."
      ],
      "metadata": {
        "id": "42ksYc-3qMVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Student(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Student, self).__init__()\n",
        "\t\tself.conv = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(3, 32, 3, 2),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Conv2d(32, 32, 3, 2),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Conv2d(32, 32, 2, 1),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.AdaptiveAvgPool2d(2)\n",
        "\t\t)\n",
        "\n",
        "\t\tself.fc = nn.Linear(128, 10)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.conv(x)\n",
        "\t\tout = out.view(x.shape[0], -1)\n",
        "\t\tout = self.fc(out)\n",
        "\t\treturn out\n",
        "\n",
        "\n",
        "class Teacher(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Teacher, self).__init__()\n",
        "\t\tself.conv = nn.Sequential(\n",
        "\t\t\tnn.Conv2d(3, 128, 3, 2),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Conv2d(128, 128, 3, 2),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Conv2d(128, 128, 2, 1),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.AdaptiveAvgPool2d(2)\n",
        "\t\t)\n",
        "\n",
        "\t\tself.fc = nn.Linear(512, 10)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.conv(x)\n",
        "\t\tout = out.view(x.shape[0], -1)\n",
        "\t\tout = self.fc(out)\n",
        "\t\treturn out"
      ],
      "metadata": {
        "id": "w76g-G3YnoQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the cross-entropy loss function to calculate the loss between the model's output and the ground truth labels."
      ],
      "metadata": {
        "id": "NH42KuO-rKwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(output, target):\n",
        "\treturn -torch.sum(output.log() * target) / output.shape[0]"
      ],
      "metadata": {
        "id": "tXo4_nwfnoS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now do the following:\n",
        "- Define data transformations to preprocess the images (resize, convert to grayscale, and transform to tensors).\n",
        "- Create training and testing datasets with these transformations.\n",
        "- Set up data loaders for efficient batching and shuffling."
      ],
      "metadata": {
        "id": "4z2n5jdWrP4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "temperature = 3\n",
        "loss_lambda = 0.5\n",
        "\n",
        "input_transform = transforms.Compose([\n",
        "\ttransforms.Resize(28),\n",
        "\ttransforms.Grayscale(3),\n",
        "\ttransforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = MNIST('./data', train=True, transform=input_transform, download=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_data = MNIST('./data', train=False, transform=input_transform, download=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "len_train_data = len(train_data)\n",
        "len_test_data = len(test_data)"
      ],
      "metadata": {
        "id": "CoY6h79GnoYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we train the **student model without knowledge distillation** using standard cross-entropy loss."
      ],
      "metadata": {
        "id": "JEzwcopDrYH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "print('\\n\\nTraining student without teacher...')\n",
        "student_without_teacher = Student().cuda()\n",
        "optimizer = torch.optim.Adam(student_without_teacher.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(num_epochs):\n",
        "\tprint(f'Epoch {epoch + 1}/{num_epochs}:')\n",
        "\n",
        "\ttrain_loss = .0\n",
        "\ttrain_acc = .0\n",
        "\tstudent_without_teacher.train()\n",
        "\tfor batch_x, batch_y in tqdm(train_loader):\n",
        "\t\tbatch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "\t\tout = torch.softmax(student_without_teacher(batch_x), dim=1)\n",
        "\t\tloss = criterion(out, batch_y)\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttrain_loss += loss.data.item()\n",
        "\t\ttrain_acc += (torch.max(out, 1)[1] == batch_y).sum().data.item()\n",
        "\tprint('Train loss: {:.6f}, acc: {:.6f}'.format(train_loss / len_train_data, train_acc / len_train_data))\n",
        "\n",
        "\teval_loss = .0\n",
        "\teval_acc = .0\n",
        "\tstudent_without_teacher.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch_x, batch_y in tqdm(test_loader):\n",
        "\t\t\tbatch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "\t\t\tout = torch.softmax(student_without_teacher(batch_x), dim=1)\n",
        "\t\t\tloss = criterion(out, batch_y)\n",
        "\n",
        "\t\t\teval_loss += loss.data.item()\n",
        "\t\t\teval_acc += (torch.max(out, 1)[1] == batch_y).sum().data.item()\n",
        "\t\tprint('Eval loss: {:.6f}, acc: {:.6f}'.format(eval_loss / len_test_data, eval_acc / len_test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6y93Azrnoa2",
        "outputId": "1cf3362f-eb42-4d70-8ec5-54462a20d4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Training student without teacher...\n",
            "Epoch 1/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:20<00:00, 45.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.027950, acc: 0.682200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 54.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.026629, acc: 0.764400\n",
            "Epoch 2/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 53.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.026460, acc: 0.768717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 62.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.026377, acc: 0.780200\n",
            "Epoch 3/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:18<00:00, 51.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.026337, acc: 0.775800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 67.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.026314, acc: 0.785000\n",
            "Epoch 4/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 52.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.026226, acc: 0.782950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 57.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.026228, acc: 0.789000\n",
            "Epoch 5/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:17<00:00, 52.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.026167, acc: 0.786267\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 67.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.026163, acc: 0.793300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we **train the teacher model** using the same procedure as training the student."
      ],
      "metadata": {
        "id": "E3EUfqDbri68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "print('\\n\\nTraining teacher...')\n",
        "teacher = Teacher().cuda()\n",
        "optimizer = torch.optim.Adam(teacher.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(num_epochs):\n",
        "\tprint(f'Epoch {epoch + 1}/{num_epochs}:')\n",
        "\n",
        "\ttrain_loss = .0\n",
        "\ttrain_acc = .0\n",
        "\tteacher.train()\n",
        "\tfor batch_x, batch_y in tqdm(train_loader):\n",
        "\t\tbatch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "\t\tout = torch.softmax(teacher(batch_x), dim=1)\n",
        "\t\tloss = criterion(out, batch_y)\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttrain_loss += loss.data.item()\n",
        "\t\ttrain_acc += (torch.max(out, 1)[1] == batch_y).sum().data.item()\n",
        "\tprint('Train loss: {:.6f}, acc: {:.6f}'.format(train_loss / len_train_data, train_acc / len_train_data))\n",
        "\n",
        "\teval_loss = .0\n",
        "\teval_acc = .0\n",
        "\tteacher.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch_x, batch_y in tqdm(test_loader):\n",
        "\t\t\tbatch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "\t\t\tout = torch.softmax(teacher(batch_x), dim=1)\n",
        "\t\t\tloss = criterion(out, batch_y)\n",
        "\n",
        "\t\t\teval_loss += loss.data.item()\n",
        "\t\t\teval_acc += (torch.max(out, 1)[1] == batch_y).sum().data.item()\n",
        "\t\tprint('Eval loss: {:.6f}, acc: {:.6f}'.format(eval_loss / len_test_data, eval_acc / len_test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOpqFHrSd56V",
        "outputId": "f2030efa-7f94-4701-df29-4dd6844bb3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Training teacher...\n",
            "Epoch 1/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:22<00:00, 42.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.028455, acc: 0.641000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:03<00:00, 51.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.025109, acc: 0.863800\n",
            "Epoch 2/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:22<00:00, 41.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.024835, acc: 0.873300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 53.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.024741, acc: 0.884900\n",
            "Epoch 3/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:22<00:00, 42.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.024085, acc: 0.921300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 52.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.023296, acc: 0.978700\n",
            "Epoch 4/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:22<00:00, 42.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.023220, acc: 0.977017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 52.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.023199, acc: 0.983700\n",
            "Epoch 5/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:22<00:00, 42.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.023156, acc: 0.980700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 53.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.023225, acc: 0.982700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we **train the student model with knowledge distillation**.\n",
        "We use a combination of standard cross-entropy loss and an additional loss function (cross_entropy_loss) to distill knowledge from the teacher.\n",
        "Temperature scaling is applied during knowledge distillation, controlling the softness of the predicted probabilities."
      ],
      "metadata": {
        "id": "zNldcxVrrqCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "print('\\n\\nTraining student with teacher...')\n",
        "student_with_teacher = Student().cuda()\n",
        "optimizer = torch.optim.Adam(student_with_teacher.parameters())\n",
        "criterion = [nn.CrossEntropyLoss(), cross_entropy_loss]\n",
        "for epoch in range(num_epochs):\n",
        "\tprint(f'Epoch {epoch + 1}/{num_epochs}:')\n",
        "\n",
        "\ttrain_loss = .0\n",
        "\ttrain_acc = .0\n",
        "\tstudent_with_teacher.train()\n",
        "\tfor batch_x, batch_y in tqdm(train_loader):\n",
        "\t\tbatch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "\t\tout = student_with_teacher(batch_x)\n",
        "\t\tbatch_y_teacher = torch.softmax(teacher(batch_x) / temperature, dim=1).detach()\n",
        "\t\tout_result = torch.softmax(out, dim=1)\n",
        "\t\tloss = (1 - loss_lambda) * criterion[0](out_result, batch_y) + \\\n",
        "\t\t       loss_lambda * temperature**2 * criterion[1](torch.softmax(out / temperature, dim=1), batch_y_teacher)\n",
        "\t\tassert not torch.isnan(loss)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\ttorch.nn.utils.clip_grad_norm_(student_with_teacher.parameters(), 1)\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttrain_loss += loss.data.item()\n",
        "\t\ttrain_acc += (torch.max(out_result, 1)[1] == batch_y).sum().data.item()\n",
        "\tprint('Train loss: {:.6f}, acc: {:.6f}'.format(train_loss / len_train_data, train_acc / len_train_data))\n",
        "\n",
        "\teval_loss = .0\n",
        "\teval_acc = .0\n",
        "\tstudent_with_teacher.eval()\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch_x, batch_y in tqdm(test_loader):\n",
        "\t\t\tbatch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n",
        "\n",
        "\t\t\tout = student_with_teacher(batch_x)\n",
        "\t\t\tbatch_y_teacher = torch.softmax(teacher(batch_x) / temperature, dim=1).detach()\n",
        "\t\t\tout_result = torch.softmax(out, dim=1)\n",
        "\t\t\tloss = (1 - loss_lambda) * criterion[0](out_result, batch_y) + \\\n",
        "\t\t\t       loss_lambda * temperature ** 2 * criterion[1](torch.softmax(out / temperature, dim=1), batch_y_teacher)\n",
        "\n",
        "\t\t\teval_loss += loss.data.item()\n",
        "\t\t\teval_acc += (torch.max(out_result, 1)[1] == batch_y).sum().data.item()\n",
        "\t\tprint('Eval loss: {:.6f}, acc: {:.6f}'.format(eval_loss / len_test_data, eval_acc / len_test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOk2-fCXd5-5",
        "outputId": "78e27776-e749-4da9-f98a-26f03aab0597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Training student with teacher...\n",
            "Epoch 1/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:21<00:00, 43.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.054643, acc: 0.820483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:03<00:00, 52.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.029902, acc: 0.922200\n",
            "Epoch 2/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:21<00:00, 44.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.026885, acc: 0.938950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:03<00:00, 51.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.023749, acc: 0.954000\n",
            "Epoch 3/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:21<00:00, 43.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.023196, acc: 0.955067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 53.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.022308, acc: 0.954500\n",
            "Epoch 4/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:21<00:00, 43.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.021392, acc: 0.963200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:03<00:00, 52.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.019958, acc: 0.971500\n",
            "Epoch 5/5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:21<00:00, 43.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.020283, acc: 0.967917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:03<00:00, 51.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval loss: 0.019338, acc: 0.973400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the teacher is trained for 5 full epochs and the student is distilled on this teacher for 5 full epochs, we can see in this example a performance boost compared to training the same student model from scratch. We can see the teacher to have accuracy around 98.3%, the student trained from scratch is around 80%, and the distilled student is around 97.3%. The student model with knowledge distillation from the teacher shows substantial improvements in accuracy during training and evaluation. It achieves approximately 97.34% accuracy on the evaluation dataset by the end of training. Knowledge distillation has effectively transferred knowledge from the teacher to the student, resulting in the student model approaching the accuracy of the teacher. The training loss is relatively higher than in the other cases because the distillation loss encourages softer predictions, leading to a more challenging optimization problem."
      ],
      "metadata": {
        "id": "tr6WgKacl_JH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, we can say that knowledge distillation is an effective technique for transferring knowledge from a more complex teacher model to a smaller student model. The student model, when trained with knowledge distillation, can achieve accuracy levels close to the teacher's accuracy, demonstrating the benefits of this approach for model compression and efficient deployment."
      ],
      "metadata": {
        "id": "8fqLhohoxHrw"
      }
    }
  ]
}