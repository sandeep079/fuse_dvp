{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Compression"
      ],
      "metadata": {
        "id": "PZHix9SiCFZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model compression in deep learning aims to deploy complex, high-performing neural networks on resource-constrained edge devices by reducing model size and latency. Size reduction involves minimizing the number and size of model parameters, resulting in decreased memory requirements during execution. Latency reduction focuses on speeding up model predictions, which, in turn, reduces energy consumption. Both aspects are interconnected, with smaller models demanding fewer memory resources, and faster models being more energy-efficient, making them suitable for deployment on devices with strict constraints. The primary goal of model compression is to simplify models while retaining their high accuracy and performance, thus enabling their efficient deployment in resource-limited environments.\n",
        "\n",
        "We will discuss about two popular methods of model compression here, namely - **Pruning** and **Quantization**."
      ],
      "metadata": {
        "id": "pgYNqeaR_0eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pruning"
      ],
      "metadata": {
        "id": "5TNZBfUZHl_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning is a methodology used to induce sparsity in neural networks by removing less important weights and activations, making the model more efficient.\n"
      ],
      "metadata": {
        "id": "lLOUlpeOOoYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Motivation:**\n",
        "\n",
        "|Age|Number of Connections|Stage|\n",
        "|---|---------------------|-----|\n",
        "|at birth|50 Trillion|newly formed|\n",
        "|1 year old|1000 Trillion|peak|\n",
        "|10 year old|500 Trillion|pruned and stabilized|\n",
        "\n",
        "- The synapses pruning mechanism in human brain development is shown in the table above.\n",
        "\n",
        "- **Pruning** mechanism removes redundant connections in the brain as human brain ages."
      ],
      "metadata": {
        "id": "hjL6zFr5HkU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning can be broadly classified into two types:"
      ],
      "metadata": {
        "id": "5-lXobClHkhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structured Pruning\n",
        "Structured Pruning operates by removing entire channels or filters from a neural network. For example, if one channel is pruned, it also eliminates corresponding channels, which can be determined by evaluating their importance. Methods vary, from calculating the importance of channels based on the sum of their weights to more sophisticated techniques like global and dynamic pruning, which aim to strike a balance between reducing the network's size and maintaining performance. Structured pruning has found applications in image segmentation and object detection, enhancing network efficiency and reducing computational resources.\n",
        "\n",
        "<!-- ![pic](https://drive.google.com/uc?id=1HjT5mMnF4BVtpb2qAMpG3cLkxAk6txaE) -->\n",
        "![pic](https://i.postimg.cc/6QLCyKCw/image.png)"
      ],
      "metadata": {
        "id": "HLRNyLItHmCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unstructured Pruning\n",
        "Unstructured Pruning, on the other hand, targets individual parameters, such as the magnitude of weights, gradients, or Hessian statistics. The \"optimal brain damage\" approach employs second-derivative information to identify unimportant weights and removes them from the network. Some methods, like the \"train–prune–retrain\" strategy, focus on learning only the important connections, which can significantly enhance performance without affecting accuracy. Unstructured pruning has the advantage of substantially reducing the number of parameters and computations, but it doesn't eliminate redundant neurons from the network, so it's essential to further investigate how to leverage unstructured pruning for current hardware architectures effectively.\n",
        "\n",
        "<!-- ![pic](https://drive.google.com/uc?id=1GwSN-Bc5i5ycEm4FvDVCSh0rY5H87c5B) -->\n",
        "![pic](https://i.postimg.cc/P51wRH3Z/image.png)"
      ],
      "metadata": {
        "id": "RnVWIYm9Ntfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various pruning techniques:\n",
        "1. **Weight Pruning(model pruning)**: Weight connections that are below some predefined thresholds are pruned (zeroed out).\n",
        "2. **Neuron Pruning**: Instead of removing the weights one by one, which can be time-consuming, we prune the neurons.\n",
        "3. **Filter Pruning**: Filters are ranked according to their importance, and the least important filters are removed from the network. The importance of the filters can be calculated by L1/L2 norm.\n",
        "4. **Layer Pruning**: Layers are pruned."
      ],
      "metadata": {
        "id": "mPezFiDLzbiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One question**: Should you prune large networks or build small dense networks?\n",
        "\n",
        "- Large-sparse models consistently outperform small-dense models and achieve upto 10x reduction in number of non-zero parameters with minimal loss in accuracy."
      ],
      "metadata": {
        "id": "y6qksh4yjvsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weights Pruning:\n",
        "Weights pruning aims to increase the sparsity of a network's weight tensors, reducing the number of parameters in the model."
      ],
      "metadata": {
        "id": "hBWnpYaiUe82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sparsity Definition**: Sparsity is defined as a measure of how many elements in a tensor are exact zeros relative to the tensor's size.\n",
        "\n",
        "**Pruning Weights, Biases, and Activations**: Pruning can be applied to weights, biases, and activations, with biases being less commonly pruned due to their contribution to a layer's output.\n",
        "\n",
        "**Iterative Pruning and Fine-Tuning**: Pruning can be performed iteratively, with fine-tuning between iterations to recover from pruning and maintain accuracy.\n",
        "\n",
        "**Pruning Criteria**: The most common pruning criteria involve comparing the absolute values of elements to a threshold. If an element's absolute value is below the threshold, it is set to zero (pruned).\n",
        "\n",
        "**Pruning Schedule**: The pruning schedule defines when, how often, and which tensors are pruned during the iterative process.\n",
        "\n",
        "**Pruning Granularity**: Pruning can be fine-grained (element-wise pruning) or coarse-grained, such as filter pruning, which removes entire groups of elements.\n",
        "\n",
        "**Sensitivity Analysis**: Sensitivity analysis helps rank tensors by their sensitivity to pruning, determining the impact of pruning on different layers in the network."
      ],
      "metadata": {
        "id": "60rFepDvO3BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Magnitude-based method: Iterative pruning + Retraining"
      ],
      "metadata": {
        "id": "pSLVdCIWJIIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![pic](https://drive.google.com/uc?id=1fwZkjelcJnq4OVdjosoQ5lUxdf116I7L) -->\n",
        "![pic](https://i.postimg.cc/4dYtw9r4/image.png)"
      ],
      "metadata": {
        "id": "snZEyTGpJKle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Magnitude-Based Pruning is a popular and straightforward pruning technique where you remove the least important weights in a neural network based on their magnitudes. The idea is to identify the connections (weights) with small absolute values, considering them less critical to the network's performance. These small weights are then pruned, effectively reducing the network's size."
      ],
      "metadata": {
        "id": "tlNkCNs9MPXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lottery Ticket Hypothesis"
      ],
      "metadata": {
        "id": "_ehyIrxAWwVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"A randomly-initialized, dense neural network contains a subnetwork that is initialized such that - when trained in isolation - it can match the test accuracy of the original network after training for at most the same number of iterations.\"**"
      ],
      "metadata": {
        "id": "WFBia2LpWwXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Lottery Ticket Hypothesis is a concept in deep learning that suggests within large, over-parameterized neural networks, there exist subnetworks (winning tickets) that, when trained in isolation, can match or even exceed the performance of the original network. These winning tickets are sparse, meaning they contain a small fraction of the original parameters."
      ],
      "metadata": {
        "id": "9Pt8sQStWwaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Lottery Ticket Hypothesis has a close relationship with pruning for model compression:\n",
        "\n",
        "1. **Pruning Criteria**: The Lottery Ticket Hypothesis suggests that subnetworks with the potential for high performance are embedded within over-parameterized networks. Magnitude-Based Pruning aligns with this idea by selecting weights with small magnitudes for removal. It assumes that these small weights contribute less to the network's performance and can be pruned without significant accuracy loss.\n",
        "\n",
        "2. **Discovery of Winning Tickets**: In the context of the Lottery Ticket Hypothesis, the subnetworks that exhibit high performance are akin to \"winning tickets.\" Magnitude-Based Pruning helps identify such winning tickets by removing the unimportant weights, effectively revealing subnetworks that have the potential to achieve good performance when retrained.\n",
        "\n",
        "3. **Iterative Pruning**: The Lottery Ticket Hypothesis also supports the idea of iterative pruning, where you prune a portion of the network and then retrain it to recover performance. This is in line with Magnitude-Based Pruning, where you can apply pruning iteratively, gradually removing less important weights and retraining the network to maintain or improve its performance.\n",
        "\n",
        "4. **Size and Efficiency**: Both methods aim to reduce the size of the neural network, making it more compact and efficient for deployment. Magnitude-Based Pruning, by removing small weights, contributes to the creation of smaller subnetworks (winning tickets) that align with the Lottery Ticket Hypothesis."
      ],
      "metadata": {
        "id": "E4C3Sq6lYBKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Quantization"
      ],
      "metadata": {
        "id": "kbH-DaunHomN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization is the process of reducing the number of bits that represent a number. Efforts in deep learning research have focused on reducing the resource demands of models, leading to the use of lower-precision numerical formats like 8-bit integers (INT8) instead of the predominant 32-bit floating point(FP32) for more efficient inference without significant accuracy loss. Lower bit-widths, such as 4/2/1-bits, are also actively researched and have shown progress in enhancing inference efficiency.\n",
        "\n",
        "- **Aggressive Quantization**: INT4 and Lower\n",
        "- **Conservative Quantization**: INT8"
      ],
      "metadata": {
        "id": "p26ZQyRIHoo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Comparison of represented value and storage bits of different data types:*\n",
        "\n",
        "|Data type|Represented Value|Storage bits|\n",
        "|---|---|---|\n",
        "|**FP64**|3.141592653589793|64 bits\n",
        "|**FP32**|3.141592653|32 bits\n",
        "|**FP16**|3.1415|16 bits\n",
        "|**INT8**|3|8 bits"
      ],
      "metadata": {
        "id": "2e9MKGpF6azD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits**:\n",
        "- significantly reduced bandwidth and storage\n",
        "- integer computer is faster than floating point compute\n",
        "- more area and energy efficient"
      ],
      "metadata": {
        "id": "zuIdYilM6awF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Algorithm: Parameter quantization|\n",
        "| -------------------------------- |\n",
        "|**Step 1:** Count the corresponding min_value and max_value in the input data (weights or activation values)|\n",
        "|**Step 2:** Choose the appropriate quantization type, symmetric (int-8) or asymmetric (uint-8)|\n",
        "|**Step 3:** Calculate the quantization parameters Z/Zero point and S/Scale according to the quantization type, min_value and max_value|\n",
        "|**Step 4:** Quantize the model based on the calibration data, converted from FP32 to INT-8|\n",
        "|**Step 5:** Verify the performance of the quantized model, and if the result is not good, try to use a different way to calculate S and Z, and re-execute the above operation|"
      ],
      "metadata": {
        "id": "cd1CZNhp8dD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization-Aware Training (QAT)"
      ],
      "metadata": {
        "id": "SURWafwXZaTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization-Aware Training helps neural networks adapt to lower bit-width representations during the training phase. It quantizes the model's parameters during both forward and backward propagation. However, the quantization happens after each gradient update and, importantly, after the weight updates in floating-point precision. Backward transfer is also carried out in a floating-point manner to avoid accumulating gradients with quantization precision, which can lead to high errors, particularly with low-precision quantization. QAT ensures that the model converges to a better loss point, compensating for the perturbations introduced by quantization."
      ],
      "metadata": {
        "id": "OtvK8mFCZeLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Training Quantization(PTQ)"
      ],
      "metadata": {
        "id": "8tc3eNgfZeN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to QAT, Post-Training Quantization is applied after the model has been trained with full precision (32-bit floating-point numbers). PTQ quantizes the trained model and adjusts the weights without the need for extensive fine-tuning. This process offers a cost-effective and low-overhead solution for quantization. A significant advantage of PTQ is that it can be applied with limited or no labeled data, making it highly practical. However, PTQ may lead to a reduction in accuracy, especially with low-precision quantization. Researchers have devised various methods to address this accuracy drop, such as bias correction, weight range balancing, and outlier handling. PTQ is a fast way to quantize neural network models, but it typically achieves lower accuracy compared to QAT."
      ],
      "metadata": {
        "id": "nIz6Oc0BZeUx"
      }
    }
  ]
}